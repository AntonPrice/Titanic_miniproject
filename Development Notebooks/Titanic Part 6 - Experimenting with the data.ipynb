{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Classification - Experimenting with the data (Part 6)\n",
    "\n",
    "Now we have an idea of our submission accuracy its time to get as much performance as possible out of our model by applying everything learned so far and from all the research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#First importing some relevant packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Import Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "#Import Keras\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Dropout, Reshape, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#Import mathematical functions\n",
    "from random import *\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Get regular expression package\n",
    "import re\n",
    "\n",
    "#Import  Scikit learn framework\n",
    "import sklearn as sk\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the functions built in previous parts\n",
    "from Titanic_Import import *\n",
    "\n",
    "full_set = pd.read_csv('D:/Datasets/Titanic/train.csv')\n",
    "sub_set = pd.read_csv('D:/Datasets/Titanic/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing submission data\n",
    "\n",
    "So the first and glaringly obvious difference between our cross validation and test data is we applied two (albiet identical) algorithms to normalize each.  Instead of normalizing the submission data seperately to our training and cross validation data lets normalize everything together and then train our model off of data normalized across the same distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_set = full_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_set = append_set.append([sub_set], ignore_index =True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating our Training Set\n",
    "def Cleanse_Data_v3(df_in):\n",
    "    #Put our dataframe into new object to avoid corrupting original dataframe\n",
    "    test_set = df_in\n",
    "    \n",
    "    test_set['Age'] = test_set.groupby(['Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\n",
    "    test_set['Fare'] = test_set.groupby(['Pclass'])['Fare'].apply(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    #Name Length from Anisotropic\n",
    "    test_set['Name_length'] = test_set['Name'].apply(len)\n",
    "    \n",
    "    test_set['Company'] = test_set['SibSp'] + test_set['Parch']\n",
    "    \n",
    "    #Normalize numerical fields\n",
    "    age_mean = test_set['Age'].mean()\n",
    "    fare_mean = test_set['Fare'].mean()\n",
    "    \n",
    "    age_range = test_set['Age'].max() - test_set['Age'].min()\n",
    "    fare_range = test_set['Fare'].max() - test_set['Fare'].min()\n",
    "    \n",
    "    #Standard deviations to test\n",
    "    age_std = test_set['Age'].std(skipna=True)\n",
    "    fare_std = test_set['Fare'].std(skipna=True)\n",
    "    \n",
    "    \n",
    "    test_set['Norm_age'] = (test_set['Age'] - age_mean) / age_range\n",
    "    test_set['Norm_fare'] = (test_set['Fare'] - fare_mean) / fare_range\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Getting our Deck\n",
    "    test_set['canc'] = test_set['Cabin'].str.replace(' ', '')\n",
    "    test_set['Deckstr'] = test_set['canc'].str[0]\n",
    "    test_set['Deckstr'] = test_set['Deckstr'].fillna(value = 'X')\n",
    "    test_set['Deckstr'] = test_set['Deckstr'].map( {'A': 1, 'B': 2, 'C' : 3,'D' : 4, 'E' : 5,'F' : 6,'G' : 7, 'T' : 8 ,'X' : 0} ).astype(int)\n",
    "    \n",
    "    #Remap Gender and create number of family members present field\n",
    "    test_set['Sex'] = test_set['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n",
    "    \n",
    "\n",
    "    #Applying Title code from Sia\n",
    "    test_set['Title'] = test_set['Name'].apply(get_title)\n",
    "    test_set['Title'] = test_set['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    test_set['Title'] = test_set['Title'].replace('Mlle', 'Miss')\n",
    "    test_set['Title'] = test_set['Title'].replace('Ms', 'Miss')\n",
    "    test_set['Title'] = test_set['Title'].replace('Mme', 'Mrs')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #Manually populate embarked with correct values (only 2 looked up correct value based on average fare)\n",
    "    values = {'Embarked': 'C'}\n",
    "    test_set = test_set.fillna(value=values)\n",
    "    test_set['Embarked'] = test_set['Embarked'].map( {'S': 0, 'C': 1, 'Q' : 2} ).astype(int)\n",
    "    \n",
    "    \n",
    "    \n",
    "    emb_set = pd.get_dummies(test_set.Embarked, prefix='Emb', dummy_na = False)\n",
    "    title_set = pd.get_dummies(test_set.Title, prefix='ti', dummy_na = True)\n",
    "    deck_set = pd.get_dummies(test_set.Deckstr, prefix='de', dummy_na = True)\n",
    "\n",
    "    \n",
    "    oh_set = pd.concat([test_set,  \n",
    "                        emb_set, \n",
    "                        title_set, \n",
    "                        deck_set\n",
    "                       ], axis=1)\n",
    "    \n",
    "    #Create output fully numeric dataframe\n",
    "    out_set = oh_set.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', \n",
    "                             'Cabin', 'canc', 'Embarked', 'Title', 'Deckstr', \n",
    "                            'Name_length',\n",
    "                           'Age', 'Fare', 'Name_length'], axis=1)\n",
    "    return out_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_set = Cleanse_Data_v3(append_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Company</th>\n",
       "      <th>Norm_age</th>\n",
       "      <th>Norm_fare</th>\n",
       "      <th>Emb_0</th>\n",
       "      <th>Emb_1</th>\n",
       "      <th>Emb_2</th>\n",
       "      <th>ti_Master</th>\n",
       "      <th>...</th>\n",
       "      <th>de_0.0</th>\n",
       "      <th>de_1.0</th>\n",
       "      <th>de_2.0</th>\n",
       "      <th>de_3.0</th>\n",
       "      <th>de_4.0</th>\n",
       "      <th>de_5.0</th>\n",
       "      <th>de_6.0</th>\n",
       "      <th>de_7.0</th>\n",
       "      <th>de_8.0</th>\n",
       "      <th>de_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.090286</td>\n",
       "      <td>-0.050800</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.110140</td>\n",
       "      <td>0.074185</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.040180</td>\n",
       "      <td>-0.049482</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.072560</td>\n",
       "      <td>0.038693</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072560</td>\n",
       "      <td>-0.049238</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.065233</td>\n",
       "      <td>-0.048441</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.310566</td>\n",
       "      <td>0.036278</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.340818</td>\n",
       "      <td>-0.023815</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.027653</td>\n",
       "      <td>-0.043220</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.190499</td>\n",
       "      <td>-0.006257</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Sex  Survived  Company  Norm_age  Norm_fare  Emb_0  Emb_1  Emb_2  \\\n",
       "0       3    0       0.0        1 -0.090286  -0.050800      1      0      0   \n",
       "1       1    1       1.0        1  0.110140   0.074185      0      1      0   \n",
       "2       3    1       1.0        0 -0.040180  -0.049482      1      0      0   \n",
       "3       1    1       1.0        1  0.072560   0.038693      1      0      0   \n",
       "4       3    0       0.0        0  0.072560  -0.049238      1      0      0   \n",
       "5       3    0       0.0        0 -0.065233  -0.048441      0      0      1   \n",
       "6       1    0       0.0        0  0.310566   0.036278      1      0      0   \n",
       "7       3    0       0.0        4 -0.340818  -0.023815      1      0      0   \n",
       "8       3    1       1.0        2 -0.027653  -0.043220      1      0      0   \n",
       "9       2    1       1.0        1 -0.190499  -0.006257      0      1      0   \n",
       "\n",
       "   ti_Master   ...    de_0.0  de_1.0  de_2.0  de_3.0  de_4.0  de_5.0  de_6.0  \\\n",
       "0          0   ...         1       0       0       0       0       0       0   \n",
       "1          0   ...         0       0       0       1       0       0       0   \n",
       "2          0   ...         1       0       0       0       0       0       0   \n",
       "3          0   ...         0       0       0       1       0       0       0   \n",
       "4          0   ...         1       0       0       0       0       0       0   \n",
       "5          0   ...         1       0       0       0       0       0       0   \n",
       "6          0   ...         0       0       0       0       0       1       0   \n",
       "7          1   ...         1       0       0       0       0       0       0   \n",
       "8          0   ...         1       0       0       0       0       0       0   \n",
       "9          0   ...         1       0       0       0       0       0       0   \n",
       "\n",
       "   de_7.0  de_8.0  de_nan  \n",
       "0       0       0       0  \n",
       "1       0       0       0  \n",
       "2       0       0       0  \n",
       "3       0       0       0  \n",
       "4       0       0       0  \n",
       "5       0       0       0  \n",
       "6       0       0       0  \n",
       "7       0       0       0  \n",
       "8       0       0       0  \n",
       "9       0       0       0  \n",
       "\n",
       "[10 rows x 25 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_set.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have our data normalized across the entire distribution lets split out our training, test and cross validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_splitter(df_in, cv_size = 100):\n",
    "    #defining new dataframe to avoid corrupting original\n",
    "    test_set = df_in\n",
    "    \n",
    "    #Split out Test data for random sampling for CV data\n",
    "    train_set = test_set[test_set['Survived'].notnull()]\n",
    "    \n",
    "    #Randomly sample this time stratifying by survival\n",
    "    cv_set = train_set.sample(cv_size)\n",
    "    \n",
    "    #Drop all rows from our training set that are in our CV set\n",
    "    new_train = train_set[~train_set.isin(cv_set)].dropna(how = 'all')\n",
    "    \n",
    "    #Create numpy arrays out of our Training and CV sets\n",
    "    Y_Train = new_train['Survived'].values\n",
    "    Y_CV = cv_set['Survived'].values\n",
    "    \n",
    "    new_train = new_train.drop(['Survived'], axis=1)\n",
    "    cv_set = cv_set.drop(['Survived'], axis=1)\n",
    "    \n",
    "    X_Train = new_train.values\n",
    "    X_CV = cv_set.values\n",
    "    \n",
    "    \n",
    "    #Get Test Data\n",
    "    sub_set = test_set[test_set['Survived'].isnull()]\n",
    "    sub_set = sub_set.drop(['Survived'], axis=1)\n",
    "    X_Test = sub_set.values\n",
    "    \n",
    "    return X_Train, Y_Train, X_CV, Y_CV, X_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train, Y_Train, X_CV, Y_CV, X_Test = dataset_splitter(clean_set, cv_size = 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our data its time to build a bespoke neural network for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model_v2(input_shape, layers, act_reg, ker_reg):\n",
    "    #Having dynamic input shape as I may do feature engineering later.\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    X = Dense(layers[0], input_dim=input_shape, activation='relu')(X_input)\n",
    "    #X = LeakyReLU()(X)\n",
    "    #X = BatchNormalization()(X)\n",
    "\n",
    "    #Our NN Layers\n",
    "    for i in range(len(layers) - 1):\n",
    "      X = Dense(layers[i + 1], activation='relu', activity_regularizer = act_reg, kernel_regularizer = ker_reg)(X)\n",
    "      #X = LeakyReLU()(X)\\\n",
    "      #X = BatchNormalization()(X)\n",
    "\n",
    "    \n",
    "    X = Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "    model = Model(inputs = X_input, outputs = X, name='Simple_model')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a LOT of experimentation and testing with parameters in the above functions the below is a representative example of the sort of performance I was able to obtain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [21, 14, 8, 5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "741/741 [==============================] - 1s 785us/step - loss: 1.1012 - acc: 0.5034\n",
      "Epoch 2/256\n",
      "741/741 [==============================] - 0s 263us/step - loss: 0.7812 - acc: 0.6086\n",
      "Epoch 3/256\n",
      "741/741 [==============================] - 0s 258us/step - loss: 0.7213 - acc: 0.6086\n",
      "Epoch 4/256\n",
      "741/741 [==============================] - 0s 252us/step - loss: 0.7025 - acc: 0.6059\n",
      "Epoch 5/256\n",
      "741/741 [==============================] - 0s 262us/step - loss: 0.6937 - acc: 0.6073\n",
      "Epoch 6/256\n",
      "741/741 [==============================] - 0s 259us/step - loss: 0.6887 - acc: 0.6073\n",
      "Epoch 7/256\n",
      "741/741 [==============================] - 0s 275us/step - loss: 0.6857 - acc: 0.6073\n",
      "Epoch 8/256\n",
      "741/741 [==============================] - 0s 266us/step - loss: 0.6833 - acc: 0.6073\n",
      "Epoch 9/256\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.6817 - acc: 0.6073\n",
      "Epoch 10/256\n",
      "741/741 [==============================] - 0s 278us/step - loss: 0.6803 - acc: 0.6073\n",
      "Epoch 11/256\n",
      "741/741 [==============================] - 0s 281us/step - loss: 0.6790 - acc: 0.6073\n",
      "Epoch 12/256\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.6783 - acc: 0.6073\n",
      "Epoch 13/256\n",
      "741/741 [==============================] - 0s 270us/step - loss: 0.6774 - acc: 0.6073\n",
      "Epoch 14/256\n",
      "741/741 [==============================] - 0s 262us/step - loss: 0.6769 - acc: 0.6073\n",
      "Epoch 15/256\n",
      "741/741 [==============================] - 0s 251us/step - loss: 0.6762 - acc: 0.6073\n",
      "Epoch 16/256\n",
      "741/741 [==============================] - 0s 254us/step - loss: 0.6757 - acc: 0.6073\n",
      "Epoch 17/256\n",
      "741/741 [==============================] - 0s 254us/step - loss: 0.6752 - acc: 0.6073\n",
      "Epoch 18/256\n",
      "741/741 [==============================] - 0s 248us/step - loss: 0.6747 - acc: 0.6073\n",
      "Epoch 19/256\n",
      "741/741 [==============================] - 0s 252us/step - loss: 0.6743 - acc: 0.6073 0s - loss: 0.6733 - acc: 0.610\n",
      "Epoch 20/256\n",
      "741/741 [==============================] - 0s 252us/step - loss: 0.6737 - acc: 0.6073\n",
      "Epoch 21/256\n",
      "741/741 [==============================] - 0s 251us/step - loss: 0.6734 - acc: 0.6073\n",
      "Epoch 22/256\n",
      "741/741 [==============================] - 0s 256us/step - loss: 0.6730 - acc: 0.6073\n",
      "Epoch 23/256\n",
      "741/741 [==============================] - 0s 259us/step - loss: 0.6727 - acc: 0.6073\n",
      "Epoch 24/256\n",
      "741/741 [==============================] - 0s 270us/step - loss: 0.6724 - acc: 0.6073\n",
      "Epoch 25/256\n",
      "741/741 [==============================] - 0s 270us/step - loss: 0.6719 - acc: 0.6073\n",
      "Epoch 26/256\n",
      "741/741 [==============================] - 0s 266us/step - loss: 0.6714 - acc: 0.6073\n",
      "Epoch 27/256\n",
      "741/741 [==============================] - 0s 269us/step - loss: 0.6701 - acc: 0.6073\n",
      "Epoch 28/256\n",
      "741/741 [==============================] - 0s 247us/step - loss: 0.6659 - acc: 0.6181\n",
      "Epoch 29/256\n",
      "741/741 [==============================] - 0s 246us/step - loss: 0.6549 - acc: 0.6829\n",
      "Epoch 30/256\n",
      "741/741 [==============================] - 0s 248us/step - loss: 0.6403 - acc: 0.7557\n",
      "Epoch 31/256\n",
      "741/741 [==============================] - 0s 250us/step - loss: 0.6215 - acc: 0.7976\n",
      "Epoch 32/256\n",
      "741/741 [==============================] - 0s 250us/step - loss: 0.6103 - acc: 0.8097 0s - loss: 0.6117 - acc: 0.806\n",
      "Epoch 33/256\n",
      "741/741 [==============================] - 0s 250us/step - loss: 0.6014 - acc: 0.8070\n",
      "Epoch 34/256\n",
      "741/741 [==============================] - 0s 247us/step - loss: 0.5915 - acc: 0.8232\n",
      "Epoch 35/256\n",
      "741/741 [==============================] - 0s 250us/step - loss: 0.5869 - acc: 0.8178\n",
      "Epoch 36/256\n",
      "741/741 [==============================] - 0s 248us/step - loss: 0.5790 - acc: 0.8313\n",
      "Epoch 37/256\n",
      "741/741 [==============================] - 0s 252us/step - loss: 0.5724 - acc: 0.8259\n",
      "Epoch 38/256\n",
      "741/741 [==============================] - 0s 259us/step - loss: 0.5645 - acc: 0.8327\n",
      "Epoch 39/256\n",
      "741/741 [==============================] - 0s 263us/step - loss: 0.5574 - acc: 0.8300\n",
      "Epoch 40/256\n",
      "741/741 [==============================] - 0s 251us/step - loss: 0.5490 - acc: 0.8408\n",
      "Epoch 41/256\n",
      "741/741 [==============================] - 0s 259us/step - loss: 0.5506 - acc: 0.8381\n",
      "Epoch 42/256\n",
      "741/741 [==============================] - 0s 248us/step - loss: 0.5408 - acc: 0.8435\n",
      "Epoch 43/256\n",
      "741/741 [==============================] - 0s 252us/step - loss: 0.5350 - acc: 0.8543\n",
      "Epoch 44/256\n",
      "741/741 [==============================] - 0s 260us/step - loss: 0.5290 - acc: 0.8610\n",
      "Epoch 45/256\n",
      "741/741 [==============================] - 0s 258us/step - loss: 0.5319 - acc: 0.8462\n",
      "Epoch 46/256\n",
      "741/741 [==============================] - 0s 248us/step - loss: 0.5222 - acc: 0.8475\n",
      "Epoch 47/256\n",
      "741/741 [==============================] - 0s 258us/step - loss: 0.5195 - acc: 0.8516\n",
      "Epoch 48/256\n",
      "741/741 [==============================] - 0s 262us/step - loss: 0.5169 - acc: 0.8502\n",
      "Epoch 49/256\n",
      "741/741 [==============================] - 0s 256us/step - loss: 0.5127 - acc: 0.8462\n",
      "Epoch 50/256\n",
      "741/741 [==============================] - 0s 248us/step - loss: 0.5130 - acc: 0.8489\n",
      "Epoch 51/256\n",
      "741/741 [==============================] - 0s 251us/step - loss: 0.5084 - acc: 0.8502\n",
      "Epoch 52/256\n",
      "741/741 [==============================] - 0s 255us/step - loss: 0.5046 - acc: 0.8556\n",
      "Epoch 53/256\n",
      "741/741 [==============================] - 0s 259us/step - loss: 0.5030 - acc: 0.8543\n",
      "Epoch 54/256\n",
      "741/741 [==============================] - 0s 255us/step - loss: 0.5035 - acc: 0.8570\n",
      "Epoch 55/256\n",
      "741/741 [==============================] - 0s 248us/step - loss: 0.5034 - acc: 0.8448\n",
      "Epoch 56/256\n",
      "741/741 [==============================] - 0s 250us/step - loss: 0.5013 - acc: 0.8556\n",
      "Epoch 57/256\n",
      "741/741 [==============================] - 0s 248us/step - loss: 0.4916 - acc: 0.8529\n",
      "Epoch 58/256\n",
      "741/741 [==============================] - 0s 256us/step - loss: 0.4919 - acc: 0.8570\n",
      "Epoch 59/256\n",
      "741/741 [==============================] - 0s 259us/step - loss: 0.4960 - acc: 0.8475\n",
      "Epoch 60/256\n",
      "741/741 [==============================] - 0s 262us/step - loss: 0.5021 - acc: 0.8462\n",
      "Epoch 61/256\n",
      "741/741 [==============================] - 0s 255us/step - loss: 0.4929 - acc: 0.8529\n",
      "Epoch 62/256\n",
      "741/741 [==============================] - 0s 255us/step - loss: 0.4845 - acc: 0.8570\n",
      "Epoch 63/256\n",
      "741/741 [==============================] - 0s 254us/step - loss: 0.4854 - acc: 0.8570\n",
      "Epoch 64/256\n",
      "741/741 [==============================] - 0s 256us/step - loss: 0.4815 - acc: 0.8516\n",
      "Epoch 65/256\n",
      "741/741 [==============================] - 0s 260us/step - loss: 0.4779 - acc: 0.8543\n",
      "Epoch 66/256\n",
      "741/741 [==============================] - 0s 254us/step - loss: 0.4935 - acc: 0.8516\n",
      "Epoch 67/256\n",
      "741/741 [==============================] - 0s 254us/step - loss: 0.4922 - acc: 0.8435\n",
      "Epoch 68/256\n",
      "741/741 [==============================] - 0s 256us/step - loss: 0.4904 - acc: 0.8529\n",
      "Epoch 69/256\n",
      "741/741 [==============================] - 0s 273us/step - loss: 0.4726 - acc: 0.8583\n",
      "Epoch 70/256\n",
      "741/741 [==============================] - 0s 256us/step - loss: 0.4735 - acc: 0.8583\n",
      "Epoch 71/256\n",
      "741/741 [==============================] - 0s 258us/step - loss: 0.4682 - acc: 0.8596\n",
      "Epoch 72/256\n",
      "741/741 [==============================] - 0s 251us/step - loss: 0.4756 - acc: 0.8543\n",
      "Epoch 73/256\n",
      "741/741 [==============================] - 0s 259us/step - loss: 0.4708 - acc: 0.8543\n",
      "Epoch 74/256\n",
      "741/741 [==============================] - 0s 260us/step - loss: 0.4634 - acc: 0.8664\n",
      "Epoch 75/256\n",
      "741/741 [==============================] - 0s 258us/step - loss: 0.4707 - acc: 0.8596\n",
      "Epoch 76/256\n",
      "741/741 [==============================] - 0s 259us/step - loss: 0.4676 - acc: 0.8691\n",
      "Epoch 77/256\n",
      "741/741 [==============================] - 0s 256us/step - loss: 0.4671 - acc: 0.8650\n",
      "Epoch 78/256\n",
      "741/741 [==============================] - 0s 263us/step - loss: 0.4691 - acc: 0.8543\n",
      "Epoch 79/256\n",
      "741/741 [==============================] - 0s 267us/step - loss: 0.4637 - acc: 0.8570\n",
      "Epoch 80/256\n",
      "741/741 [==============================] - 0s 279us/step - loss: 0.4591 - acc: 0.8596\n",
      "Epoch 81/256\n",
      "741/741 [==============================] - 0s 281us/step - loss: 0.4588 - acc: 0.8596\n",
      "Epoch 82/256\n",
      "741/741 [==============================] - 0s 275us/step - loss: 0.4606 - acc: 0.8583\n",
      "Epoch 83/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 266us/step - loss: 0.4562 - acc: 0.8610\n",
      "Epoch 84/256\n",
      "741/741 [==============================] - 0s 265us/step - loss: 0.4577 - acc: 0.8543\n",
      "Epoch 85/256\n",
      "741/741 [==============================] - 0s 274us/step - loss: 0.4555 - acc: 0.8637\n",
      "Epoch 86/256\n",
      "741/741 [==============================] - 0s 259us/step - loss: 0.4606 - acc: 0.8556\n",
      "Epoch 87/256\n",
      "741/741 [==============================] - 0s 262us/step - loss: 0.4533 - acc: 0.8610\n",
      "Epoch 88/256\n",
      "741/741 [==============================] - 0s 260us/step - loss: 0.4564 - acc: 0.8489\n",
      "Epoch 89/256\n",
      "741/741 [==============================] - 0s 258us/step - loss: 0.4517 - acc: 0.8677\n",
      "Epoch 90/256\n",
      "741/741 [==============================] - 0s 273us/step - loss: 0.4516 - acc: 0.8610\n",
      "Epoch 91/256\n",
      "741/741 [==============================] - 0s 252us/step - loss: 0.4496 - acc: 0.8637\n",
      "Epoch 92/256\n",
      "741/741 [==============================] - 0s 258us/step - loss: 0.4447 - acc: 0.8650\n",
      "Epoch 93/256\n",
      "741/741 [==============================] - 0s 263us/step - loss: 0.4473 - acc: 0.8596\n",
      "Epoch 94/256\n",
      "741/741 [==============================] - 0s 271us/step - loss: 0.4423 - acc: 0.8731\n",
      "Epoch 95/256\n",
      "741/741 [==============================] - 0s 265us/step - loss: 0.4442 - acc: 0.8704\n",
      "Epoch 96/256\n",
      "741/741 [==============================] - 0s 258us/step - loss: 0.4401 - acc: 0.8691\n",
      "Epoch 97/256\n",
      "741/741 [==============================] - 0s 266us/step - loss: 0.4488 - acc: 0.8610\n",
      "Epoch 98/256\n",
      "741/741 [==============================] - 0s 256us/step - loss: 0.4465 - acc: 0.8650\n",
      "Epoch 99/256\n",
      "741/741 [==============================] - 0s 265us/step - loss: 0.4456 - acc: 0.8664\n",
      "Epoch 100/256\n",
      "741/741 [==============================] - 0s 262us/step - loss: 0.4450 - acc: 0.8637\n",
      "Epoch 101/256\n",
      "741/741 [==============================] - 0s 260us/step - loss: 0.4476 - acc: 0.8596\n",
      "Epoch 102/256\n",
      "741/741 [==============================] - 0s 366us/step - loss: 0.4400 - acc: 0.8610\n",
      "Epoch 103/256\n",
      "741/741 [==============================] - 0s 275us/step - loss: 0.4529 - acc: 0.8637\n",
      "Epoch 104/256\n",
      "741/741 [==============================] - 0s 273us/step - loss: 0.4365 - acc: 0.8718\n",
      "Epoch 105/256\n",
      "741/741 [==============================] - 0s 273us/step - loss: 0.4355 - acc: 0.8664\n",
      "Epoch 106/256\n",
      "741/741 [==============================] - 0s 282us/step - loss: 0.4363 - acc: 0.8664\n",
      "Epoch 107/256\n",
      "741/741 [==============================] - 0s 341us/step - loss: 0.4473 - acc: 0.8596\n",
      "Epoch 108/256\n",
      "741/741 [==============================] - 0s 341us/step - loss: 0.4462 - acc: 0.8637\n",
      "Epoch 109/256\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4490 - acc: 0.8610\n",
      "Epoch 110/256\n",
      "741/741 [==============================] - 0s 282us/step - loss: 0.4414 - acc: 0.8623\n",
      "Epoch 111/256\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.4333 - acc: 0.8691\n",
      "Epoch 112/256\n",
      "741/741 [==============================] - 0s 266us/step - loss: 0.4337 - acc: 0.8677\n",
      "Epoch 113/256\n",
      "741/741 [==============================] - 0s 271us/step - loss: 0.4312 - acc: 0.8664\n",
      "Epoch 114/256\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4313 - acc: 0.8623\n",
      "Epoch 115/256\n",
      "741/741 [==============================] - 0s 271us/step - loss: 0.4388 - acc: 0.8610\n",
      "Epoch 116/256\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.4287 - acc: 0.8691\n",
      "Epoch 117/256\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4253 - acc: 0.8664\n",
      "Epoch 118/256\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4256 - acc: 0.8745\n",
      "Epoch 119/256\n",
      "741/741 [==============================] - 0s 283us/step - loss: 0.4278 - acc: 0.8610\n",
      "Epoch 120/256\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4260 - acc: 0.8718\n",
      "Epoch 121/256\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4734 - acc: 0.8529\n",
      "Epoch 122/256\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4240 - acc: 0.8731\n",
      "Epoch 123/256\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4210 - acc: 0.8731\n",
      "Epoch 124/256\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4177 - acc: 0.8745\n",
      "Epoch 125/256\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4346 - acc: 0.8596\n",
      "Epoch 126/256\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4248 - acc: 0.8704\n",
      "Epoch 127/256\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4193 - acc: 0.8758\n",
      "Epoch 128/256\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4146 - acc: 0.8758\n",
      "Epoch 129/256\n",
      "741/741 [==============================] - 0s 283us/step - loss: 0.4169 - acc: 0.8718\n",
      "Epoch 130/256\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4151 - acc: 0.8758\n",
      "Epoch 131/256\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4127 - acc: 0.8731\n",
      "Epoch 132/256\n",
      "741/741 [==============================] - 0s 285us/step - loss: 0.4136 - acc: 0.8704\n",
      "Epoch 133/256\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4103 - acc: 0.8745\n",
      "Epoch 134/256\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4083 - acc: 0.8758\n",
      "Epoch 135/256\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4128 - acc: 0.8745\n",
      "Epoch 136/256\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4134 - acc: 0.8745\n",
      "Epoch 137/256\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4117 - acc: 0.8745\n",
      "Epoch 138/256\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4165 - acc: 0.8704\n",
      "Epoch 139/256\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4206 - acc: 0.8664\n",
      "Epoch 140/256\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4128 - acc: 0.8718\n",
      "Epoch 141/256\n",
      "741/741 [==============================] - 0s 285us/step - loss: 0.4041 - acc: 0.8826\n",
      "Epoch 142/256\n",
      "741/741 [==============================] - 0s 279us/step - loss: 0.4031 - acc: 0.8758\n",
      "Epoch 143/256\n",
      "741/741 [==============================] - 0s 285us/step - loss: 0.4151 - acc: 0.8718\n",
      "Epoch 144/256\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4090 - acc: 0.8718\n",
      "Epoch 145/256\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4068 - acc: 0.8772\n",
      "Epoch 146/256\n",
      "741/741 [==============================] - 0s 285us/step - loss: 0.4037 - acc: 0.8731\n",
      "Epoch 147/256\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4054 - acc: 0.8677\n",
      "Epoch 148/256\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.4062 - acc: 0.8718\n",
      "Epoch 149/256\n",
      "741/741 [==============================] - 0s 282us/step - loss: 0.4038 - acc: 0.8731\n",
      "Epoch 150/256\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4038 - acc: 0.8785\n",
      "Epoch 151/256\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4002 - acc: 0.8799\n",
      "Epoch 152/256\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4019 - acc: 0.8664\n",
      "Epoch 153/256\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.3991 - acc: 0.8839\n",
      "Epoch 154/256\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.3997 - acc: 0.8772\n",
      "Epoch 155/256\n",
      "741/741 [==============================] - 0s 281us/step - loss: 0.3998 - acc: 0.8785\n",
      "Epoch 156/256\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3993 - acc: 0.8772\n",
      "Epoch 157/256\n",
      "741/741 [==============================] - 0s 285us/step - loss: 0.3934 - acc: 0.8785\n",
      "Epoch 158/256\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3963 - acc: 0.8745\n",
      "Epoch 159/256\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.3987 - acc: 0.8758\n",
      "Epoch 160/256\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3994 - acc: 0.8718\n",
      "Epoch 161/256\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4001 - acc: 0.8731\n",
      "Epoch 162/256\n",
      "741/741 [==============================] - 0s 285us/step - loss: 0.3982 - acc: 0.8772\n",
      "Epoch 163/256\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3934 - acc: 0.8812\n",
      "Epoch 164/256\n",
      "741/741 [==============================] - 0s 283us/step - loss: 0.4040 - acc: 0.8677\n",
      "Epoch 165/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 287us/step - loss: 0.3950 - acc: 0.8772\n",
      "Epoch 166/256\n",
      "741/741 [==============================] - 0s 278us/step - loss: 0.3963 - acc: 0.8772\n",
      "Epoch 167/256\n",
      "741/741 [==============================] - 0s 278us/step - loss: 0.3992 - acc: 0.8745\n",
      "Epoch 168/256\n",
      "741/741 [==============================] - 0s 274us/step - loss: 0.4013 - acc: 0.8772\n",
      "Epoch 169/256\n",
      "741/741 [==============================] - 0s 282us/step - loss: 0.4000 - acc: 0.8731\n",
      "Epoch 170/256\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3906 - acc: 0.8785\n",
      "Epoch 171/256\n",
      "741/741 [==============================] - 0s 282us/step - loss: 0.3944 - acc: 0.8704\n",
      "Epoch 172/256\n",
      "741/741 [==============================] - 0s 281us/step - loss: 0.3899 - acc: 0.8799\n",
      "Epoch 173/256\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.3864 - acc: 0.8826\n",
      "Epoch 174/256\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3969 - acc: 0.8731\n",
      "Epoch 175/256\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3883 - acc: 0.8826\n",
      "Epoch 176/256\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4053 - acc: 0.8650\n",
      "Epoch 177/256\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3938 - acc: 0.8745\n",
      "Epoch 178/256\n",
      "741/741 [==============================] - 0s 283us/step - loss: 0.3873 - acc: 0.8772\n",
      "Epoch 179/256\n",
      "741/741 [==============================] - 0s 278us/step - loss: 0.3936 - acc: 0.8691\n",
      "Epoch 180/256\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.3907 - acc: 0.8812\n",
      "Epoch 181/256\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.3874 - acc: 0.8812\n",
      "Epoch 182/256\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.3866 - acc: 0.8745\n",
      "Epoch 183/256\n",
      "741/741 [==============================] - 0s 333us/step - loss: 0.3946 - acc: 0.8745\n",
      "Epoch 184/256\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3900 - acc: 0.8731\n",
      "Epoch 185/256\n",
      "741/741 [==============================] - 0s 283us/step - loss: 0.3869 - acc: 0.8853\n",
      "Epoch 186/256\n",
      "741/741 [==============================] - 0s 282us/step - loss: 0.3881 - acc: 0.8785\n",
      "Epoch 187/256\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3886 - acc: 0.8731\n",
      "Epoch 188/256\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.3995 - acc: 0.8731\n",
      "Epoch 189/256\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3909 - acc: 0.8704\n",
      "Epoch 190/256\n",
      "741/741 [==============================] - 0s 275us/step - loss: 0.3832 - acc: 0.8772\n",
      "Epoch 191/256\n",
      "741/741 [==============================] - 0s 282us/step - loss: 0.3907 - acc: 0.8799\n",
      "Epoch 192/256\n",
      "741/741 [==============================] - 0s 282us/step - loss: 0.3827 - acc: 0.8799\n",
      "Epoch 193/256\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.3875 - acc: 0.8745\n",
      "Epoch 194/256\n",
      "741/741 [==============================] - 0s 281us/step - loss: 0.3852 - acc: 0.8745\n",
      "Epoch 195/256\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3852 - acc: 0.8772\n",
      "Epoch 196/256\n",
      "741/741 [==============================] - 0s 285us/step - loss: 0.3814 - acc: 0.8839\n",
      "Epoch 197/256\n",
      "741/741 [==============================] - 0s 277us/step - loss: 0.3822 - acc: 0.8826\n",
      "Epoch 198/256\n",
      "741/741 [==============================] - 0s 279us/step - loss: 0.3875 - acc: 0.8704\n",
      "Epoch 199/256\n",
      "741/741 [==============================] - 0s 281us/step - loss: 0.3935 - acc: 0.8664\n",
      "Epoch 200/256\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3805 - acc: 0.8812\n",
      "Epoch 201/256\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3801 - acc: 0.8731\n",
      "Epoch 202/256\n",
      "741/741 [==============================] - 0s 278us/step - loss: 0.3803 - acc: 0.8745\n",
      "Epoch 203/256\n",
      "741/741 [==============================] - 0s 283us/step - loss: 0.3792 - acc: 0.8785\n",
      "Epoch 204/256\n",
      "741/741 [==============================] - 0s 279us/step - loss: 0.3869 - acc: 0.8718\n",
      "Epoch 205/256\n",
      "741/741 [==============================] - 0s 281us/step - loss: 0.3819 - acc: 0.8826\n",
      "Epoch 206/256\n",
      "741/741 [==============================] - 0s 282us/step - loss: 0.3845 - acc: 0.8799\n",
      "Epoch 207/256\n",
      "741/741 [==============================] - 0s 282us/step - loss: 0.3790 - acc: 0.8772\n",
      "Epoch 208/256\n",
      "741/741 [==============================] - 0s 279us/step - loss: 0.3792 - acc: 0.8826\n",
      "Epoch 209/256\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3831 - acc: 0.8731\n",
      "Epoch 210/256\n",
      "741/741 [==============================] - 0s 301us/step - loss: 0.3917 - acc: 0.8718\n",
      "Epoch 211/256\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3825 - acc: 0.8772\n",
      "Epoch 212/256\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3867 - acc: 0.8731\n",
      "Epoch 213/256\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3769 - acc: 0.8785\n",
      "Epoch 214/256\n",
      "741/741 [==============================] - 0s 282us/step - loss: 0.3777 - acc: 0.8758\n",
      "Epoch 215/256\n",
      "741/741 [==============================] - 0s 278us/step - loss: 0.3793 - acc: 0.8826\n",
      "Epoch 216/256\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3814 - acc: 0.8758\n",
      "Epoch 217/256\n",
      "741/741 [==============================] - 0s 277us/step - loss: 0.3843 - acc: 0.8772\n",
      "Epoch 218/256\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3811 - acc: 0.8799\n",
      "Epoch 219/256\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3810 - acc: 0.8839\n",
      "Epoch 220/256\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3764 - acc: 0.8745\n",
      "Epoch 221/256\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3764 - acc: 0.8731\n",
      "Epoch 222/256\n",
      "741/741 [==============================] - 0s 283us/step - loss: 0.3795 - acc: 0.8799\n",
      "Epoch 223/256\n",
      "741/741 [==============================] - 0s 281us/step - loss: 0.3854 - acc: 0.8758\n",
      "Epoch 224/256\n",
      "741/741 [==============================] - 0s 285us/step - loss: 0.3802 - acc: 0.8799\n",
      "Epoch 225/256\n",
      "741/741 [==============================] - 0s 281us/step - loss: 0.3824 - acc: 0.8731\n",
      "Epoch 226/256\n",
      "741/741 [==============================] - 0s 281us/step - loss: 0.3797 - acc: 0.8731\n",
      "Epoch 227/256\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3775 - acc: 0.8826\n",
      "Epoch 228/256\n",
      "741/741 [==============================] - 0s 285us/step - loss: 0.3819 - acc: 0.8812\n",
      "Epoch 229/256\n",
      "741/741 [==============================] - 0s 282us/step - loss: 0.3785 - acc: 0.8785\n",
      "Epoch 230/256\n",
      "741/741 [==============================] - 0s 283us/step - loss: 0.3795 - acc: 0.8799\n",
      "Epoch 231/256\n",
      "741/741 [==============================] - 0s 285us/step - loss: 0.3953 - acc: 0.8718\n",
      "Epoch 232/256\n",
      "741/741 [==============================] - 0s 282us/step - loss: 0.3837 - acc: 0.8745\n",
      "Epoch 233/256\n",
      "741/741 [==============================] - 0s 278us/step - loss: 0.3733 - acc: 0.8745\n",
      "Epoch 234/256\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3806 - acc: 0.8785\n",
      "Epoch 235/256\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3745 - acc: 0.8826\n",
      "Epoch 236/256\n",
      "741/741 [==============================] - 0s 282us/step - loss: 0.3861 - acc: 0.8704\n",
      "Epoch 237/256\n",
      "741/741 [==============================] - 0s 279us/step - loss: 0.3845 - acc: 0.8731\n",
      "Epoch 238/256\n",
      "741/741 [==============================] - 0s 278us/step - loss: 0.3914 - acc: 0.8691\n",
      "Epoch 239/256\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3774 - acc: 0.8758\n",
      "Epoch 240/256\n",
      "741/741 [==============================] - 0s 285us/step - loss: 0.3766 - acc: 0.8731\n",
      "Epoch 241/256\n",
      "741/741 [==============================] - 0s 282us/step - loss: 0.3749 - acc: 0.8758\n",
      "Epoch 242/256\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3831 - acc: 0.8650\n",
      "Epoch 243/256\n",
      "741/741 [==============================] - 0s 277us/step - loss: 0.3737 - acc: 0.8799\n",
      "Epoch 244/256\n",
      "741/741 [==============================] - 0s 283us/step - loss: 0.3813 - acc: 0.8731\n",
      "Epoch 245/256\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3768 - acc: 0.8812\n",
      "Epoch 246/256\n",
      "741/741 [==============================] - 0s 279us/step - loss: 0.3752 - acc: 0.8758\n",
      "Epoch 247/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 271us/step - loss: 0.3702 - acc: 0.8799\n",
      "Epoch 248/256\n",
      "741/741 [==============================] - 0s 282us/step - loss: 0.3761 - acc: 0.8704\n",
      "Epoch 249/256\n",
      "741/741 [==============================] - 0s 282us/step - loss: 0.3755 - acc: 0.8799\n",
      "Epoch 250/256\n",
      "741/741 [==============================] - 0s 285us/step - loss: 0.3772 - acc: 0.8758\n",
      "Epoch 251/256\n",
      "741/741 [==============================] - 0s 281us/step - loss: 0.3763 - acc: 0.8718\n",
      "Epoch 252/256\n",
      "741/741 [==============================] - 0s 283us/step - loss: 0.3777 - acc: 0.8812\n",
      "Epoch 253/256\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3890 - acc: 0.8677\n",
      "Epoch 254/256\n",
      "741/741 [==============================] - 0s 278us/step - loss: 0.3724 - acc: 0.8799\n",
      "Epoch 255/256\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3709 - acc: 0.8758\n",
      "Epoch 256/256\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.3974 - acc: 0.8691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x347cd748>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = NN_model_v2((X_Train.shape[1], ), layers, regularizers.l2(0.01), None)\n",
    "test_model.compile(optimizer = \"Adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "test_model.fit(x = X_Train, y = Y_Train, epochs = 256, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So having done a lot more testing I have found that the F1 score is a much better indicator of the probable accuracy on the public test data than the cross validation accuracy.  So lets have a look at the confusion matrices once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = test_model.predict(x = X_Train)\n",
    "cv_pred = test_model.predict(x = X_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hat = normalize_predictions(train_pred)\n",
    "cv_hat = normalize_predictions(cv_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  87.17948717948718\n",
      "F1 Score =  0.8217636022514071\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "      <th>Actual True</th>\n",
       "      <th>Actual False</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pred True</td>\n",
       "      <td>219.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pred False</td>\n",
       "      <td>72.0</td>\n",
       "      <td>427.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Labels  Actual True  Actual False\n",
       "0   Pred True        219.0          23.0\n",
       "1  Pred False         72.0         427.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc1, score1, conf1 = Calc_Accuracy(Y_Train, train_hat)\n",
    "\n",
    "print(\"Accuracy = \", acc1)\n",
    "print(\"F1 Score = \", score1)\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix\")\n",
    "conf1[[\"Labels\", \"Actual True\", \"Actual False\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  80.0\n",
      "F1 Score =  0.7058823529411765\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "      <th>Actual True</th>\n",
       "      <th>Actual False</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pred True</td>\n",
       "      <td>36.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pred False</td>\n",
       "      <td>15.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Labels  Actual True  Actual False\n",
       "0   Pred True         36.0          15.0\n",
       "1  Pred False         15.0          84.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc2, score2, conf2 = Calc_Accuracy(Y_CV, cv_hat)\n",
    "\n",
    "print(\"Accuracy = \", acc2)\n",
    "print(\"F1 Score = \", score2)\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix\")\n",
    "conf2[[\"Labels\", \"Actual True\", \"Actual False\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cells just contain the code to output a prediction set for upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = test_model.predict(x = X_Test)\n",
    "\n",
    "test_hat = normalize_predictions(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = Create_output_frame(sub_set, test_hat)\n",
    "sub_df.to_csv(\"Predictions.csv\", index=False, float_format='%1d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And unfortunately with the best performing models I still had approximately the same performance as from the first upload and I could only match the previous best.  \n",
    "\n",
    "After doing a LOT of testing with different features/architectures/regularization/training iterations there's not yet a satisfactory neural network model for this data.\n",
    "\n",
    "So one area for improving models is to get more training data.  As its impossible (and immoral) to go back in time and sink another titanic, instead lets generate our own training data by using a Generative Adversarial Network (GAN).\n",
    "\n",
    "## Generative Adversarial Network\n",
    "\n",
    "This is the first ever GAN I have made and is based off of the template model given  - https://deeplearning4j.org/generative-adversarial-network\n",
    "\n",
    "So the idea is to train two networks, one to generate training data given gaussian noise input and another to spot fake data, trained from both real data and fake generated data.\n",
    "\n",
    "The idea being our generator gets good enough to fool our discriminator and thus can generate fake training data which we can then add into our training data pool.\n",
    "\n",
    "So firstly lets transform our data into a format that we can use to train a GAN.  Fortunately we already have this almost entirely done with the earlier appended dataset, the only thing we realistically need to do is to fill in the NaN values in the Survived column with a dummy variable. \n",
    "\n",
    "This isn't ideal as in theory any fake data will have a skewed survival rate, however we can apply a simple rounding function afterward to the fake data, and as both real and fake data are assumed to come from the same distribution overall this should not affect our generated data distribution too negatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_train = clean_set\n",
    "gan_train['Survived'] = gan_train.Survived.fillna(value=0.5)\n",
    "actual_gan = gan_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have our data lets build our GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self, num_features):\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', \n",
    "                                   optimizer=optimizer,\n",
    "                                    metrics=['accuracy'])\n",
    "\n",
    "        # Build and compile the generator\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(num_features,))\n",
    "        fake = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = True\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(fake)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator) takes\n",
    "        # noise as input => generates images => determines validity \n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "        \n",
    "    def build_generator(self):\n",
    "\n",
    "        noise_shape = (self.num_features,)\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(5 ,activation='relu' , input_shape=noise_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(7 ,activation='relu' ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(9 ,activation='relu' ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(12 ,activation='relu' ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(17 ,activation='relu' ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(self.num_features))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(np.prod(noise_shape), activation='sigmoid'))\n",
    "        model.add(Reshape(noise_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=noise_shape)\n",
    "        fake = model(noise)\n",
    "\n",
    "        return Model(noise, fake)\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "\n",
    "        fake_shape = (self.num_features,)\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(17,activation='relu', input_shape=fake_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(12,activation='relu' ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(9 ,activation='relu' ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(7 ,activation='relu' ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(5 ,activation='relu' ))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Dense(1 ,activation='sigmoid' ))\n",
    " \n",
    "        model.summary()\n",
    "\n",
    "        out = Input(shape=fake_shape)\n",
    "        validity = model(out)\n",
    "\n",
    "        return Model(out, validity)\n",
    "    \n",
    "    def train(self, X_train, epochs, batch_size=128, interval = 100):\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            reals = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (half_batch, X_train.shape[1]))\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_fakes = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(reals, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_fakes, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, X_train.shape[1]))\n",
    "\n",
    "            # The generator wants the discriminator to label the generated samples\n",
    "            # as valid (ones)\n",
    "            valid_y = np.array([1] * batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "            # Plot the progress\n",
    "            if epoch % interval == 0:\n",
    "                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "    \n",
    "    #Generate Training Examples      \n",
    "    def Generate_data(self, num_examples):\n",
    "        noise = np.random.normal(0, 1, (num_examples, self.num_features))\n",
    "        gen_fakes = self.generator.predict(noise)\n",
    "        \n",
    "        discrim_preds = self.discriminator.predict(gen_fakes)\n",
    "        return gen_fakes, discrim_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to compile the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 17)                442       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 17)                68        \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 12)                216       \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 12)                48        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 9)                 117       \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 9)                 36        \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 7)                 70        \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 7)                 28        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 5)                 40        \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 5)                 20        \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 1,091\n",
      "Trainable params: 991\n",
      "Non-trainable params: 100\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 5)                 130       \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 5)                 20        \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 7)                 42        \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 7)                 28        \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 9)                 72        \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 9)                 36        \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 12)                120       \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 12)                48        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 17)                221       \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 17)                68        \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 25)                450       \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 25)                100       \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 25)                0         \n",
      "=================================================================\n",
      "Total params: 1,985\n",
      "Trainable params: 1,835\n",
      "Non-trainable params: 150\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan = GAN(actual_gan.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to train the GAN (note depending on parameters this may take a LONG time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.724442, acc.: 50.78%] [G loss: 0.748606]\n",
      "100 [D loss: 0.695665, acc.: 49.22%] [G loss: 0.646381]\n",
      "200 [D loss: 0.700983, acc.: 49.22%] [G loss: 0.603774]\n",
      "300 [D loss: 0.703950, acc.: 49.22%] [G loss: 0.564096]\n",
      "400 [D loss: 0.710166, acc.: 50.00%] [G loss: 0.528174]\n",
      "500 [D loss: 0.718502, acc.: 50.00%] [G loss: 0.496352]\n",
      "600 [D loss: 0.726243, acc.: 50.00%] [G loss: 0.468353]\n",
      "700 [D loss: 0.735141, acc.: 50.00%] [G loss: 0.444457]\n",
      "800 [D loss: 0.744017, acc.: 50.00%] [G loss: 0.424114]\n",
      "900 [D loss: 0.751790, acc.: 50.00%] [G loss: 0.406614]\n",
      "1000 [D loss: 0.760143, acc.: 50.00%] [G loss: 0.391569]\n",
      "1100 [D loss: 0.766845, acc.: 50.00%] [G loss: 0.378735]\n",
      "1200 [D loss: 0.773553, acc.: 50.00%] [G loss: 0.367536]\n",
      "1300 [D loss: 0.779846, acc.: 50.00%] [G loss: 0.357813]\n",
      "1400 [D loss: 0.785583, acc.: 50.00%] [G loss: 0.349296]\n",
      "1500 [D loss: 0.790930, acc.: 50.00%] [G loss: 0.341772]\n",
      "1600 [D loss: 0.795907, acc.: 50.00%] [G loss: 0.335132]\n",
      "1700 [D loss: 0.800403, acc.: 50.00%] [G loss: 0.329236]\n",
      "1800 [D loss: 0.804625, acc.: 50.00%] [G loss: 0.323934]\n",
      "1900 [D loss: 0.808538, acc.: 50.00%] [G loss: 0.319170]\n",
      "2000 [D loss: 0.812171, acc.: 50.00%] [G loss: 0.314805]\n",
      "2100 [D loss: 0.815561, acc.: 50.00%] [G loss: 0.310883]\n",
      "2200 [D loss: 0.818704, acc.: 50.00%] [G loss: 0.307306]\n",
      "2300 [D loss: 0.821620, acc.: 50.00%] [G loss: 0.304026]\n",
      "2400 [D loss: 0.824391, acc.: 50.00%] [G loss: 0.301016]\n",
      "2500 [D loss: 0.826993, acc.: 50.00%] [G loss: 0.298233]\n",
      "2600 [D loss: 0.829425, acc.: 50.00%] [G loss: 0.295656]\n",
      "2700 [D loss: 0.831724, acc.: 50.00%] [G loss: 0.293257]\n",
      "2800 [D loss: 0.833898, acc.: 50.00%] [G loss: 0.291022]\n",
      "2900 [D loss: 0.835953, acc.: 50.00%] [G loss: 0.288942]\n",
      "3000 [D loss: 0.837919, acc.: 50.00%] [G loss: 0.286978]\n",
      "3100 [D loss: 0.839772, acc.: 50.00%] [G loss: 0.285145]\n",
      "3200 [D loss: 0.841533, acc.: 50.00%] [G loss: 0.283415]\n",
      "3300 [D loss: 0.843221, acc.: 50.00%] [G loss: 0.281776]\n",
      "3400 [D loss: 0.844824, acc.: 50.00%] [G loss: 0.280231]\n",
      "3500 [D loss: 0.846364, acc.: 50.00%] [G loss: 0.278768]\n",
      "3600 [D loss: 0.847839, acc.: 50.00%] [G loss: 0.277374]\n",
      "3700 [D loss: 0.849257, acc.: 50.00%] [G loss: 0.276052]\n",
      "3800 [D loss: 0.850613, acc.: 50.00%] [G loss: 0.274793]\n",
      "3900 [D loss: 0.851919, acc.: 50.00%] [G loss: 0.273595]\n",
      "4000 [D loss: 0.853180, acc.: 50.00%] [G loss: 0.272440]\n",
      "4100 [D loss: 0.854385, acc.: 50.00%] [G loss: 0.271343]\n",
      "4200 [D loss: 0.855549, acc.: 50.00%] [G loss: 0.270293]\n",
      "4300 [D loss: 0.856671, acc.: 50.00%] [G loss: 0.269280]\n",
      "4400 [D loss: 0.857759, acc.: 50.00%] [G loss: 0.268313]\n",
      "4500 [D loss: 0.858808, acc.: 50.00%] [G loss: 0.267383]\n",
      "4600 [D loss: 0.859822, acc.: 50.00%] [G loss: 0.266490]\n",
      "4700 [D loss: 0.860804, acc.: 50.00%] [G loss: 0.265630]\n",
      "4800 [D loss: 0.861754, acc.: 50.00%] [G loss: 0.264803]\n",
      "4900 [D loss: 0.862674, acc.: 50.00%] [G loss: 0.264004]\n"
     ]
    }
   ],
   "source": [
    "gan.train(actual_gan, epochs=5000, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a sample of generated data to see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data, predicted = gan.Generate_data(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5803122 , 0.6479198 , 0.5143897 , 0.27804846, 0.5138869 ,\n",
       "       0.36106023, 0.4686081 , 0.71719   , 0.1788259 , 0.8167821 ,\n",
       "       0.829268  , 0.453935  , 0.51976174, 0.5388331 , 0.5564493 ,\n",
       "       0.5708552 , 0.37824744, 0.2602682 , 0.54370034, 0.5598483 ,\n",
       "       0.69468004, 0.32086805, 0.57113135, 0.27646056, 0.7292193 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7684363], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the generated data looks absoloutely nothing like our real data.  This is somewhat dissapointing, however maybe if the categorical fields were rounded to the nearest integer it could actually be valuable. \n",
    "\n",
    "\n",
    "So lets clean the data up a bit and train a model on GAN data and use our training data as a cross validation set.\n",
    "\n",
    "Of course we will also have to extract our fake X and Y sets for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_fake_data(fake_data):\n",
    "    fake_Y = np.around(fake_data[:, 2])\n",
    "    fake_X = fake_data\n",
    "    fake_X = np.delete(fake_X, [2], axis=1)\n",
    "    \n",
    "    fake_X[:, (0, 1, 2)] = np.around(fake_X[:, (0, 1, 2)])\n",
    "    fake_X[:, 5:] = np.around(fake_X[:, 5:])\n",
    "    \n",
    "    return fake_X, fake_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets now segment our fake data and train a model based upon this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_X, fake_Y = segment_fake_data(fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 0.        , 0.5138869 , 0.36106023,\n",
       "       0.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "       0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "       0.        , 1.        , 0.        , 1.        ], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_X[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So by the looks of it there are a few too many ones in our one-hot vector so the GAN did likely not learn the relations between one-hot vectors.\n",
    "\n",
    "Well lets build a model and test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [17, 12, 9, 7, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 4.5374 - acc: 0.6050A: 0s - loss: 4.9902 - acc: 0.5\n",
      "Epoch 2/64\n",
      "1000/1000 [==============================] - 0s 493us/step - loss: 2.7871 - acc: 0.6710\n",
      "Epoch 3/64\n",
      "1000/1000 [==============================] - 1s 535us/step - loss: 2.0420 - acc: 0.7240\n",
      "Epoch 4/64\n",
      "1000/1000 [==============================] - 0s 498us/step - loss: 1.6193 - acc: 0.7490\n",
      "Epoch 5/64\n",
      "1000/1000 [==============================] - 0s 491us/step - loss: 1.3802 - acc: 0.7600\n",
      "Epoch 6/64\n",
      "1000/1000 [==============================] - 0s 497us/step - loss: 1.1841 - acc: 0.7750\n",
      "Epoch 7/64\n",
      "1000/1000 [==============================] - 0s 493us/step - loss: 1.0503 - acc: 0.7960\n",
      "Epoch 8/64\n",
      "1000/1000 [==============================] - 1s 501us/step - loss: 0.9609 - acc: 0.8040 0s - loss: 0.9706 - acc: 0.805\n",
      "Epoch 9/64\n",
      "1000/1000 [==============================] - 1s 511us/step - loss: 0.9393 - acc: 0.7670\n",
      "Epoch 10/64\n",
      "1000/1000 [==============================] - 1s 513us/step - loss: 0.8580 - acc: 0.8070\n",
      "Epoch 11/64\n",
      "1000/1000 [==============================] - 1s 532us/step - loss: 0.7956 - acc: 0.8300\n",
      "Epoch 12/64\n",
      "1000/1000 [==============================] - 1s 513us/step - loss: 0.7628 - acc: 0.8330\n",
      "Epoch 13/64\n",
      "1000/1000 [==============================] - 1s 507us/step - loss: 0.7033 - acc: 0.8610\n",
      "Epoch 14/64\n",
      "1000/1000 [==============================] - 1s 501us/step - loss: 0.6668 - acc: 0.8380\n",
      "Epoch 15/64\n",
      "1000/1000 [==============================] - 0s 492us/step - loss: 0.6318 - acc: 0.8610\n",
      "Epoch 16/64\n",
      "1000/1000 [==============================] - 1s 501us/step - loss: 0.6910 - acc: 0.8210\n",
      "Epoch 17/64\n",
      "1000/1000 [==============================] - 0s 496us/step - loss: 0.6197 - acc: 0.8430\n",
      "Epoch 18/64\n",
      "1000/1000 [==============================] - 1s 512us/step - loss: 0.5937 - acc: 0.8510\n",
      "Epoch 19/64\n",
      "1000/1000 [==============================] - 1s 506us/step - loss: 0.5527 - acc: 0.9030\n",
      "Epoch 20/64\n",
      "1000/1000 [==============================] - 1s 507us/step - loss: 0.5276 - acc: 0.9100\n",
      "Epoch 21/64\n",
      "1000/1000 [==============================] - 1s 507us/step - loss: 0.4907 - acc: 0.9070\n",
      "Epoch 22/64\n",
      "1000/1000 [==============================] - 0s 495us/step - loss: 0.4667 - acc: 0.9010\n",
      "Epoch 23/64\n",
      "1000/1000 [==============================] - 1s 502us/step - loss: 0.4406 - acc: 0.9280\n",
      "Epoch 24/64\n",
      "1000/1000 [==============================] - 1s 502us/step - loss: 0.3982 - acc: 0.9360\n",
      "Epoch 25/64\n",
      "1000/1000 [==============================] - 0s 493us/step - loss: 0.3924 - acc: 0.9320\n",
      "Epoch 26/64\n",
      "1000/1000 [==============================] - 1s 501us/step - loss: 0.3834 - acc: 0.9320 0s - loss: 0.4131 - acc: 0.\n",
      "Epoch 27/64\n",
      "1000/1000 [==============================] - 0s 493us/step - loss: 0.4002 - acc: 0.9250\n",
      "Epoch 28/64\n",
      "1000/1000 [==============================] - 1s 502us/step - loss: 0.3813 - acc: 0.9280\n",
      "Epoch 29/64\n",
      "1000/1000 [==============================] - 1s 515us/step - loss: 0.3724 - acc: 0.9270\n",
      "Epoch 30/64\n",
      "1000/1000 [==============================] - 0s 497us/step - loss: 0.3720 - acc: 0.9260\n",
      "Epoch 31/64\n",
      "1000/1000 [==============================] - 1s 501us/step - loss: 0.3836 - acc: 0.9270\n",
      "Epoch 32/64\n",
      "1000/1000 [==============================] - 1s 509us/step - loss: 0.3595 - acc: 0.9400\n",
      "Epoch 33/64\n",
      "1000/1000 [==============================] - 1s 506us/step - loss: 0.3642 - acc: 0.9340\n",
      "Epoch 34/64\n",
      "1000/1000 [==============================] - 1s 527us/step - loss: 0.3015 - acc: 0.9590\n",
      "Epoch 35/64\n",
      "1000/1000 [==============================] - 1s 536us/step - loss: 0.3061 - acc: 0.9510\n",
      "Epoch 36/64\n",
      "1000/1000 [==============================] - 1s 508us/step - loss: 0.2877 - acc: 0.9520\n",
      "Epoch 37/64\n",
      "1000/1000 [==============================] - 1s 505us/step - loss: 0.3302 - acc: 0.9300\n",
      "Epoch 38/64\n",
      "1000/1000 [==============================] - 1s 501us/step - loss: 0.2968 - acc: 0.9430\n",
      "Epoch 39/64\n",
      "1000/1000 [==============================] - 1s 506us/step - loss: 0.2665 - acc: 0.9530\n",
      "Epoch 40/64\n",
      "1000/1000 [==============================] - 1s 500us/step - loss: 0.2687 - acc: 0.9530\n",
      "Epoch 41/64\n",
      "1000/1000 [==============================] - 1s 502us/step - loss: 0.2755 - acc: 0.9490\n",
      "Epoch 42/64\n",
      "1000/1000 [==============================] - 0s 494us/step - loss: 0.2591 - acc: 0.9470\n",
      "Epoch 43/64\n",
      "1000/1000 [==============================] - 0s 495us/step - loss: 0.2919 - acc: 0.9320\n",
      "Epoch 44/64\n",
      "1000/1000 [==============================] - 0s 496us/step - loss: 0.3579 - acc: 0.9070\n",
      "Epoch 45/64\n",
      "1000/1000 [==============================] - 0s 498us/step - loss: 0.3537 - acc: 0.9030\n",
      "Epoch 46/64\n",
      "1000/1000 [==============================] - 0s 496us/step - loss: 0.3309 - acc: 0.9140\n",
      "Epoch 47/64\n",
      "1000/1000 [==============================] - 1s 504us/step - loss: 0.2609 - acc: 0.9480\n",
      "Epoch 48/64\n",
      "1000/1000 [==============================] - 1s 506us/step - loss: 0.2638 - acc: 0.9450\n",
      "Epoch 49/64\n",
      "1000/1000 [==============================] - 1s 520us/step - loss: 0.2646 - acc: 0.9530\n",
      "Epoch 50/64\n",
      "1000/1000 [==============================] - 1s 508us/step - loss: 0.2545 - acc: 0.9390\n",
      "Epoch 51/64\n",
      "1000/1000 [==============================] - 0s 497us/step - loss: 0.2461 - acc: 0.9560\n",
      "Epoch 52/64\n",
      "1000/1000 [==============================] - 0s 495us/step - loss: 0.2324 - acc: 0.9550\n",
      "Epoch 53/64\n",
      "1000/1000 [==============================] - 0s 496us/step - loss: 0.2266 - acc: 0.9590\n",
      "Epoch 54/64\n",
      "1000/1000 [==============================] - 1s 504us/step - loss: 0.2038 - acc: 0.9600 0s - loss: 0.1910 - acc: \n",
      "Epoch 55/64\n",
      "1000/1000 [==============================] - 0s 497us/step - loss: 0.2144 - acc: 0.9540\n",
      "Epoch 56/64\n",
      "1000/1000 [==============================] - 1s 500us/step - loss: 0.2186 - acc: 0.9480\n",
      "Epoch 57/64\n",
      "1000/1000 [==============================] - 0s 493us/step - loss: 0.1915 - acc: 0.9680\n",
      "Epoch 58/64\n",
      "1000/1000 [==============================] - 1s 501us/step - loss: 0.2013 - acc: 0.9640\n",
      "Epoch 59/64\n",
      "1000/1000 [==============================] - 0s 494us/step - loss: 0.2165 - acc: 0.9570\n",
      "Epoch 60/64\n",
      "1000/1000 [==============================] - 1s 508us/step - loss: 0.2196 - acc: 0.9580\n",
      "Epoch 61/64\n",
      "1000/1000 [==============================] - 1s 513us/step - loss: 0.1867 - acc: 0.9610\n",
      "Epoch 62/64\n",
      "1000/1000 [==============================] - 1s 509us/step - loss: 0.2558 - acc: 0.9340\n",
      "Epoch 63/64\n",
      "1000/1000 [==============================] - 0s 496us/step - loss: 0.2170 - acc: 0.9490 0s - loss: 0.2285 - acc: 0.94\n",
      "Epoch 64/64\n",
      "1000/1000 [==============================] - 1s 506us/step - loss: 0.2213 - acc: 0.9500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4659ab38>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = NN_model_v2((fake_X.shape[1], ), layers, regularizers.l2(0.01), None)\n",
    "test_model.compile(optimizer = \"Adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "test_model.fit(x = fake_X, y = fake_Y, epochs = 64, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = test_model.predict(x = X_Train)\n",
    "cv_pred = test_model.predict(x = X_CV)\n",
    "train_hat = normalize_predictions(train_pred)\n",
    "cv_hat = normalize_predictions(cv_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  51.956815114709855\n",
      "F1 Score =  0.34074074074074073\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "      <th>Actual True</th>\n",
       "      <th>Actual False</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pred True</td>\n",
       "      <td>92.0</td>\n",
       "      <td>293.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pred False</td>\n",
       "      <td>172.0</td>\n",
       "      <td>184.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Labels  Actual True  Actual False\n",
       "0   Pred True         92.0         293.0\n",
       "1  Pred False        172.0         184.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc1, score1, conf1 = Calc_Accuracy(Y_Train, train_hat)\n",
    "\n",
    "print(\"Accuracy = \", acc1)\n",
    "print(\"F1 Score = \", score1)\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix\")\n",
    "conf1[[\"Labels\", \"Actual True\", \"Actual False\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  51.33333333333333\n",
      "F1 Score =  0.34234234234234234\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "      <th>Actual True</th>\n",
       "      <th>Actual False</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pred True</td>\n",
       "      <td>19.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pred False</td>\n",
       "      <td>26.0</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Labels  Actual True  Actual False\n",
       "0   Pred True         19.0          58.0\n",
       "1  Pred False         26.0          47.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc2, score2, conf2 = Calc_Accuracy(Y_CV, cv_hat)\n",
    "\n",
    "print(\"Accuracy = \", acc2)\n",
    "print(\"F1 Score = \", score2)\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix\")\n",
    "conf2[[\"Labels\", \"Actual True\", \"Actual False\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as expected our performance was pretty abysmal.  \n",
    "\n",
    "This worked about as well as randomly guessing unfortunately.  \n",
    "\n",
    "However this experience was valuable in and of itself as I got to play around with a GAN and understand the principles involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why did it Fail?\n",
    "\n",
    "This section is a bit of speculation based off the theory and intuitions about how GAN's work.\n",
    "\n",
    "We know GAN's are very effective at generating image data, (see https://arxiv.org/abs/1611.01331 for example), but it failed spectacularly in this test.  I can think of two primary reasons for this - \n",
    "* Orthogonalization of features\n",
    "* Convolutional Layers\n",
    "\n",
    "### Orthogonalization of features\n",
    "\n",
    "While in this type of binary classification having features that are completely orthogonal provides optimal performance, however image data does not contain orthogonal features, in fact pixels are generally assumed to have some relationship with nearby pixels, whether it be to define a line or tone shift in the image.  \n",
    "\n",
    "Due to the normalizing effects of going deeper through a network this would make it very difficult for a network to learn this absoloute orthogonalization, but would make it much easier to learn relationships between nearby pixels, especially with convolutional layers.\n",
    "\n",
    "### Convolutional Layers\n",
    "\n",
    "So this is a bit speculative having yet to use a convolutional GAN on image data, but I suspect that the filters learned in convolutional layers, when applied to gaussian noise effectively imprint features onto the image and build up a clearer picture with more complex features as the image propogates deeper through the network.  All the gaussian noise would do is effectively act as a random selection for which visual features are applied and where, which when trained against another convolutional network,  the discriminator network would find it harder and harder to distinguish which features are from genuine images and which are imprinted onto gaussian noise as they would appear genuine to a human observer.\n",
    "\n",
    "Hence our data of a series of one hot vectors and a couple of normalized numeric fields would not be suited to generation in a GAN due to the lack of similarity between local columns and lack of convolutional layers to build up a more detailed picture the deeper through the network you go.\n",
    "\n",
    "*Note all of the above is pure speculation and may be entirely innaccurate*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
