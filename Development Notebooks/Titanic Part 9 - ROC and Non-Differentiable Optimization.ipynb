{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Classification - ROC and Non-Decomposable  Objective Optimization\n",
    "\n",
    "So the focus of this notebook is to learn what the ROC is and the AUCROC as a measure and how they are useful.  And then move on to implementing this as an approximate optimization metric for Keras based neural networks.\n",
    "\n",
    "Note the primary references for this are - \n",
    "\n",
    "https://arxiv.org/abs/1608.04802\n",
    "\n",
    "http://www.dataschool.io/roc-curves-and-auc-explained/\n",
    "\n",
    "Also to make things significantly simpler I will only be dealing with binary classification for this notebook although in future I may use this optimization objective for more advanced problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##### First importing some relevant packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Stop pandas from truncating output view\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "#Import Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "#Import Keras\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Dropout, Reshape, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "#Import mathematical functions\n",
    "from random import *\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Get regular expression package\n",
    "import re\n",
    "\n",
    "#Import  Scikit learn framework\n",
    "import sklearn as sk\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the functions built in previous parts\n",
    "from Titanic_Import import *\n",
    "\n",
    "full_set = pd.read_csv('D:/Datasets/Titanic/train.csv')\n",
    "sub_set = pd.read_csv('D:/Datasets/Titanic/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_set = full_set\n",
    "append_set = append_set.append([sub_set], ignore_index =True )\n",
    "clean_set = Cleanse_Data_v3(append_set)\n",
    "X_Train, Y_Train, X_CV, Y_CV, X_Test = dataset_splitter(clean_set, cv_size = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first thing I'm going to need is a couple of $\\hat{Y}$ vectors to compute the ROC curve.  So lets build a simple classifier, also I will be using the sci-kit learn implementation to make things a bit simpler to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scisvm = svm.SVC()\n",
    "scisvm.fit(X_Train, Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Resuts\n",
      "Accuracy =  81.47612156295224\n",
      "F1 Score =  0.7387755102040815\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True        181.0          48.0\n",
      "1  Pred False         80.0         382.0\n",
      "-------------------------------\n",
      "Cross Validation Resuts\n",
      "Accuracy =  90.0\n",
      "F1 Score =  0.8749999999999999\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True         70.0           9.0\n",
      "1  Pred False         11.0         110.0\n"
     ]
    }
   ],
   "source": [
    "yhat_train = scisvm.predict(X_Train)\n",
    "yhat_cv = scisvm.predict(X_CV)\n",
    "\n",
    "print('Training Resuts')  \n",
    "show_acc(Y_Train, yhat_train)\n",
    "print('-------------------------------')\n",
    "print('Cross Validation Resuts')  \n",
    "show_acc(Y_CV, yhat_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have our basic classifier, and a baseline of how the accuracy and F1 scores and confusion matrices look.  Lets have a look at the ROC as a metric then.\n",
    "\n",
    "Note I modified the code from \n",
    "\n",
    "https://www.kaggle.com/nicapotato/catboost-aggregate-features/code\n",
    "\n",
    "to use as a baseline for the syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Score :  0.790929341530785\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(Y_Train, yhat_train)\n",
    "score = roc_auc_score(Y_Train, yhat_train)\n",
    "\n",
    "plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC ' % (score))\n",
    "print('ROC Score : ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay this worked lets put it into a function and see how it did on Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Score :  0.8942836393816785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8942836393816785"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_ROC(Y, Y_hat) :\n",
    "    fpr, tpr, thresholds = roc_curve(Y, Y_hat)\n",
    "    score = roc_auc_score(Y, Y_hat)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC ' % (score))\n",
    "    print('ROC Score : ', score)\n",
    "    \n",
    "    return score\n",
    "\n",
    "show_ROC(Y_CV, yhat_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems like a reasonable way to visualize where data imbalances may cause errors in learning.\n",
    "\n",
    "## Creating an optimization metric in Keras\n",
    "\n",
    "So the next challenge will be to create a loss function to put into a keras metric to allow neural networks to optimize similar measures directly.\n",
    "\n",
    "So to achieve this I'll implement the algorithms laid out in - Scalable Learning of Non-Decomposable Objectives. <br>\n",
    "https://arxiv.org/abs/1608.04802\n",
    "\n",
    "The basic idea is to use some mathematical trickery to create an inequality boundary for precision (or recall) at a fixed recall (or precision).  \n",
    "\n",
    "This is achieved by splitting the loss function into positive and negative examples (eq. 1) and then applying another layer on top of the existing loss functions (eq. 5) and introducing another variable to optimize (eq. 6) - labelled as $\\lambda$ in the above paper.\n",
    "\n",
    "The resultant equation is then effectively an approximation of true/false positive rates which can be used to generate loss functions that optimize over approximations of the non-decomposable objectives.\n",
    "\n",
    "Sources - \n",
    "\n",
    "Simple F-Score implementation as keras metric (used for syntax) <br>\n",
    "https://gist.github.com/shadySource/fa287fc6448f84004284f5c086f59661/\n",
    "\n",
    "Tensorflow implementation in Keras (again used for syntax) <br>\n",
    "https://github.com/keras-team/keras/blob/master/keras/backend/tensorflow_backend.py\n",
    "\n",
    "Optimizing Non-decomposable Measures with Deep Networks <br>\n",
    "https://arxiv.org/abs/1802.00086\n",
    "\n",
    "## Metric Implementation in Keras\n",
    "\n",
    "As I've not used custom metrics in Keras before a logical place to start is to implement one of these metrics in keras directly to view the progress as it updates.\n",
    "\n",
    "So to this end I will generate a new neural-network generation function (for experimentation) as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model_v2(input_shape, layers, act_reg, ker_reg):\n",
    "    #Having dynamic input shape as I may do feature engineering later.\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    X = Dense(layers[0], input_dim=input_shape, activation='relu')(X_input)\n",
    "    #X = LeakyReLU()(X)\n",
    "    #X = BatchNormalization()(X)\n",
    "\n",
    "    #Our NN Layers\n",
    "    for i in range(len(layers) - 1):\n",
    "      X = Dense(layers[i + 1], activation='relu', activity_regularizer = act_reg, kernel_regularizer = ker_reg)(X)\n",
    "      #X = LeakyReLU()(X)\\\n",
    "      #X = BatchNormalization()(X)\n",
    "\n",
    "    \n",
    "    X = Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "    model = Model(inputs = X_input, outputs = X, name='Simple_model')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this created I can now work on a function for a new metric, for simplicity I will generate the F1 score as this is a metric I'm somewhat familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_F1_score(y_true, y_pred):\n",
    "    \n",
    "    pred = tf.round(y_pred)\n",
    "    y_pred_inv = tf.add(1.0, tf.multiply(-1.0, pred))\n",
    "    y_true_inv = tf.add(1.0, tf.multiply(-1.0, y_true))\n",
    "    \n",
    "    tp = tf.reduce_sum(tf.multiply(y_true, y_pred))\n",
    "    fp = tf.reduce_sum(tf.multiply(y_true_inv, y_pred))\n",
    "    fn = tf.reduce_sum(tf.multiply(y_true, y_pred_inv))\n",
    "    \n",
    "    prec =  tf.divide(tp, tf.add(tf.add(tp, fp), 0.0000000001))\n",
    "    rec = tf.divide(tp, tf.add(tf.add(tp, fn), 0.0000000001))\n",
    "    \n",
    "    score = tf.multiply(2.0, tf.divide(tf.multiply(prec, rec), tf.add(tf.add(prec, rec), 0.000000001)))\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to see how it works in a traditional network optimizing with a normal binary cross-entropy loss function to use as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [21, 14, 8, 5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "691/691 [==============================] - 6s 9ms/step - loss: 1.6691 - acc: 0.6223 - K_F1_score: 0.3450\n",
      "Epoch 2/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.9348 - acc: 0.6223 - K_F1_score: 0.3473\n",
      "Epoch 3/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.7747 - acc: 0.6223 - K_F1_score: 0.3469\n",
      "Epoch 4/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.7199 - acc: 0.6223 - K_F1_score: 0.3432\n",
      "Epoch 5/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.6979 - acc: 0.6223 - K_F1_score: 0.3358\n",
      "Epoch 6/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.6875 - acc: 0.6223 - K_F1_score: 0.3355\n",
      "Epoch 7/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.6814 - acc: 0.6223 - K_F1_score: 0.3352\n",
      "Epoch 8/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.6779 - acc: 0.6223 - K_F1_score: 0.3330\n",
      "Epoch 9/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.6752 - acc: 0.6223 - K_F1_score: 0.3312\n",
      "Epoch 10/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.6734 - acc: 0.6223 - K_F1_score: 0.3333\n",
      "Epoch 11/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.6717 - acc: 0.6223 - K_F1_score: 0.3318\n",
      "Epoch 12/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.6700 - acc: 0.6223 - K_F1_score: 0.3307\n",
      "Epoch 13/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.6684 - acc: 0.6223 - K_F1_score: 0.3303\n",
      "Epoch 14/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.6668 - acc: 0.6223 - K_F1_score: 0.3247\n",
      "Epoch 15/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.6645 - acc: 0.6223 - K_F1_score: 0.3328\n",
      "Epoch 16/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.6617 - acc: 0.6223 - K_F1_score: 0.3328\n",
      "Epoch 17/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.6588 - acc: 0.6223 - K_F1_score: 0.3323\n",
      "Epoch 18/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.6560 - acc: 0.6223 - K_F1_score: 0.3334\n",
      "Epoch 19/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.6526 - acc: 0.6223 - K_F1_score: 0.3336\n",
      "Epoch 20/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.6497 - acc: 0.6223 - K_F1_score: 0.3351\n",
      "Epoch 21/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.6472 - acc: 0.6223 - K_F1_score: 0.3330\n",
      "Epoch 22/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.6441 - acc: 0.6223 - K_F1_score: 0.3331\n",
      "Epoch 23/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.6405 - acc: 0.6223 - K_F1_score: 0.3347\n",
      "Epoch 24/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.6369 - acc: 0.6223 - K_F1_score: 0.3346\n",
      "Epoch 25/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.6339 - acc: 0.6223 - K_F1_score: 0.3322\n",
      "Epoch 26/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.6323 - acc: 0.6223 - K_F1_score: 0.3391\n",
      "Epoch 27/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.6277 - acc: 0.6223 - K_F1_score: 0.3395\n",
      "Epoch 28/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.6239 - acc: 0.6223 - K_F1_score: 0.3417\n",
      "Epoch 29/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.6210 - acc: 0.6223 - K_F1_score: 0.3419\n",
      "Epoch 30/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.6186 - acc: 0.6223 - K_F1_score: 0.3430\n",
      "Epoch 31/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.6140 - acc: 0.6223 - K_F1_score: 0.3436\n",
      "Epoch 32/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.6118 - acc: 0.6223 - K_F1_score: 0.3459\n",
      "Epoch 33/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 0.6088 - acc: 0.6223 - K_F1_score: 0.3501\n",
      "Epoch 34/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.6048 - acc: 0.6223 - K_F1_score: 0.3487\n",
      "Epoch 35/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.6024 - acc: 0.6223 - K_F1_score: 0.3515\n",
      "Epoch 36/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.5982 - acc: 0.6223 - K_F1_score: 0.3551\n",
      "Epoch 37/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.5933 - acc: 0.6237 - K_F1_score: 0.3607\n",
      "Epoch 38/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.5885 - acc: 0.6252 - K_F1_score: 0.3623\n",
      "Epoch 39/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.5865 - acc: 0.6440 - K_F1_score: 0.3719\n",
      "Epoch 40/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 0.5845 - acc: 0.6816 - K_F1_score: 0.3942\n",
      "Epoch 41/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.5796 - acc: 0.7598 - K_F1_score: 0.4529\n",
      "Epoch 42/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.5750 - acc: 0.7988 - K_F1_score: 0.4805\n",
      "Epoch 43/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 0.5717 - acc: 0.8133 - K_F1_score: 0.5019\n",
      "Epoch 44/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.5716 - acc: 0.8191 - K_F1_score: 0.5136\n",
      "Epoch 45/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.5649 - acc: 0.8321 - K_F1_score: 0.5219\n",
      "Epoch 46/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.5631 - acc: 0.8336 - K_F1_score: 0.5360\n",
      "Epoch 47/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 0.5648 - acc: 0.8350 - K_F1_score: 0.5458\n",
      "Epoch 48/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.5548 - acc: 0.8379 - K_F1_score: 0.5311\n",
      "Epoch 49/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.5519 - acc: 0.8452 - K_F1_score: 0.5482\n",
      "Epoch 50/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.5535 - acc: 0.8321 - K_F1_score: 0.5501\n",
      "Epoch 51/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.5480 - acc: 0.8394 - K_F1_score: 0.5548\n",
      "Epoch 52/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.5536 - acc: 0.8292 - K_F1_score: 0.5633\n",
      "Epoch 53/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.5436 - acc: 0.8394 - K_F1_score: 0.5772\n",
      "Epoch 54/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.5358 - acc: 0.8524 - K_F1_score: 0.5779\n",
      "Epoch 55/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 0.5335 - acc: 0.8437 - K_F1_score: 0.5674\n",
      "Epoch 56/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.5292 - acc: 0.8452 - K_F1_score: 0.5772\n",
      "Epoch 57/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.5349 - acc: 0.8365 - K_F1_score: 0.5747\n",
      "Epoch 58/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.5256 - acc: 0.8466 - K_F1_score: 0.5812\n",
      "Epoch 59/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.5299 - acc: 0.8466 - K_F1_score: 0.5664\n",
      "Epoch 60/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.5241 - acc: 0.8452 - K_F1_score: 0.5979\n",
      "Epoch 61/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.5181 - acc: 0.8538 - K_F1_score: 0.5937\n",
      "Epoch 62/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.5197 - acc: 0.8480 - K_F1_score: 0.5954\n",
      "Epoch 63/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.5149 - acc: 0.8553 - K_F1_score: 0.5934\n",
      "Epoch 64/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.5166 - acc: 0.8365 - K_F1_score: 0.5879\n",
      "Epoch 65/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.5099 - acc: 0.8509 - K_F1_score: 0.6066\n",
      "Epoch 66/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 0.5097 - acc: 0.8480 - K_F1_score: 0.6010\n",
      "Epoch 67/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.5065 - acc: 0.8567 - K_F1_score: 0.5998\n",
      "Epoch 68/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.5011 - acc: 0.8582 - K_F1_score: 0.6182\n",
      "Epoch 69/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 304us/step - loss: 0.5026 - acc: 0.8524 - K_F1_score: 0.6110\n",
      "Epoch 70/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.5019 - acc: 0.8625 - K_F1_score: 0.6169\n",
      "Epoch 71/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.5043 - acc: 0.8509 - K_F1_score: 0.6198\n",
      "Epoch 72/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4989 - acc: 0.8538 - K_F1_score: 0.6161\n",
      "Epoch 73/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4961 - acc: 0.8495 - K_F1_score: 0.6181\n",
      "Epoch 74/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4934 - acc: 0.8538 - K_F1_score: 0.6246\n",
      "Epoch 75/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 0.4874 - acc: 0.8625 - K_F1_score: 0.6378\n",
      "Epoch 76/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.4990 - acc: 0.8408 - K_F1_score: 0.6149\n",
      "Epoch 77/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4898 - acc: 0.8625 - K_F1_score: 0.6388\n",
      "Epoch 78/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4902 - acc: 0.8553 - K_F1_score: 0.6264\n",
      "Epoch 79/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4935 - acc: 0.8524 - K_F1_score: 0.6226\n",
      "Epoch 80/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.4832 - acc: 0.8654 - K_F1_score: 0.6326\n",
      "Epoch 81/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.4810 - acc: 0.8669 - K_F1_score: 0.6375\n",
      "Epoch 82/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4857 - acc: 0.8567 - K_F1_score: 0.6295\n",
      "Epoch 83/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4876 - acc: 0.8524 - K_F1_score: 0.6274\n",
      "Epoch 84/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.4800 - acc: 0.8669 - K_F1_score: 0.6510\n",
      "Epoch 85/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.4762 - acc: 0.8640 - K_F1_score: 0.6354\n",
      "Epoch 86/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4839 - acc: 0.8495 - K_F1_score: 0.6326\n",
      "Epoch 87/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4773 - acc: 0.8582 - K_F1_score: 0.6244\n",
      "Epoch 88/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4732 - acc: 0.8582 - K_F1_score: 0.6272\n",
      "Epoch 89/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 0.4727 - acc: 0.8669 - K_F1_score: 0.6460\n",
      "Epoch 90/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4789 - acc: 0.8538 - K_F1_score: 0.6409\n",
      "Epoch 91/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4694 - acc: 0.8611 - K_F1_score: 0.6452\n",
      "Epoch 92/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4692 - acc: 0.8611 - K_F1_score: 0.6431\n",
      "Epoch 93/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4754 - acc: 0.8567 - K_F1_score: 0.6370\n",
      "Epoch 94/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4673 - acc: 0.8611 - K_F1_score: 0.6405\n",
      "Epoch 95/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 0.4643 - acc: 0.8654 - K_F1_score: 0.6469\n",
      "Epoch 96/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4704 - acc: 0.8582 - K_F1_score: 0.6533\n",
      "Epoch 97/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4673 - acc: 0.8683 - K_F1_score: 0.6572\n",
      "Epoch 98/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4650 - acc: 0.8596 - K_F1_score: 0.6523\n",
      "Epoch 99/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4661 - acc: 0.8596 - K_F1_score: 0.6488\n",
      "Epoch 100/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4601 - acc: 0.8611 - K_F1_score: 0.6468\n",
      "Epoch 101/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4529 - acc: 0.8698 - K_F1_score: 0.6516\n",
      "Epoch 102/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4567 - acc: 0.8712 - K_F1_score: 0.6652\n",
      "Epoch 103/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4594 - acc: 0.8611 - K_F1_score: 0.6545\n",
      "Epoch 104/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4608 - acc: 0.8640 - K_F1_score: 0.6565\n",
      "Epoch 105/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4551 - acc: 0.8640 - K_F1_score: 0.6630\n",
      "Epoch 106/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4528 - acc: 0.8698 - K_F1_score: 0.6670\n",
      "Epoch 107/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4554 - acc: 0.8524 - K_F1_score: 0.6564\n",
      "Epoch 108/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4636 - acc: 0.8538 - K_F1_score: 0.6519\n",
      "Epoch 109/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4527 - acc: 0.8654 - K_F1_score: 0.6546\n",
      "Epoch 110/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4463 - acc: 0.8669 - K_F1_score: 0.6606\n",
      "Epoch 111/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4455 - acc: 0.8755 - K_F1_score: 0.6727\n",
      "Epoch 112/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4439 - acc: 0.8683 - K_F1_score: 0.6681\n",
      "Epoch 113/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4438 - acc: 0.8712 - K_F1_score: 0.6661\n",
      "Epoch 114/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4430 - acc: 0.8726 - K_F1_score: 0.6690\n",
      "Epoch 115/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4432 - acc: 0.8683 - K_F1_score: 0.6729\n",
      "Epoch 116/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4443 - acc: 0.8669 - K_F1_score: 0.6677\n",
      "Epoch 117/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4444 - acc: 0.8625 - K_F1_score: 0.6677\n",
      "Epoch 118/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4519 - acc: 0.8567 - K_F1_score: 0.6563\n",
      "Epoch 119/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 0.4454 - acc: 0.8712 - K_F1_score: 0.6764\n",
      "Epoch 120/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4600 - acc: 0.8553 - K_F1_score: 0.6585\n",
      "Epoch 121/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4504 - acc: 0.8611 - K_F1_score: 0.6521\n",
      "Epoch 122/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4438 - acc: 0.8640 - K_F1_score: 0.6678\n",
      "Epoch 123/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4449 - acc: 0.8611 - K_F1_score: 0.6675\n",
      "Epoch 124/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4378 - acc: 0.8726 - K_F1_score: 0.6753\n",
      "Epoch 125/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4359 - acc: 0.8726 - K_F1_score: 0.6796\n",
      "Epoch 126/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4337 - acc: 0.8698 - K_F1_score: 0.6791\n",
      "Epoch 127/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4310 - acc: 0.8770 - K_F1_score: 0.6865\n",
      "Epoch 128/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 0.4432 - acc: 0.8683 - K_F1_score: 0.6805\n",
      "Epoch 129/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4381 - acc: 0.8625 - K_F1_score: 0.6700\n",
      "Epoch 130/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4508 - acc: 0.8611 - K_F1_score: 0.6694\n",
      "Epoch 131/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.4446 - acc: 0.8683 - K_F1_score: 0.6781\n",
      "Epoch 132/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 0.4368 - acc: 0.8669 - K_F1_score: 0.6626\n",
      "Epoch 133/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4445 - acc: 0.8654 - K_F1_score: 0.6761\n",
      "Epoch 134/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4362 - acc: 0.8582 - K_F1_score: 0.6634\n",
      "Epoch 135/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 0.4288 - acc: 0.8712 - K_F1_score: 0.6797\n",
      "Epoch 136/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4274 - acc: 0.8726 - K_F1_score: 0.6823\n",
      "Epoch 137/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 289us/step - loss: 0.4269 - acc: 0.8799 - K_F1_score: 0.6788\n",
      "Epoch 138/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4248 - acc: 0.8741 - K_F1_score: 0.6881\n",
      "Epoch 139/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.4235 - acc: 0.8741 - K_F1_score: 0.6819\n",
      "Epoch 140/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4270 - acc: 0.8683 - K_F1_score: 0.6805\n",
      "Epoch 141/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4325 - acc: 0.8640 - K_F1_score: 0.6750\n",
      "Epoch 142/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.4271 - acc: 0.8755 - K_F1_score: 0.6784\n",
      "Epoch 143/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4235 - acc: 0.8698 - K_F1_score: 0.6822\n",
      "Epoch 144/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.4305 - acc: 0.8669 - K_F1_score: 0.6877\n",
      "Epoch 145/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.4226 - acc: 0.8726 - K_F1_score: 0.6956\n",
      "Epoch 146/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.4224 - acc: 0.8741 - K_F1_score: 0.6904\n",
      "Epoch 147/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4221 - acc: 0.8813 - K_F1_score: 0.6935\n",
      "Epoch 148/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4241 - acc: 0.8741 - K_F1_score: 0.6822\n",
      "Epoch 149/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.4241 - acc: 0.8726 - K_F1_score: 0.6833\n",
      "Epoch 150/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4323 - acc: 0.8596 - K_F1_score: 0.6676\n",
      "Epoch 151/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.4259 - acc: 0.8726 - K_F1_score: 0.6995\n",
      "Epoch 152/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4332 - acc: 0.8596 - K_F1_score: 0.6759\n",
      "Epoch 153/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4322 - acc: 0.8654 - K_F1_score: 0.6842\n",
      "Epoch 154/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.4207 - acc: 0.8698 - K_F1_score: 0.6804\n",
      "Epoch 155/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.4186 - acc: 0.8698 - K_F1_score: 0.6832\n",
      "Epoch 156/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4160 - acc: 0.8784 - K_F1_score: 0.7002\n",
      "Epoch 157/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4184 - acc: 0.8726 - K_F1_score: 0.6911\n",
      "Epoch 158/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4170 - acc: 0.8712 - K_F1_score: 0.6874\n",
      "Epoch 159/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.4335 - acc: 0.8596 - K_F1_score: 0.6869\n",
      "Epoch 160/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4237 - acc: 0.8683 - K_F1_score: 0.6916\n",
      "Epoch 161/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4214 - acc: 0.8712 - K_F1_score: 0.6951\n",
      "Epoch 162/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4255 - acc: 0.8640 - K_F1_score: 0.6672\n",
      "Epoch 163/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4124 - acc: 0.8698 - K_F1_score: 0.6941\n",
      "Epoch 164/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.4120 - acc: 0.8755 - K_F1_score: 0.6898\n",
      "Epoch 165/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4182 - acc: 0.8726 - K_F1_score: 0.6931\n",
      "Epoch 166/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4109 - acc: 0.8770 - K_F1_score: 0.6932\n",
      "Epoch 167/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4177 - acc: 0.8741 - K_F1_score: 0.6942\n",
      "Epoch 168/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4108 - acc: 0.8755 - K_F1_score: 0.7019\n",
      "Epoch 169/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4160 - acc: 0.8683 - K_F1_score: 0.6879\n",
      "Epoch 170/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4196 - acc: 0.8654 - K_F1_score: 0.6826\n",
      "Epoch 171/256\n",
      "691/691 [==============================] - 0s 362us/step - loss: 0.4104 - acc: 0.8726 - K_F1_score: 0.6986\n",
      "Epoch 172/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.4067 - acc: 0.8813 - K_F1_score: 0.7049\n",
      "Epoch 173/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 0.4097 - acc: 0.8726 - K_F1_score: 0.7018\n",
      "Epoch 174/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 0.4093 - acc: 0.8726 - K_F1_score: 0.6914\n",
      "Epoch 175/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4057 - acc: 0.8770 - K_F1_score: 0.7049\n",
      "Epoch 176/256\n",
      "691/691 [==============================] - 0s 391us/step - loss: 0.4079 - acc: 0.8755 - K_F1_score: 0.6950\n",
      "Epoch 177/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4041 - acc: 0.8813 - K_F1_score: 0.7039\n",
      "Epoch 178/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 0.4069 - acc: 0.8698 - K_F1_score: 0.6892\n",
      "Epoch 179/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4063 - acc: 0.8683 - K_F1_score: 0.7008\n",
      "Epoch 180/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4065 - acc: 0.8712 - K_F1_score: 0.6960\n",
      "Epoch 181/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 0.4037 - acc: 0.8726 - K_F1_score: 0.7075\n",
      "Epoch 182/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 0.4033 - acc: 0.8712 - K_F1_score: 0.6943\n",
      "Epoch 183/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4084 - acc: 0.8755 - K_F1_score: 0.7046\n",
      "Epoch 184/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 0.4134 - acc: 0.8698 - K_F1_score: 0.6908\n",
      "Epoch 185/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4117 - acc: 0.8669 - K_F1_score: 0.6985\n",
      "Epoch 186/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.4058 - acc: 0.8741 - K_F1_score: 0.6962\n",
      "Epoch 187/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.4093 - acc: 0.8683 - K_F1_score: 0.6964\n",
      "Epoch 188/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4010 - acc: 0.8726 - K_F1_score: 0.6941\n",
      "Epoch 189/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4023 - acc: 0.8741 - K_F1_score: 0.6991\n",
      "Epoch 190/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4092 - acc: 0.8698 - K_F1_score: 0.6914\n",
      "Epoch 191/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4000 - acc: 0.8784 - K_F1_score: 0.7116\n",
      "Epoch 192/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.3990 - acc: 0.8683 - K_F1_score: 0.7052\n",
      "Epoch 193/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.3962 - acc: 0.8799 - K_F1_score: 0.7127\n",
      "Epoch 194/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4017 - acc: 0.8755 - K_F1_score: 0.7041\n",
      "Epoch 195/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4041 - acc: 0.8683 - K_F1_score: 0.6995\n",
      "Epoch 196/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.4051 - acc: 0.8726 - K_F1_score: 0.7024\n",
      "Epoch 197/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.3990 - acc: 0.8683 - K_F1_score: 0.6989\n",
      "Epoch 198/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.3989 - acc: 0.8712 - K_F1_score: 0.7017\n",
      "Epoch 199/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.3981 - acc: 0.8857 - K_F1_score: 0.7118\n",
      "Epoch 200/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.3975 - acc: 0.8784 - K_F1_score: 0.7037\n",
      "Epoch 201/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4019 - acc: 0.8698 - K_F1_score: 0.7073\n",
      "Epoch 202/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.3965 - acc: 0.8741 - K_F1_score: 0.7049\n",
      "Epoch 203/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.3977 - acc: 0.8726 - K_F1_score: 0.7027\n",
      "Epoch 204/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 275us/step - loss: 0.3937 - acc: 0.8784 - K_F1_score: 0.7026\n",
      "Epoch 205/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4004 - acc: 0.8698 - K_F1_score: 0.7006\n",
      "Epoch 206/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4023 - acc: 0.8698 - K_F1_score: 0.7048\n",
      "Epoch 207/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.3971 - acc: 0.8755 - K_F1_score: 0.7011\n",
      "Epoch 208/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.3924 - acc: 0.8784 - K_F1_score: 0.7166\n",
      "Epoch 209/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.3957 - acc: 0.8755 - K_F1_score: 0.7018\n",
      "Epoch 210/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.4028 - acc: 0.8726 - K_F1_score: 0.6982\n",
      "Epoch 211/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.3957 - acc: 0.8726 - K_F1_score: 0.7140\n",
      "Epoch 212/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.3968 - acc: 0.8741 - K_F1_score: 0.7010\n",
      "Epoch 213/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 0.4051 - acc: 0.8654 - K_F1_score: 0.7033\n",
      "Epoch 214/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.3933 - acc: 0.8698 - K_F1_score: 0.7043\n",
      "Epoch 215/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.3969 - acc: 0.8712 - K_F1_score: 0.6974\n",
      "Epoch 216/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.3908 - acc: 0.8799 - K_F1_score: 0.7011\n",
      "Epoch 217/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.4001 - acc: 0.8799 - K_F1_score: 0.7043\n",
      "Epoch 218/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.3899 - acc: 0.8770 - K_F1_score: 0.7066\n",
      "Epoch 219/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.3937 - acc: 0.8698 - K_F1_score: 0.7059\n",
      "Epoch 220/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.4067 - acc: 0.8640 - K_F1_score: 0.6897\n",
      "Epoch 221/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.3953 - acc: 0.8755 - K_F1_score: 0.7168\n",
      "Epoch 222/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.3967 - acc: 0.8669 - K_F1_score: 0.6960\n",
      "Epoch 223/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.3906 - acc: 0.8741 - K_F1_score: 0.7007\n",
      "Epoch 224/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.3938 - acc: 0.8784 - K_F1_score: 0.7025\n",
      "Epoch 225/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.3922 - acc: 0.8726 - K_F1_score: 0.7174\n",
      "Epoch 226/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.3875 - acc: 0.8726 - K_F1_score: 0.7078\n",
      "Epoch 227/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.3914 - acc: 0.8712 - K_F1_score: 0.7083\n",
      "Epoch 228/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.3965 - acc: 0.8755 - K_F1_score: 0.7058\n",
      "Epoch 229/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.3909 - acc: 0.8698 - K_F1_score: 0.6994\n",
      "Epoch 230/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.3899 - acc: 0.8726 - K_F1_score: 0.7087\n",
      "Epoch 231/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.3859 - acc: 0.8813 - K_F1_score: 0.7025\n",
      "Epoch 232/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.3874 - acc: 0.8784 - K_F1_score: 0.7128\n",
      "Epoch 233/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.3911 - acc: 0.8741 - K_F1_score: 0.7087\n",
      "Epoch 234/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.3889 - acc: 0.8712 - K_F1_score: 0.7095\n",
      "Epoch 235/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.3868 - acc: 0.8828 - K_F1_score: 0.7234\n",
      "Epoch 236/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.3872 - acc: 0.8755 - K_F1_score: 0.7079\n",
      "Epoch 237/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 0.3885 - acc: 0.8784 - K_F1_score: 0.7180\n",
      "Epoch 238/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.3931 - acc: 0.8726 - K_F1_score: 0.7150\n",
      "Epoch 239/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.3838 - acc: 0.8813 - K_F1_score: 0.7115\n",
      "Epoch 240/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.3958 - acc: 0.8741 - K_F1_score: 0.7093\n",
      "Epoch 241/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.3836 - acc: 0.8799 - K_F1_score: 0.7031\n",
      "Epoch 242/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.3825 - acc: 0.8799 - K_F1_score: 0.7189\n",
      "Epoch 243/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.3882 - acc: 0.8799 - K_F1_score: 0.7165\n",
      "Epoch 244/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.3832 - acc: 0.8784 - K_F1_score: 0.7121\n",
      "Epoch 245/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.3838 - acc: 0.8770 - K_F1_score: 0.7251\n",
      "Epoch 246/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.3857 - acc: 0.8726 - K_F1_score: 0.7068\n",
      "Epoch 247/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.3847 - acc: 0.8683 - K_F1_score: 0.7098\n",
      "Epoch 248/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 0.3836 - acc: 0.8842 - K_F1_score: 0.7094\n",
      "Epoch 249/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.3797 - acc: 0.8813 - K_F1_score: 0.7133\n",
      "Epoch 250/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.3805 - acc: 0.8842 - K_F1_score: 0.7161\n",
      "Epoch 251/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 0.3814 - acc: 0.8784 - K_F1_score: 0.7108\n",
      "Epoch 252/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 0.3862 - acc: 0.8683 - K_F1_score: 0.7034\n",
      "Epoch 253/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.3802 - acc: 0.8799 - K_F1_score: 0.7167\n",
      "Epoch 254/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.3826 - acc: 0.8755 - K_F1_score: 0.7113\n",
      "Epoch 255/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 0.3869 - acc: 0.8712 - K_F1_score: 0.7117\n",
      "Epoch 256/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 0.3880 - acc: 0.8755 - K_F1_score: 0.7013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5ce8be0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = NN_model_v2((X_Train.shape[1], ), layers, regularizers.l2(0.01), None)\n",
    "test_model.compile(optimizer = \"Adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\", K_F1_score])\n",
    "test_model.fit(x = X_Train, y = Y_Train, epochs = 256, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  88.71201157742402\n",
      "F1 Score =  0.8414634146341463\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True        207.0          24.0\n",
      "1  Pred False         54.0         406.0\n",
      "Accuracy =  85.5\n",
      "F1 Score =  0.8129032258064517\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True         63.0          11.0\n",
      "1  Pred False         18.0         108.0\n"
     ]
    }
   ],
   "source": [
    "train_pred = test_model.predict(x = X_Train)\n",
    "cv_pred = test_model.predict(x = X_CV)\n",
    "\n",
    "train_hat = normalize_predictions(train_pred)\n",
    "cv_hat = normalize_predictions(cv_pred)\n",
    "\n",
    "show_acc(Y_Train, train_hat)\n",
    "show_acc(Y_CV, cv_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Score :  0.8426704014939308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8426704014939308"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_ROC(Y_CV, cv_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this measure is an approximation of the actual measure during training as it appears as it is evaluated per most-recent minibatch in that particular epoch.\n",
    "\n",
    "## Implementing R@P Optimization\n",
    "\n",
    "So now to implement the R@P formulae in tensorflow verbatim from the paper for 2 reasons - to see how it does vs traditional accuracy, and to get some experience in implementing custom loss functions in tensorflow.\n",
    "\n",
    "One note about the below implementation is I decided to divert from the original paper in the base loss function and used Sigmoidal Logistic Crossentropy Loss as opposed to Hinge Loss as the original authors, the reasoning for this is simply that I am considering only a binary classification problem and not a multi-class classification problem that the original authors used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Took tensorflow function imports from tensorflow source code\n",
    "import math\n",
    "\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import function\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import candidate_sampling_ops\n",
    "from tensorflow.python.ops import embedding_ops\n",
    "from tensorflow.python.ops import gen_array_ops  # pylint: disable=unused-import\n",
    "from tensorflow.python.ops import gen_nn_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import sparse_ops\n",
    "from tensorflow.python.ops import variables\n",
    "from tensorflow.python.util.deprecation import deprecated_args\n",
    "from tensorflow.python.util.deprecation import deprecated_argument_lookup\n",
    "from tensorflow.python.util.tf_export import tf_export\n",
    "\n",
    "#Optimize recall at fixed precision\n",
    "def r_at_p_loss(target, output, alph_tf = 0.95):\n",
    "    \n",
    "\n",
    "    lam_tf = tf.Variable(0.01, tf.float32, name='lam_tf')\n",
    "    #alph_tf = tf.placeholder(tf.float32, name='alph_tf')\n",
    "    \n",
    "    #lam_tf = K.variable(lam_tf, name='lam_tf')\n",
    "    \n",
    "    zeros = tf.zeros_like(output)\n",
    "    ones = tf.ones_like(output)\n",
    "    \n",
    "    cond_z = tf.equal(zeros, target)\n",
    "    cont_one = tf.not_equal(zeros, target)\n",
    "    \n",
    "    ind_z = tf.where(cond_z)\n",
    "    ind_one = tf.where(cont_one)\n",
    "    \n",
    "    one_labels = tf.gather_nd(target, ind_one)\n",
    "    one_logits = tf.gather_nd(output, ind_one)\n",
    "    \n",
    "    z_labels = tf.gather_nd(target, ind_z)\n",
    "    z_logits = tf.gather_nd(output, ind_z)\n",
    "    \n",
    "    L_plus = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=one_labels, logits=one_logits))\n",
    "    L_minus = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=z_labels, logits=z_logits))\n",
    "    \n",
    "    Y_plus = tf.reduce_sum(one_labels)\n",
    "    \n",
    "\n",
    "    loss_one = tf.multiply(tf.add(1.0, lam_tf), L_plus) \n",
    "    loss_two = tf.multiply(tf.multiply(tf.divide(alph_tf, tf.subtract(1.0, alph_tf)), lam_tf), L_minus)\n",
    "    loss_three = tf.multiply(-1.0, tf.multiply(lam_tf, Y_plus))\n",
    "    \n",
    "    fin_loss = tf.add(tf.add(loss_one, loss_two), loss_three)\n",
    "    \n",
    "    lam_tf = upd_lambda_rp(lam_tf,  L_plus, L_minus, Y_plus, alph_tf)\n",
    "    \n",
    "    return fin_loss\n",
    "\n",
    "#Define function to optimize lambda by gradient descent (or is it ascent?)\n",
    "def upd_lambda_rp(lam,  L_plus, L_minus, Y_plus, alph_tf):\n",
    "    lr = 0.01\n",
    "    \n",
    "    #Differential of loss function by lambda\n",
    "    dl =  tf.subtract(tf.add(L_plus, tf.divide(alph_tf, tf.subtract(1.0, alph_tf))), Y_plus)\n",
    "    new_lam = tf.add(lam, tf.multiply(lr, dl))\n",
    "    \n",
    "    return new_lam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "691/691 [==============================] - 1s 753us/step - loss: 9.6174 - acc: 0.3980 - K_F1_score: 0.5033\n",
      "Epoch 2/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 9.2905 - acc: 0.3777 - K_F1_score: 0.5475\n",
      "Epoch 3/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 9.1652 - acc: 0.3777 - K_F1_score: 0.5473\n",
      "Epoch 4/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 9.0826 - acc: 0.3777 - K_F1_score: 0.5479\n",
      "Epoch 5/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 9.0085 - acc: 0.3777 - K_F1_score: 0.5511\n",
      "Epoch 6/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 8.9302 - acc: 0.3777 - K_F1_score: 0.5575\n",
      "Epoch 7/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 8.8343 - acc: 0.3777 - K_F1_score: 0.5699\n",
      "Epoch 8/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 8.7524 - acc: 0.3777 - K_F1_score: 0.5816\n",
      "Epoch 9/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 8.6563 - acc: 0.3777 - K_F1_score: 0.5977\n",
      "Epoch 10/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 8.5851 - acc: 0.3777 - K_F1_score: 0.6059\n",
      "Epoch 11/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 8.5334 - acc: 0.3777 - K_F1_score: 0.6084\n",
      "Epoch 12/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 8.4742 - acc: 0.3777 - K_F1_score: 0.6162\n",
      "Epoch 13/256\n",
      "691/691 [==============================] - 0s 362us/step - loss: 8.4364 - acc: 0.3821 - K_F1_score: 0.6237\n",
      "Epoch 14/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 8.4211 - acc: 0.7236 - K_F1_score: 0.5939\n",
      "Epoch 15/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 8.3852 - acc: 0.7757 - K_F1_score: 0.5883\n",
      "Epoch 16/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 8.3621 - acc: 0.7771 - K_F1_score: 0.5971\n",
      "Epoch 17/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 8.3359 - acc: 0.7873 - K_F1_score: 0.5973\n",
      "Epoch 18/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 8.2964 - acc: 0.8003 - K_F1_score: 0.6026\n",
      "Epoch 19/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 8.2850 - acc: 0.8075 - K_F1_score: 0.6070\n",
      "Epoch 20/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 8.2616 - acc: 0.8075 - K_F1_score: 0.6142\n",
      "Epoch 21/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 8.2423 - acc: 0.8032 - K_F1_score: 0.6128\n",
      "Epoch 22/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 8.2228 - acc: 0.8104 - K_F1_score: 0.6142\n",
      "Epoch 23/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 8.1809 - acc: 0.8090 - K_F1_score: 0.6222\n",
      "Epoch 24/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 8.1983 - acc: 0.8162 - K_F1_score: 0.6234\n",
      "Epoch 25/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 8.1780 - acc: 0.8220 - K_F1_score: 0.6297\n",
      "Epoch 26/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 8.1471 - acc: 0.8162 - K_F1_score: 0.6315\n",
      "Epoch 27/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 8.1270 - acc: 0.8205 - K_F1_score: 0.6337\n",
      "Epoch 28/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 8.1110 - acc: 0.8263 - K_F1_score: 0.6377\n",
      "Epoch 29/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 8.0959 - acc: 0.8249 - K_F1_score: 0.6383\n",
      "Epoch 30/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 8.0990 - acc: 0.8205 - K_F1_score: 0.6484\n",
      "Epoch 31/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 8.1004 - acc: 0.8220 - K_F1_score: 0.6448\n",
      "Epoch 32/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 8.0596 - acc: 0.8307 - K_F1_score: 0.6528\n",
      "Epoch 33/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 8.0846 - acc: 0.8119 - K_F1_score: 0.6505\n",
      "Epoch 34/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 8.0683 - acc: 0.8162 - K_F1_score: 0.6513\n",
      "Epoch 35/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 8.0479 - acc: 0.8292 - K_F1_score: 0.6571\n",
      "Epoch 36/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 8.0431 - acc: 0.8307 - K_F1_score: 0.6556\n",
      "Epoch 37/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 8.0233 - acc: 0.8307 - K_F1_score: 0.6578\n",
      "Epoch 38/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 8.0165 - acc: 0.8307 - K_F1_score: 0.6593\n",
      "Epoch 39/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 8.0361 - acc: 0.8278 - K_F1_score: 0.6597\n",
      "Epoch 40/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 8.0153 - acc: 0.8234 - K_F1_score: 0.6644\n",
      "Epoch 41/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 8.0090 - acc: 0.8234 - K_F1_score: 0.6654\n",
      "Epoch 42/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.9836 - acc: 0.8321 - K_F1_score: 0.6721\n",
      "Epoch 43/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 8.0128 - acc: 0.8220 - K_F1_score: 0.6689\n",
      "Epoch 44/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.9631 - acc: 0.8336 - K_F1_score: 0.6764\n",
      "Epoch 45/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.9725 - acc: 0.8307 - K_F1_score: 0.6672\n",
      "Epoch 46/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.9569 - acc: 0.8292 - K_F1_score: 0.6662\n",
      "Epoch 47/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.9980 - acc: 0.8350 - K_F1_score: 0.6748\n",
      "Epoch 48/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.9672 - acc: 0.8292 - K_F1_score: 0.6761\n",
      "Epoch 49/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.9361 - acc: 0.8321 - K_F1_score: 0.6805\n",
      "Epoch 50/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.9247 - acc: 0.8365 - K_F1_score: 0.6839\n",
      "Epoch 51/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.9077 - acc: 0.8408 - K_F1_score: 0.6906\n",
      "Epoch 52/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.9334 - acc: 0.8365 - K_F1_score: 0.6852\n",
      "Epoch 53/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.9063 - acc: 0.8394 - K_F1_score: 0.6830\n",
      "Epoch 54/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.9012 - acc: 0.8365 - K_F1_score: 0.6857\n",
      "Epoch 55/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.9217 - acc: 0.8249 - K_F1_score: 0.6863\n",
      "Epoch 56/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.9102 - acc: 0.8408 - K_F1_score: 0.6950\n",
      "Epoch 57/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.9031 - acc: 0.8278 - K_F1_score: 0.6936\n",
      "Epoch 58/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.8887 - acc: 0.8249 - K_F1_score: 0.6914\n",
      "Epoch 59/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.8948 - acc: 0.8292 - K_F1_score: 0.6889\n",
      "Epoch 60/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.9041 - acc: 0.8278 - K_F1_score: 0.6914\n",
      "Epoch 61/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.8704 - acc: 0.8278 - K_F1_score: 0.6970\n",
      "Epoch 62/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.8701 - acc: 0.8379 - K_F1_score: 0.7036\n",
      "Epoch 63/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.8388 - acc: 0.8408 - K_F1_score: 0.7066\n",
      "Epoch 64/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.8381 - acc: 0.8394 - K_F1_score: 0.7050\n",
      "Epoch 65/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.8277 - acc: 0.8423 - K_F1_score: 0.7070\n",
      "Epoch 66/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.7842 - acc: 0.8437 - K_F1_score: 0.7048\n",
      "Epoch 67/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 7.8136 - acc: 0.8452 - K_F1_score: 0.7095\n",
      "Epoch 68/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.7930 - acc: 0.8423 - K_F1_score: 0.7068\n",
      "Epoch 69/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 318us/step - loss: 7.8549 - acc: 0.8220 - K_F1_score: 0.7065\n",
      "Epoch 70/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.8535 - acc: 0.8408 - K_F1_score: 0.7149\n",
      "Epoch 71/256\n",
      "691/691 [==============================] - 0s 362us/step - loss: 7.8135 - acc: 0.8365 - K_F1_score: 0.7091\n",
      "Epoch 72/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.7869 - acc: 0.8437 - K_F1_score: 0.7115\n",
      "Epoch 73/256\n",
      "691/691 [==============================] - 0s 376us/step - loss: 7.8131 - acc: 0.8321 - K_F1_score: 0.7056\n",
      "Epoch 74/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 7.7996 - acc: 0.8394 - K_F1_score: 0.7162\n",
      "Epoch 75/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.7621 - acc: 0.8423 - K_F1_score: 0.7196\n",
      "Epoch 76/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.7704 - acc: 0.8437 - K_F1_score: 0.7162\n",
      "Epoch 77/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.7587 - acc: 0.8452 - K_F1_score: 0.7157\n",
      "Epoch 78/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.7777 - acc: 0.8423 - K_F1_score: 0.7227\n",
      "Epoch 79/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.7602 - acc: 0.8379 - K_F1_score: 0.7171\n",
      "Epoch 80/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.8238 - acc: 0.8249 - K_F1_score: 0.7101\n",
      "Epoch 81/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.7983 - acc: 0.8336 - K_F1_score: 0.7194\n",
      "Epoch 82/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.8273 - acc: 0.8234 - K_F1_score: 0.7006\n",
      "Epoch 83/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.7759 - acc: 0.8394 - K_F1_score: 0.7176\n",
      "Epoch 84/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.7878 - acc: 0.8307 - K_F1_score: 0.7119\n",
      "Epoch 85/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.7544 - acc: 0.8394 - K_F1_score: 0.7204\n",
      "Epoch 86/256\n",
      "691/691 [==============================] - 0s 362us/step - loss: 7.7486 - acc: 0.8423 - K_F1_score: 0.7238\n",
      "Epoch 87/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 7.7407 - acc: 0.8437 - K_F1_score: 0.7247\n",
      "Epoch 88/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.7470 - acc: 0.8437 - K_F1_score: 0.7270\n",
      "Epoch 89/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.7534 - acc: 0.8423 - K_F1_score: 0.7269\n",
      "Epoch 90/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.7245 - acc: 0.8365 - K_F1_score: 0.7251\n",
      "Epoch 91/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.7529 - acc: 0.8321 - K_F1_score: 0.7257\n",
      "Epoch 92/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.7440 - acc: 0.8379 - K_F1_score: 0.7284\n",
      "Epoch 93/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.7353 - acc: 0.8423 - K_F1_score: 0.7326\n",
      "Epoch 94/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.7340 - acc: 0.8408 - K_F1_score: 0.7254\n",
      "Epoch 95/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.7289 - acc: 0.8437 - K_F1_score: 0.7321\n",
      "Epoch 96/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.7050 - acc: 0.8423 - K_F1_score: 0.7220\n",
      "Epoch 97/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.7172 - acc: 0.8437 - K_F1_score: 0.7311\n",
      "Epoch 98/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.7250 - acc: 0.8437 - K_F1_score: 0.7228\n",
      "Epoch 99/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.7254 - acc: 0.8336 - K_F1_score: 0.7273\n",
      "Epoch 100/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.7355 - acc: 0.8423 - K_F1_score: 0.7292\n",
      "Epoch 101/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.7065 - acc: 0.8408 - K_F1_score: 0.7315\n",
      "Epoch 102/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.7069 - acc: 0.8408 - K_F1_score: 0.7313\n",
      "Epoch 103/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.7189 - acc: 0.8423 - K_F1_score: 0.7312\n",
      "Epoch 104/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.7094 - acc: 0.8408 - K_F1_score: 0.7272\n",
      "Epoch 105/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.7478 - acc: 0.8394 - K_F1_score: 0.7247\n",
      "Epoch 106/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.7236 - acc: 0.8394 - K_F1_score: 0.7293\n",
      "Epoch 107/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.6896 - acc: 0.8466 - K_F1_score: 0.7384\n",
      "Epoch 108/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.6903 - acc: 0.8452 - K_F1_score: 0.7348\n",
      "Epoch 109/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.6802 - acc: 0.8466 - K_F1_score: 0.7392\n",
      "Epoch 110/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.6787 - acc: 0.8466 - K_F1_score: 0.7391\n",
      "Epoch 111/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.6892 - acc: 0.8466 - K_F1_score: 0.7413\n",
      "Epoch 112/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.6631 - acc: 0.8466 - K_F1_score: 0.7423\n",
      "Epoch 113/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.6568 - acc: 0.8480 - K_F1_score: 0.7431\n",
      "Epoch 114/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.6644 - acc: 0.8466 - K_F1_score: 0.7339\n",
      "Epoch 115/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.6613 - acc: 0.8466 - K_F1_score: 0.7428\n",
      "Epoch 116/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.6678 - acc: 0.8480 - K_F1_score: 0.7432\n",
      "Epoch 117/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.6659 - acc: 0.8480 - K_F1_score: 0.7404\n",
      "Epoch 118/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 7.6500 - acc: 0.8466 - K_F1_score: 0.7420\n",
      "Epoch 119/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.6708 - acc: 0.8466 - K_F1_score: 0.7428\n",
      "Epoch 120/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.7084 - acc: 0.8119 - K_F1_score: 0.7195\n",
      "Epoch 121/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.7172 - acc: 0.8437 - K_F1_score: 0.7438\n",
      "Epoch 122/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.6794 - acc: 0.8350 - K_F1_score: 0.7310\n",
      "Epoch 123/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.6697 - acc: 0.8437 - K_F1_score: 0.7371\n",
      "Epoch 124/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.6628 - acc: 0.8452 - K_F1_score: 0.7425\n",
      "Epoch 125/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 7.6669 - acc: 0.8466 - K_F1_score: 0.7436\n",
      "Epoch 126/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.6540 - acc: 0.8466 - K_F1_score: 0.7408\n",
      "Epoch 127/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.6727 - acc: 0.8408 - K_F1_score: 0.7369\n",
      "Epoch 128/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.6631 - acc: 0.8466 - K_F1_score: 0.7427\n",
      "Epoch 129/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.6692 - acc: 0.8452 - K_F1_score: 0.7414\n",
      "Epoch 130/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.6554 - acc: 0.8423 - K_F1_score: 0.7405\n",
      "Epoch 131/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 7.6489 - acc: 0.8452 - K_F1_score: 0.7479\n",
      "Epoch 132/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.6325 - acc: 0.8480 - K_F1_score: 0.7416\n",
      "Epoch 133/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.6362 - acc: 0.8480 - K_F1_score: 0.7426\n",
      "Epoch 134/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 7.6407 - acc: 0.8480 - K_F1_score: 0.7410\n",
      "Epoch 135/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 7.6382 - acc: 0.8480 - K_F1_score: 0.7462\n",
      "Epoch 136/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.6184 - acc: 0.8480 - K_F1_score: 0.7363\n",
      "Epoch 137/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 304us/step - loss: 7.6360 - acc: 0.8480 - K_F1_score: 0.7511\n",
      "Epoch 138/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.6233 - acc: 0.8480 - K_F1_score: 0.7482\n",
      "Epoch 139/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.6290 - acc: 0.8480 - K_F1_score: 0.7446\n",
      "Epoch 140/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.6284 - acc: 0.8466 - K_F1_score: 0.7415\n",
      "Epoch 141/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.6334 - acc: 0.8466 - K_F1_score: 0.7501\n",
      "Epoch 142/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.6232 - acc: 0.8480 - K_F1_score: 0.7497\n",
      "Epoch 143/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 7.6166 - acc: 0.8480 - K_F1_score: 0.7547\n",
      "Epoch 144/256\n",
      "691/691 [==============================] - 0s 391us/step - loss: 7.6009 - acc: 0.8480 - K_F1_score: 0.7494\n",
      "Epoch 145/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 7.5989 - acc: 0.8480 - K_F1_score: 0.7443\n",
      "Epoch 146/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.6189 - acc: 0.8480 - K_F1_score: 0.7549\n",
      "Epoch 147/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.6271 - acc: 0.8480 - K_F1_score: 0.7499\n",
      "Epoch 148/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.6055 - acc: 0.8480 - K_F1_score: 0.7551\n",
      "Epoch 149/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5993 - acc: 0.8466 - K_F1_score: 0.7491\n",
      "Epoch 150/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5906 - acc: 0.8466 - K_F1_score: 0.7541\n",
      "Epoch 151/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.6088 - acc: 0.8480 - K_F1_score: 0.7588\n",
      "Epoch 152/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.5940 - acc: 0.8452 - K_F1_score: 0.7421\n",
      "Epoch 153/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.6306 - acc: 0.8452 - K_F1_score: 0.7426\n",
      "Epoch 154/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.6234 - acc: 0.8437 - K_F1_score: 0.7504\n",
      "Epoch 155/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.6111 - acc: 0.8452 - K_F1_score: 0.7486\n",
      "Epoch 156/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.6050 - acc: 0.8480 - K_F1_score: 0.7525\n",
      "Epoch 157/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.6124 - acc: 0.8480 - K_F1_score: 0.7520\n",
      "Epoch 158/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.5803 - acc: 0.8480 - K_F1_score: 0.7598\n",
      "Epoch 159/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5966 - acc: 0.8480 - K_F1_score: 0.7491\n",
      "Epoch 160/256\n",
      "691/691 [==============================] - 0s 362us/step - loss: 7.5792 - acc: 0.8480 - K_F1_score: 0.7569\n",
      "Epoch 161/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 7.5835 - acc: 0.8480 - K_F1_score: 0.7498\n",
      "Epoch 162/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5930 - acc: 0.8480 - K_F1_score: 0.7536\n",
      "Epoch 163/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.5838 - acc: 0.8480 - K_F1_score: 0.7472\n",
      "Epoch 164/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5956 - acc: 0.8466 - K_F1_score: 0.7558\n",
      "Epoch 165/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.5920 - acc: 0.8480 - K_F1_score: 0.7508\n",
      "Epoch 166/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.6618 - acc: 0.8234 - K_F1_score: 0.7315\n",
      "Epoch 167/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.6616 - acc: 0.8205 - K_F1_score: 0.7333\n",
      "Epoch 168/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.7026 - acc: 0.8205 - K_F1_score: 0.7346\n",
      "Epoch 169/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.6949 - acc: 0.8119 - K_F1_score: 0.7292\n",
      "Epoch 170/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 7.6766 - acc: 0.8220 - K_F1_score: 0.7385\n",
      "Epoch 171/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.7086 - acc: 0.8278 - K_F1_score: 0.7359\n",
      "Epoch 172/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.6962 - acc: 0.8321 - K_F1_score: 0.7335\n",
      "Epoch 173/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 7.7082 - acc: 0.8148 - K_F1_score: 0.7339\n",
      "Epoch 174/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.6928 - acc: 0.8061 - K_F1_score: 0.7233\n",
      "Epoch 175/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.6463 - acc: 0.8336 - K_F1_score: 0.7436\n",
      "Epoch 176/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.5768 - acc: 0.8466 - K_F1_score: 0.7593\n",
      "Epoch 177/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5942 - acc: 0.8466 - K_F1_score: 0.7517\n",
      "Epoch 178/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.5727 - acc: 0.8466 - K_F1_score: 0.7541\n",
      "Epoch 179/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5928 - acc: 0.8480 - K_F1_score: 0.7486\n",
      "Epoch 180/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5589 - acc: 0.8480 - K_F1_score: 0.7574\n",
      "Epoch 181/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5852 - acc: 0.8480 - K_F1_score: 0.7634\n",
      "Epoch 182/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.5763 - acc: 0.8466 - K_F1_score: 0.7554\n",
      "Epoch 183/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5729 - acc: 0.8466 - K_F1_score: 0.7516\n",
      "Epoch 184/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5646 - acc: 0.8480 - K_F1_score: 0.7658\n",
      "Epoch 185/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.5631 - acc: 0.8480 - K_F1_score: 0.7634\n",
      "Epoch 186/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5583 - acc: 0.8480 - K_F1_score: 0.7575\n",
      "Epoch 187/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.5526 - acc: 0.8480 - K_F1_score: 0.7632\n",
      "Epoch 188/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.5690 - acc: 0.8480 - K_F1_score: 0.7618\n",
      "Epoch 189/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.5601 - acc: 0.8480 - K_F1_score: 0.7615\n",
      "Epoch 190/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.5568 - acc: 0.8480 - K_F1_score: 0.7588\n",
      "Epoch 191/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5559 - acc: 0.8480 - K_F1_score: 0.7641\n",
      "Epoch 192/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.5525 - acc: 0.8480 - K_F1_score: 0.7639\n",
      "Epoch 193/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.5225 - acc: 0.8480 - K_F1_score: 0.7608\n",
      "Epoch 194/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5411 - acc: 0.8480 - K_F1_score: 0.7669\n",
      "Epoch 195/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 7.5315 - acc: 0.8480 - K_F1_score: 0.7588\n",
      "Epoch 196/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 7.5460 - acc: 0.8480 - K_F1_score: 0.7683\n",
      "Epoch 197/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.5601 - acc: 0.8480 - K_F1_score: 0.7675\n",
      "Epoch 198/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5477 - acc: 0.8480 - K_F1_score: 0.7682\n",
      "Epoch 199/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.5391 - acc: 0.8480 - K_F1_score: 0.7668\n",
      "Epoch 200/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.5482 - acc: 0.8480 - K_F1_score: 0.7644\n",
      "Epoch 201/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5276 - acc: 0.8480 - K_F1_score: 0.7645\n",
      "Epoch 202/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.5628 - acc: 0.8480 - K_F1_score: 0.7654\n",
      "Epoch 203/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.5279 - acc: 0.8480 - K_F1_score: 0.7680\n",
      "Epoch 204/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 304us/step - loss: 7.5526 - acc: 0.8480 - K_F1_score: 0.7695\n",
      "Epoch 205/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.5206 - acc: 0.8480 - K_F1_score: 0.7613\n",
      "Epoch 206/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.5268 - acc: 0.8480 - K_F1_score: 0.7628\n",
      "Epoch 207/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5422 - acc: 0.8480 - K_F1_score: 0.7648\n",
      "Epoch 208/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.5444 - acc: 0.8480 - K_F1_score: 0.7669\n",
      "Epoch 209/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.5311 - acc: 0.8480 - K_F1_score: 0.7710\n",
      "Epoch 210/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.5322 - acc: 0.8480 - K_F1_score: 0.7654\n",
      "Epoch 211/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.5128 - acc: 0.8466 - K_F1_score: 0.7693\n",
      "Epoch 212/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.5360 - acc: 0.8480 - K_F1_score: 0.7712\n",
      "Epoch 213/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.5246 - acc: 0.8480 - K_F1_score: 0.7712\n",
      "Epoch 214/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.5107 - acc: 0.8466 - K_F1_score: 0.7691\n",
      "Epoch 215/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.5289 - acc: 0.8480 - K_F1_score: 0.7659\n",
      "Epoch 216/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.5312 - acc: 0.8480 - K_F1_score: 0.7641\n",
      "Epoch 217/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 7.5247 - acc: 0.8480 - K_F1_score: 0.7635\n",
      "Epoch 218/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 7.5352 - acc: 0.8480 - K_F1_score: 0.7727\n",
      "Epoch 219/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 7.5034 - acc: 0.8480 - K_F1_score: 0.7716\n",
      "Epoch 220/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.5276 - acc: 0.8480 - K_F1_score: 0.7720\n",
      "Epoch 221/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.5107 - acc: 0.8480 - K_F1_score: 0.7701\n",
      "Epoch 222/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5176 - acc: 0.8480 - K_F1_score: 0.7679\n",
      "Epoch 223/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 7.5138 - acc: 0.8466 - K_F1_score: 0.7707\n",
      "Epoch 224/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.5527 - acc: 0.8394 - K_F1_score: 0.7709\n",
      "Epoch 225/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.5733 - acc: 0.8350 - K_F1_score: 0.7515\n",
      "Epoch 226/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.5663 - acc: 0.8336 - K_F1_score: 0.7574\n",
      "Epoch 227/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.5887 - acc: 0.8350 - K_F1_score: 0.7672\n",
      "Epoch 228/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.5526 - acc: 0.8365 - K_F1_score: 0.7595\n",
      "Epoch 229/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5859 - acc: 0.8336 - K_F1_score: 0.7550\n",
      "Epoch 230/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5629 - acc: 0.8321 - K_F1_score: 0.7544\n",
      "Epoch 231/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.5904 - acc: 0.8350 - K_F1_score: 0.7653\n",
      "Epoch 232/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.5644 - acc: 0.8365 - K_F1_score: 0.7565\n",
      "Epoch 233/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 7.5351 - acc: 0.8437 - K_F1_score: 0.7717\n",
      "Epoch 234/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.5179 - acc: 0.8452 - K_F1_score: 0.7757\n",
      "Epoch 235/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 7.5125 - acc: 0.8452 - K_F1_score: 0.7732\n",
      "Epoch 236/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.5281 - acc: 0.8452 - K_F1_score: 0.7730\n",
      "Epoch 237/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5112 - acc: 0.8452 - K_F1_score: 0.7692\n",
      "Epoch 238/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5132 - acc: 0.8452 - K_F1_score: 0.7701\n",
      "Epoch 239/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5361 - acc: 0.8452 - K_F1_score: 0.7722\n",
      "Epoch 240/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.5193 - acc: 0.8452 - K_F1_score: 0.7693\n",
      "Epoch 241/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.5169 - acc: 0.8452 - K_F1_score: 0.7741\n",
      "Epoch 242/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5206 - acc: 0.8452 - K_F1_score: 0.7690\n",
      "Epoch 243/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.4953 - acc: 0.8452 - K_F1_score: 0.7776\n",
      "Epoch 244/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 7.5132 - acc: 0.8452 - K_F1_score: 0.7662\n",
      "Epoch 245/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.5089 - acc: 0.8452 - K_F1_score: 0.7772\n",
      "Epoch 246/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 7.5093 - acc: 0.8452 - K_F1_score: 0.7743\n",
      "Epoch 247/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5181 - acc: 0.8452 - K_F1_score: 0.7721\n",
      "Epoch 248/256\n",
      "691/691 [==============================] - 0s 347us/step - loss: 7.5317 - acc: 0.8452 - K_F1_score: 0.7666\n",
      "Epoch 249/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.5100 - acc: 0.8452 - K_F1_score: 0.7772\n",
      "Epoch 250/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5105 - acc: 0.8452 - K_F1_score: 0.7756\n",
      "Epoch 251/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.5079 - acc: 0.8452 - K_F1_score: 0.7735\n",
      "Epoch 252/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 7.4939 - acc: 0.8452 - K_F1_score: 0.7720\n",
      "Epoch 253/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5105 - acc: 0.8452 - K_F1_score: 0.7774\n",
      "Epoch 254/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.5053 - acc: 0.8452 - K_F1_score: 0.7797\n",
      "Epoch 255/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.4920 - acc: 0.8452 - K_F1_score: 0.7742\n",
      "Epoch 256/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 7.4916 - acc: 0.8452 - K_F1_score: 0.7693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10f60be0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = NN_model_v2((X_Train.shape[1], ), layers, regularizers.l2(0.01), None)\n",
    "test_model.compile(optimizer = \"Adam\", loss = r_at_p_loss, metrics = [\"accuracy\", K_F1_score])\n",
    "test_model.fit(x = X_Train, y = Y_Train, epochs = 256, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  84.5151953690304\n",
      "F1 Score =  0.8043875685557588\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True        220.0          66.0\n",
      "1  Pred False         41.0         364.0\n",
      "Accuracy =  85.5\n",
      "F1 Score =  0.8323699421965318\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True         72.0          20.0\n",
      "1  Pred False          9.0          99.0\n"
     ]
    }
   ],
   "source": [
    "train_pred = test_model.predict(x = X_Train)\n",
    "cv_pred = test_model.predict(x = X_CV)\n",
    "\n",
    "train_hat = normalize_predictions(train_pred)\n",
    "cv_hat = normalize_predictions(cv_pred)\n",
    "\n",
    "show_acc(Y_Train, train_hat)\n",
    "show_acc(Y_CV, cv_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Score :  0.8604108309990661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8604108309990661"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_ROC(Y_CV, cv_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So training R@P95 appears to have lower accuracy when optimizing, but generalizes to the data better and suffers from less overfitting, while also improving the ROC score.  Obviously this is very subject to convergence at local minima as a result of random initialization,  so should be taken somewhat lightly as a result.  However it is encouraging nonetheless.\n",
    "\n",
    "This indicates that there's solid diversity within the trained weights between the two systems, so when using ensembles of neural networks having systems trained with multiple objectives will likely be a very valuable ensembling technique.\n",
    "\n",
    "### Optimizing over PR Curve\n",
    "\n",
    "So the next logical step is to iterate this method over K steps to effectively maximize the area under the PR curve, this is done by iterating over multiple values of R@P and integrating via Simpson's Rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lams = {}\n",
    "\n",
    "#To optimize aucpr simply integrate over r@p across a range using simpson's rule integration\n",
    "def aucpr_loss(target, output, k = 5):\n",
    "    \n",
    "    zeros = tf.zeros_like(output)\n",
    "    ones = tf.ones_like(output)\n",
    "    \n",
    "    #Define all Lplus and Lminus matrices\n",
    "    cond_z = tf.equal(zeros, target)\n",
    "    cont_one = tf.not_equal(zeros, target)\n",
    "    \n",
    "    ind_z = tf.where(cond_z)\n",
    "    ind_one = tf.where(cont_one)\n",
    "    \n",
    "    one_labels = tf.gather_nd(target, ind_one)\n",
    "    one_logits = tf.gather_nd(output, ind_one)\n",
    "    \n",
    "    z_labels = tf.gather_nd(target, ind_z)\n",
    "    z_logits = tf.gather_nd(output, ind_z)\n",
    "    \n",
    "    #Calculate basic loss functions\n",
    "    L_plus = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=one_labels, logits=one_logits))\n",
    "    L_minus = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=z_labels, logits=z_logits))\n",
    "        \n",
    "    Y_plus = tf.reduce_sum(one_labels)\n",
    "    \n",
    "    #dynamic assign of lambda variables\n",
    "    for j in range(k):\n",
    "        lams['lam_tf'+str(k)] =  tf.Variable(0.01, tf.float32, name='lam_tf'+str(k))\n",
    "    \n",
    "    \n",
    "    fin_loss = 0.0\n",
    "    \n",
    "    #Simpson's rule integrate\n",
    "    for i in range(k):\n",
    "        if i == 0 :\n",
    "            alph_t1 = float(0.5)\n",
    "        else :\n",
    "            alph_t1 = float(0.5 + ((1 - 0.5)*(i))/k)\n",
    "        \n",
    "        alph_t = float(0.5 + ((0.95 - 0.5)*(i + 1)/k))\n",
    "        #alph_t = 0.95\n",
    "    \n",
    "        #Calculate p@r losses\n",
    "        loss_one_t = tf.multiply(tf.add(1.0, lams['lam_tf'+str(k)]), L_plus) \n",
    "        loss_two_t = tf.multiply(tf.multiply(tf.divide(alph_t, tf.subtract(1.0, alph_t)), lams['lam_tf'+str(k)]), L_minus)\n",
    "        loss_three_t = tf.multiply(-1.0, tf.multiply(lams['lam_tf'+str(k)], Y_plus))\n",
    "        \n",
    "        #Calculate t losses (note had to add absoloute function here to increase convergence)\n",
    "        fin_loss_t = tf.abs(tf.add(tf.add(loss_one_t, loss_two_t), loss_three_t))\n",
    "        \n",
    "        #Update lambda_k\n",
    "        lams['lam_tf'+str(k)] = upd_lambda_rp(lams['lam_tf'+str(k)],  L_plus, L_minus, Y_plus, alph_t)\n",
    "        \n",
    "        #Update and sum\n",
    "        if i == 0:\n",
    "            loss_in = fin_loss_t*alph_t\n",
    "            #loss_in = 0.0\n",
    "        else :\n",
    "            loss_in = (alph_t - alph_t1)*fin_loss_t\n",
    "            #loss_in = abs(fin_loss_t)\n",
    "        \n",
    "        fin_loss += loss_in\n",
    "    \n",
    "    return fin_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "691/691 [==============================] - 0s 666us/step - loss: 5.7919 - acc: 0.6107 - K_F1_score: 0.3466\n",
      "Epoch 2/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.9658 - acc: 0.6223 - K_F1_score: 0.3334\n",
      "Epoch 3/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.6718 - acc: 0.6223 - K_F1_score: 0.3219\n",
      "Epoch 4/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.6691 - acc: 0.6223 - K_F1_score: 0.3138\n",
      "Epoch 5/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.7342 - acc: 0.6223 - K_F1_score: 0.3019\n",
      "Epoch 6/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.7300 - acc: 0.6223 - K_F1_score: 0.2909\n",
      "Epoch 7/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.6553 - acc: 0.6223 - K_F1_score: 0.2870\n",
      "Epoch 8/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.5877 - acc: 0.6223 - K_F1_score: 0.2784\n",
      "Epoch 9/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.7301 - acc: 0.6223 - K_F1_score: 0.2787\n",
      "Epoch 10/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.5578 - acc: 0.6223 - K_F1_score: 0.2772\n",
      "Epoch 11/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.6401 - acc: 0.6223 - K_F1_score: 0.2775\n",
      "Epoch 12/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.6014 - acc: 0.6223 - K_F1_score: 0.2768\n",
      "Epoch 13/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.6126 - acc: 0.6223 - K_F1_score: 0.2767\n",
      "Epoch 14/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.5603 - acc: 0.6223 - K_F1_score: 0.2826\n",
      "Epoch 15/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.5068 - acc: 0.6223 - K_F1_score: 0.2791\n",
      "Epoch 16/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.5667 - acc: 0.6223 - K_F1_score: 0.2908\n",
      "Epoch 17/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.6014 - acc: 0.6223 - K_F1_score: 0.2835\n",
      "Epoch 18/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.6377 - acc: 0.6223 - K_F1_score: 0.2820\n",
      "Epoch 19/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.5583 - acc: 0.6223 - K_F1_score: 0.2789\n",
      "Epoch 20/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.5315 - acc: 0.6223 - K_F1_score: 0.2813\n",
      "Epoch 21/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.6432 - acc: 0.6223 - K_F1_score: 0.2776\n",
      "Epoch 22/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.5991 - acc: 0.6223 - K_F1_score: 0.2803\n",
      "Epoch 23/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.4671 - acc: 0.6223 - K_F1_score: 0.2810\n",
      "Epoch 24/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.6206 - acc: 0.6223 - K_F1_score: 0.2768\n",
      "Epoch 25/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.6274 - acc: 0.6223 - K_F1_score: 0.2867\n",
      "Epoch 26/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.6149 - acc: 0.6223 - K_F1_score: 0.2951\n",
      "Epoch 27/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.6272 - acc: 0.6223 - K_F1_score: 0.2825\n",
      "Epoch 28/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.5771 - acc: 0.6223 - K_F1_score: 0.2827\n",
      "Epoch 29/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 5.6324 - acc: 0.6223 - K_F1_score: 0.2831\n",
      "Epoch 30/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 5.6052 - acc: 0.6223 - K_F1_score: 0.2749\n",
      "Epoch 31/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.6659 - acc: 0.6223 - K_F1_score: 0.2827\n",
      "Epoch 32/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.6506 - acc: 0.6223 - K_F1_score: 0.2792\n",
      "Epoch 33/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.6210 - acc: 0.6223 - K_F1_score: 0.2950\n",
      "Epoch 34/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.6327 - acc: 0.6223 - K_F1_score: 0.2958\n",
      "Epoch 35/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.5880 - acc: 0.6223 - K_F1_score: 0.2911\n",
      "Epoch 36/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.4986 - acc: 0.6223 - K_F1_score: 0.2892\n",
      "Epoch 37/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.6571 - acc: 0.6223 - K_F1_score: 0.2934\n",
      "Epoch 38/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.5888 - acc: 0.6223 - K_F1_score: 0.2961\n",
      "Epoch 39/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.6242 - acc: 0.6223 - K_F1_score: 0.2907\n",
      "Epoch 40/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.4949 - acc: 0.6223 - K_F1_score: 0.2884\n",
      "Epoch 41/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.6915 - acc: 0.6223 - K_F1_score: 0.2931\n",
      "Epoch 42/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.6230 - acc: 0.6223 - K_F1_score: 0.3009\n",
      "Epoch 43/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.5624 - acc: 0.6223 - K_F1_score: 0.2960\n",
      "Epoch 44/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.4910 - acc: 0.6223 - K_F1_score: 0.3000\n",
      "Epoch 45/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.5422 - acc: 0.6223 - K_F1_score: 0.3096\n",
      "Epoch 46/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.5402 - acc: 0.6223 - K_F1_score: 0.3127\n",
      "Epoch 47/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.7961 - acc: 0.6223 - K_F1_score: 0.3153\n",
      "Epoch 48/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.5520 - acc: 0.6223 - K_F1_score: 0.3194\n",
      "Epoch 49/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.5148 - acc: 0.6223 - K_F1_score: 0.3248\n",
      "Epoch 50/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.5714 - acc: 0.6223 - K_F1_score: 0.3222\n",
      "Epoch 51/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 5.5507 - acc: 0.6223 - K_F1_score: 0.3214\n",
      "Epoch 52/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.5120 - acc: 0.6223 - K_F1_score: 0.3185\n",
      "Epoch 53/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.5335 - acc: 0.6223 - K_F1_score: 0.3232\n",
      "Epoch 54/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.4707 - acc: 0.6223 - K_F1_score: 0.3301\n",
      "Epoch 55/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.5125 - acc: 0.6223 - K_F1_score: 0.3274\n",
      "Epoch 56/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 5.5737 - acc: 0.6223 - K_F1_score: 0.3323\n",
      "Epoch 57/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.5759 - acc: 0.6223 - K_F1_score: 0.3265\n",
      "Epoch 58/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 5.4521 - acc: 0.6208 - K_F1_score: 0.3356\n",
      "Epoch 59/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.5296 - acc: 0.6295 - K_F1_score: 0.3516\n",
      "Epoch 60/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.4828 - acc: 0.6483 - K_F1_score: 0.3617\n",
      "Epoch 61/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.4761 - acc: 0.6614 - K_F1_score: 0.3679\n",
      "Epoch 62/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.4360 - acc: 0.6831 - K_F1_score: 0.3943\n",
      "Epoch 63/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.4530 - acc: 0.6889 - K_F1_score: 0.3995\n",
      "Epoch 64/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.4149 - acc: 0.6975 - K_F1_score: 0.4079\n",
      "Epoch 65/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 5.5436 - acc: 0.7337 - K_F1_score: 0.4552\n",
      "Epoch 66/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.4506 - acc: 0.7178 - K_F1_score: 0.4344\n",
      "Epoch 67/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.4382 - acc: 0.7381 - K_F1_score: 0.4530\n",
      "Epoch 68/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3574 - acc: 0.7352 - K_F1_score: 0.4645\n",
      "Epoch 69/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 289us/step - loss: 5.4124 - acc: 0.7931 - K_F1_score: 0.5454\n",
      "Epoch 70/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.4921 - acc: 0.7873 - K_F1_score: 0.5369\n",
      "Epoch 71/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.4942 - acc: 0.7699 - K_F1_score: 0.5098\n",
      "Epoch 72/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.4783 - acc: 0.7511 - K_F1_score: 0.4945\n",
      "Epoch 73/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.4039 - acc: 0.7467 - K_F1_score: 0.4841\n",
      "Epoch 74/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.4064 - acc: 0.7771 - K_F1_score: 0.5408\n",
      "Epoch 75/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.3716 - acc: 0.7786 - K_F1_score: 0.5464\n",
      "Epoch 76/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3219 - acc: 0.7945 - K_F1_score: 0.5744\n",
      "Epoch 77/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3910 - acc: 0.7887 - K_F1_score: 0.5566\n",
      "Epoch 78/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.3524 - acc: 0.8104 - K_F1_score: 0.6166\n",
      "Epoch 79/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3710 - acc: 0.8017 - K_F1_score: 0.6028\n",
      "Epoch 80/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.4287 - acc: 0.7858 - K_F1_score: 0.5853\n",
      "Epoch 81/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.2749 - acc: 0.8003 - K_F1_score: 0.5993\n",
      "Epoch 82/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.4039 - acc: 0.7902 - K_F1_score: 0.5724\n",
      "Epoch 83/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.4416 - acc: 0.8032 - K_F1_score: 0.6165\n",
      "Epoch 84/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.4432 - acc: 0.8133 - K_F1_score: 0.6347\n",
      "Epoch 85/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.4098 - acc: 0.7858 - K_F1_score: 0.5927\n",
      "Epoch 86/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.4314 - acc: 0.8017 - K_F1_score: 0.6448\n",
      "Epoch 87/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3418 - acc: 0.8003 - K_F1_score: 0.6166\n",
      "Epoch 88/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.3651 - acc: 0.7988 - K_F1_score: 0.6388\n",
      "Epoch 89/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.3546 - acc: 0.7988 - K_F1_score: 0.6245\n",
      "Epoch 90/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.4157 - acc: 0.8104 - K_F1_score: 0.6329\n",
      "Epoch 91/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.3991 - acc: 0.7945 - K_F1_score: 0.6163\n",
      "Epoch 92/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3964 - acc: 0.8133 - K_F1_score: 0.6582\n",
      "Epoch 93/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.3966 - acc: 0.8061 - K_F1_score: 0.6554\n",
      "Epoch 94/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3407 - acc: 0.8017 - K_F1_score: 0.6449\n",
      "Epoch 95/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.3088 - acc: 0.8090 - K_F1_score: 0.6380\n",
      "Epoch 96/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.3154 - acc: 0.8017 - K_F1_score: 0.6164\n",
      "Epoch 97/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.4140 - acc: 0.7974 - K_F1_score: 0.6254\n",
      "Epoch 98/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3119 - acc: 0.7959 - K_F1_score: 0.6143\n",
      "Epoch 99/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.3179 - acc: 0.8032 - K_F1_score: 0.6124\n",
      "Epoch 100/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3336 - acc: 0.7945 - K_F1_score: 0.6010\n",
      "Epoch 101/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3798 - acc: 0.8090 - K_F1_score: 0.6376\n",
      "Epoch 102/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.4077 - acc: 0.8104 - K_F1_score: 0.6723\n",
      "Epoch 103/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.4084 - acc: 0.8032 - K_F1_score: 0.6536\n",
      "Epoch 104/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.4074 - acc: 0.8119 - K_F1_score: 0.6729\n",
      "Epoch 105/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.3650 - acc: 0.7931 - K_F1_score: 0.6259\n",
      "Epoch 106/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.3395 - acc: 0.7959 - K_F1_score: 0.6293\n",
      "Epoch 107/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3781 - acc: 0.8061 - K_F1_score: 0.6559\n",
      "Epoch 108/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.4210 - acc: 0.8191 - K_F1_score: 0.6418\n",
      "Epoch 109/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.3938 - acc: 0.8278 - K_F1_score: 0.6868\n",
      "Epoch 110/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.3716 - acc: 0.8003 - K_F1_score: 0.6093\n",
      "Epoch 111/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.3789 - acc: 0.8177 - K_F1_score: 0.6693\n",
      "Epoch 112/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.3815 - acc: 0.8205 - K_F1_score: 0.6597\n",
      "Epoch 113/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.2744 - acc: 0.8104 - K_F1_score: 0.6323\n",
      "Epoch 114/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3010 - acc: 0.8133 - K_F1_score: 0.6669\n",
      "Epoch 115/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3321 - acc: 0.8177 - K_F1_score: 0.6834\n",
      "Epoch 116/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.3285 - acc: 0.8234 - K_F1_score: 0.7038\n",
      "Epoch 117/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 5.3164 - acc: 0.8075 - K_F1_score: 0.6535\n",
      "Epoch 118/256\n",
      "691/691 [==============================] - 0s 333us/step - loss: 5.2945 - acc: 0.8003 - K_F1_score: 0.6370\n",
      "Epoch 119/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.3159 - acc: 0.8104 - K_F1_score: 0.6787\n",
      "Epoch 120/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.3982 - acc: 0.8234 - K_F1_score: 0.6735\n",
      "Epoch 121/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.2921 - acc: 0.8205 - K_F1_score: 0.7029\n",
      "Epoch 122/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3723 - acc: 0.8162 - K_F1_score: 0.6918\n",
      "Epoch 123/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3437 - acc: 0.8061 - K_F1_score: 0.6616\n",
      "Epoch 124/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.2695 - acc: 0.8090 - K_F1_score: 0.6744\n",
      "Epoch 125/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3022 - acc: 0.8061 - K_F1_score: 0.6512\n",
      "Epoch 126/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.2742 - acc: 0.8090 - K_F1_score: 0.6488\n",
      "Epoch 127/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.3076 - acc: 0.8119 - K_F1_score: 0.6518\n",
      "Epoch 128/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.3229 - acc: 0.8032 - K_F1_score: 0.6456\n",
      "Epoch 129/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.2799 - acc: 0.8148 - K_F1_score: 0.6829\n",
      "Epoch 130/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3199 - acc: 0.8148 - K_F1_score: 0.6913\n",
      "Epoch 131/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3681 - acc: 0.8177 - K_F1_score: 0.6969\n",
      "Epoch 132/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.3437 - acc: 0.8148 - K_F1_score: 0.6974\n",
      "Epoch 133/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3974 - acc: 0.8148 - K_F1_score: 0.6911\n",
      "Epoch 134/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.3174 - acc: 0.8104 - K_F1_score: 0.6898\n",
      "Epoch 135/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.4022 - acc: 0.8104 - K_F1_score: 0.6719\n",
      "Epoch 136/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.2829 - acc: 0.8133 - K_F1_score: 0.6831\n",
      "Epoch 137/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 275us/step - loss: 5.3422 - acc: 0.8104 - K_F1_score: 0.6590\n",
      "Epoch 138/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3511 - acc: 0.8119 - K_F1_score: 0.6628\n",
      "Epoch 139/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3574 - acc: 0.7959 - K_F1_score: 0.6453\n",
      "Epoch 140/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.4670 - acc: 0.8234 - K_F1_score: 0.7133\n",
      "Epoch 141/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.3673 - acc: 0.7988 - K_F1_score: 0.6479\n",
      "Epoch 142/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.2825 - acc: 0.7974 - K_F1_score: 0.6455\n",
      "Epoch 143/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.2600 - acc: 0.8090 - K_F1_score: 0.6584\n",
      "Epoch 144/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.4050 - acc: 0.8191 - K_F1_score: 0.6879\n",
      "Epoch 145/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.2965 - acc: 0.8234 - K_F1_score: 0.7013\n",
      "Epoch 146/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.3033 - acc: 0.8061 - K_F1_score: 0.6570\n",
      "Epoch 147/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.4421 - acc: 0.8191 - K_F1_score: 0.6880\n",
      "Epoch 148/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3276 - acc: 0.8234 - K_F1_score: 0.7001\n",
      "Epoch 149/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3164 - acc: 0.8075 - K_F1_score: 0.6559\n",
      "Epoch 150/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.4357 - acc: 0.8263 - K_F1_score: 0.7036\n",
      "Epoch 151/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3276 - acc: 0.8162 - K_F1_score: 0.6713\n",
      "Epoch 152/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3796 - acc: 0.8191 - K_F1_score: 0.6930\n",
      "Epoch 153/256\n",
      "691/691 [==============================] - 0s 217us/step - loss: 5.3092 - acc: 0.8177 - K_F1_score: 0.6943\n",
      "Epoch 154/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.2898 - acc: 0.8090 - K_F1_score: 0.6697\n",
      "Epoch 155/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.3073 - acc: 0.8220 - K_F1_score: 0.7216\n",
      "Epoch 156/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.2848 - acc: 0.8075 - K_F1_score: 0.6634\n",
      "Epoch 157/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.4287 - acc: 0.8234 - K_F1_score: 0.6890\n",
      "Epoch 158/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3221 - acc: 0.8119 - K_F1_score: 0.6737\n",
      "Epoch 159/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.3480 - acc: 0.8104 - K_F1_score: 0.6585\n",
      "Epoch 160/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.3009 - acc: 0.8090 - K_F1_score: 0.6868\n",
      "Epoch 161/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3411 - acc: 0.8234 - K_F1_score: 0.7033\n",
      "Epoch 162/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.3510 - acc: 0.8205 - K_F1_score: 0.6939\n",
      "Epoch 163/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.4018 - acc: 0.8263 - K_F1_score: 0.7132\n",
      "Epoch 164/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.2740 - acc: 0.8148 - K_F1_score: 0.6863\n",
      "Epoch 165/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 5.3425 - acc: 0.8061 - K_F1_score: 0.6632\n",
      "Epoch 166/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3667 - acc: 0.8090 - K_F1_score: 0.7060\n",
      "Epoch 167/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.2379 - acc: 0.8162 - K_F1_score: 0.7036\n",
      "Epoch 168/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.3414 - acc: 0.8119 - K_F1_score: 0.6805\n",
      "Epoch 169/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3595 - acc: 0.8148 - K_F1_score: 0.6730\n",
      "Epoch 170/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.4091 - acc: 0.8162 - K_F1_score: 0.6829\n",
      "Epoch 171/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3889 - acc: 0.8148 - K_F1_score: 0.6776\n",
      "Epoch 172/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.4142 - acc: 0.8205 - K_F1_score: 0.6786\n",
      "Epoch 173/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.2459 - acc: 0.8307 - K_F1_score: 0.7063\n",
      "Epoch 174/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.2865 - acc: 0.8133 - K_F1_score: 0.6867\n",
      "Epoch 175/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.2866 - acc: 0.8061 - K_F1_score: 0.6688\n",
      "Epoch 176/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.2346 - acc: 0.8249 - K_F1_score: 0.6897\n",
      "Epoch 177/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3431 - acc: 0.8162 - K_F1_score: 0.6740\n",
      "Epoch 178/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.2710 - acc: 0.8162 - K_F1_score: 0.6772\n",
      "Epoch 179/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3870 - acc: 0.8148 - K_F1_score: 0.6990\n",
      "Epoch 180/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3321 - acc: 0.8220 - K_F1_score: 0.7008\n",
      "Epoch 181/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.3216 - acc: 0.8177 - K_F1_score: 0.6902\n",
      "Epoch 182/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.4273 - acc: 0.8205 - K_F1_score: 0.7023\n",
      "Epoch 183/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3345 - acc: 0.8379 - K_F1_score: 0.7416\n",
      "Epoch 184/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.2979 - acc: 0.8177 - K_F1_score: 0.7115\n",
      "Epoch 185/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.3460 - acc: 0.8177 - K_F1_score: 0.7193\n",
      "Epoch 186/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3352 - acc: 0.8119 - K_F1_score: 0.6991\n",
      "Epoch 187/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.3226 - acc: 0.8090 - K_F1_score: 0.6654\n",
      "Epoch 188/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.3232 - acc: 0.8162 - K_F1_score: 0.6953\n",
      "Epoch 189/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.3796 - acc: 0.8148 - K_F1_score: 0.6892\n",
      "Epoch 190/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.3805 - acc: 0.8205 - K_F1_score: 0.7056\n",
      "Epoch 191/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3960 - acc: 0.8205 - K_F1_score: 0.7058\n",
      "Epoch 192/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.3595 - acc: 0.8249 - K_F1_score: 0.6904\n",
      "Epoch 193/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3568 - acc: 0.8278 - K_F1_score: 0.7447\n",
      "Epoch 194/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3184 - acc: 0.8133 - K_F1_score: 0.6913\n",
      "Epoch 195/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3380 - acc: 0.8162 - K_F1_score: 0.6880\n",
      "Epoch 196/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3003 - acc: 0.8191 - K_F1_score: 0.6998\n",
      "Epoch 197/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.3630 - acc: 0.8148 - K_F1_score: 0.6851\n",
      "Epoch 198/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.2722 - acc: 0.8205 - K_F1_score: 0.6825\n",
      "Epoch 199/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.2877 - acc: 0.8249 - K_F1_score: 0.7169\n",
      "Epoch 200/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.3175 - acc: 0.8119 - K_F1_score: 0.6923\n",
      "Epoch 201/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.2997 - acc: 0.8234 - K_F1_score: 0.7093\n",
      "Epoch 202/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.4125 - acc: 0.8336 - K_F1_score: 0.7126\n",
      "Epoch 203/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3650 - acc: 0.8278 - K_F1_score: 0.7031\n",
      "Epoch 204/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 333us/step - loss: 5.2814 - acc: 0.8292 - K_F1_score: 0.7089\n",
      "Epoch 205/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 5.2649 - acc: 0.8205 - K_F1_score: 0.6953\n",
      "Epoch 206/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3486 - acc: 0.8162 - K_F1_score: 0.6886\n",
      "Epoch 207/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.2816 - acc: 0.8263 - K_F1_score: 0.6948\n",
      "Epoch 208/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.3443 - acc: 0.8307 - K_F1_score: 0.7071\n",
      "Epoch 209/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.2293 - acc: 0.8307 - K_F1_score: 0.7101\n",
      "Epoch 210/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.2683 - acc: 0.8292 - K_F1_score: 0.7228\n",
      "Epoch 211/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.2835 - acc: 0.8278 - K_F1_score: 0.7060\n",
      "Epoch 212/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.3324 - acc: 0.8307 - K_F1_score: 0.7129\n",
      "Epoch 213/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.1996 - acc: 0.8263 - K_F1_score: 0.7052\n",
      "Epoch 214/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.2224 - acc: 0.8249 - K_F1_score: 0.7085\n",
      "Epoch 215/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.2891 - acc: 0.8350 - K_F1_score: 0.7331\n",
      "Epoch 216/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3011 - acc: 0.8350 - K_F1_score: 0.7183\n",
      "Epoch 217/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.3444 - acc: 0.8249 - K_F1_score: 0.7140\n",
      "Epoch 218/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3701 - acc: 0.8220 - K_F1_score: 0.7141\n",
      "Epoch 219/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3019 - acc: 0.8220 - K_F1_score: 0.7219\n",
      "Epoch 220/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3123 - acc: 0.8220 - K_F1_score: 0.6993\n",
      "Epoch 221/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.2904 - acc: 0.8234 - K_F1_score: 0.7030\n",
      "Epoch 222/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.2923 - acc: 0.8263 - K_F1_score: 0.7031\n",
      "Epoch 223/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.2819 - acc: 0.8249 - K_F1_score: 0.7002\n",
      "Epoch 224/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.2650 - acc: 0.8350 - K_F1_score: 0.7174\n",
      "Epoch 225/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.2883 - acc: 0.8249 - K_F1_score: 0.7146\n",
      "Epoch 226/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 5.2700 - acc: 0.8307 - K_F1_score: 0.7037\n",
      "Epoch 227/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 5.3363 - acc: 0.8119 - K_F1_score: 0.7136\n",
      "Epoch 228/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 5.3287 - acc: 0.8321 - K_F1_score: 0.7350\n",
      "Epoch 229/256\n",
      "691/691 [==============================] - 0s 318us/step - loss: 5.2529 - acc: 0.8220 - K_F1_score: 0.7047\n",
      "Epoch 230/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.2902 - acc: 0.8263 - K_F1_score: 0.7111\n",
      "Epoch 231/256\n",
      "691/691 [==============================] - 0s 232us/step - loss: 5.2337 - acc: 0.8278 - K_F1_score: 0.6992\n",
      "Epoch 232/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.2896 - acc: 0.8336 - K_F1_score: 0.7133\n",
      "Epoch 233/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.3680 - acc: 0.8307 - K_F1_score: 0.7181\n",
      "Epoch 234/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.2321 - acc: 0.8292 - K_F1_score: 0.7121\n",
      "Epoch 235/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 5.3136 - acc: 0.8249 - K_F1_score: 0.7003\n",
      "Epoch 236/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.2494 - acc: 0.8307 - K_F1_score: 0.6933\n",
      "Epoch 237/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.2666 - acc: 0.8263 - K_F1_score: 0.7093\n",
      "Epoch 238/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.4423 - acc: 0.8292 - K_F1_score: 0.7061\n",
      "Epoch 239/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 5.2816 - acc: 0.8292 - K_F1_score: 0.7030\n",
      "Epoch 240/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.2388 - acc: 0.8321 - K_F1_score: 0.7197\n",
      "Epoch 241/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.2743 - acc: 0.8220 - K_F1_score: 0.7206\n",
      "Epoch 242/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.3759 - acc: 0.8278 - K_F1_score: 0.7249\n",
      "Epoch 243/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.4057 - acc: 0.8292 - K_F1_score: 0.7342\n",
      "Epoch 244/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.2407 - acc: 0.8249 - K_F1_score: 0.6948\n",
      "Epoch 245/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.2355 - acc: 0.8292 - K_F1_score: 0.7219\n",
      "Epoch 246/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.2031 - acc: 0.8249 - K_F1_score: 0.7047\n",
      "Epoch 247/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3638 - acc: 0.8292 - K_F1_score: 0.7137\n",
      "Epoch 248/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.3665 - acc: 0.8220 - K_F1_score: 0.6891\n",
      "Epoch 249/256\n",
      "691/691 [==============================] - 0s 246us/step - loss: 5.2396 - acc: 0.8249 - K_F1_score: 0.7025\n",
      "Epoch 250/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 5.3014 - acc: 0.8321 - K_F1_score: 0.7151\n",
      "Epoch 251/256\n",
      "691/691 [==============================] - 0s 289us/step - loss: 5.3301 - acc: 0.8263 - K_F1_score: 0.6960\n",
      "Epoch 252/256\n",
      "691/691 [==============================] - 0s 275us/step - loss: 5.3256 - acc: 0.8336 - K_F1_score: 0.7014\n",
      "Epoch 253/256\n",
      "691/691 [==============================] - 0s 304us/step - loss: 5.2201 - acc: 0.8278 - K_F1_score: 0.7068\n",
      "Epoch 254/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.2667 - acc: 0.8249 - K_F1_score: 0.7006\n",
      "Epoch 255/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.2849 - acc: 0.8307 - K_F1_score: 0.7110\n",
      "Epoch 256/256\n",
      "691/691 [==============================] - 0s 260us/step - loss: 5.2522 - acc: 0.8263 - K_F1_score: 0.6954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2792c320>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lams = {}\n",
    "#regularizers.l2(0.01)\n",
    "test_model = NN_model_v2((X_Train.shape[1], ), layers, None, None)\n",
    "test_model.compile(optimizer = \"sgd\", loss = aucpr_loss, metrics = [\"accuracy\", K_F1_score])\n",
    "test_model.fit(x = X_Train, y = Y_Train, epochs = 256, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  83.06801736613603\n",
      "F1 Score =  0.7334851936218678\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True        161.0          17.0\n",
      "1  Pred False        100.0         413.0\n",
      "Accuracy =  89.5\n",
      "F1 Score =  0.864516129032258\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True         67.0           7.0\n",
      "1  Pred False         14.0         112.0\n"
     ]
    }
   ],
   "source": [
    "train_pred = test_model.predict(x = X_Train)\n",
    "cv_pred = test_model.predict(x = X_CV)\n",
    "\n",
    "train_hat = normalize_predictions(train_pred)\n",
    "cv_hat = normalize_predictions(cv_pred)\n",
    "\n",
    "show_acc(Y_Train, train_hat)\n",
    "show_acc(Y_CV, cv_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Score :  0.884168482207698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.884168482207698"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_ROC(Y_CV, cv_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So implementing a pseudo integral over multiple values seems to have yielded a somehwat interesting result with a poor training accuracy/F1 score but excelled cross validation results, however this is definitely a fluke.  Across multiple iterations it was somewhat inconsistent with convergence and yielded middling results.\n",
    "\n",
    "However, this is likely to be a useful metric to incorporate into future ensembles so worth keeping around.\n",
    "\n",
    "\n",
    "Overall the experiment into using non-differentiable objectives seems to have given encouraging results. The next step is therefore to implement AUCROC or other metrics, or potentially use genetic programming to develop entirely new metrics, or implement algorithms from Optimizing Non-decomposable Measures with Deep Networks, such as DUPLE which looked like a solid algorithm.  There's many more directions opened up by this relatively unexplored territory.\n",
    "\n",
    "Below is also an implementation of the P@R algorithm, however this seemed to not converge, whether through incorrect implementation or poor algorithm I don't know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for optimizing precision at fixed recall\n",
    "def p_at_r_loss(target, output, beta_tf = 0.7):\n",
    "\n",
    "    lam_tf = tf.Variable(0.01, tf.float32, name='lam_tf')\n",
    "    #alph_tf = tf.placeholder(tf.float32, name='alph_tf')\n",
    "    \n",
    "    #lam_tf = K.variable(lam_tf, name='lam_tf')\n",
    "    \n",
    "    zeros = tf.zeros_like(output)\n",
    "    ones = tf.ones_like(output)\n",
    "    \n",
    "    cond_z = tf.equal(zeros, target)\n",
    "    cont_one = tf.not_equal(zeros, target)\n",
    "    \n",
    "    ind_z = tf.where(cond_z)\n",
    "    ind_one = tf.where(cont_one)\n",
    "    \n",
    "    one_labels = tf.gather_nd(target, ind_one)\n",
    "    one_logits = tf.gather_nd(output, ind_one)\n",
    "    \n",
    "    z_labels = tf.gather_nd(target, ind_z)\n",
    "    z_logits = tf.gather_nd(output, ind_z)\n",
    "    \n",
    "    L_plus = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=one_labels, logits=one_logits))\n",
    "    L_minus = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=z_labels, logits=z_logits))\n",
    "    \n",
    "    Y_plus = tf.reduce_sum(one_labels)\n",
    "    \n",
    "\n",
    "    loss_one = tf.add(tf.add(tf.divide(L_plus, Y_plus), beta_tf), -1.0)\n",
    "                      \n",
    "    fin_loss = tf.add(tf.multiply(lam_tf, loss_one), L_minus)\n",
    "    \n",
    "    lam_tf = upd_lambda_pr(lam_tf,  L_plus, L_minus, Y_plus, beta_tf)\n",
    "    \n",
    "    return fin_loss\n",
    "\n",
    "#Define function to optimize lambda by gradient descent (or is it ascent?)\n",
    "def upd_lambda_pr(lam,  L_plus, L_minus, Y_plus, beta_tf):\n",
    "    lr = 0.01\n",
    "    \n",
    "    #Differential of loss by lambda\n",
    "    dl =  tf.add(tf.add(tf.divide(L_plus, Y_plus), beta_tf), -1.0)\n",
    "    new_lam = tf.add(lam, tf.multiply(lr, dl))\n",
    "    \n",
    "    return new_lam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
