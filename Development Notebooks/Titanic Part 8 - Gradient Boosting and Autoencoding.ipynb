{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Classification - Gradient Boosting and Autoencoding (Part 8)\n",
    "\n",
    "So the next primary methodologies I wish to learn more about are Gradient Boosting, so as building a decision tree from scratch was incredibly useful.  So now I wish to build a Gradient Boosting algorithm from scratch to gain greater insight into how they work.\n",
    "\n",
    "## Building a Gradient Boosting Machine\n",
    "\n",
    "So my sources for learning Gradient boosting are as follows - \n",
    "\n",
    "https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d\n",
    "\n",
    "From my understanding this is a technique for effectively ensembling other ML algorithms by recursively fitting the residuals of the previous algorithm.\n",
    "\n",
    "This seems like a relatively straightforward series of steps - \n",
    "\n",
    "1. Train initial model\n",
    "2. Calculate initial residuals - ie $ R_1 = {y} - \\hat{y_1}$\n",
    "3. Train new model on residuals $R$ as this model's y vector\n",
    "4. Calculate new final y output as $\\hat{y_2} = \\hat{y_1} + R_1$\n",
    "5. Iterate steps 2 through 4 as many times as necessary as $\\hat{y_{n+1}} = \\hat{y_n} + R_n$\n",
    "\n",
    "With this frameworks multiple models of either the same or differing algorithms/architectures can be used as a way of ensembling models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#First importing some relevant packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Stop pandas from truncating output view\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "#Import Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "#Import Keras\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Dropout, Reshape, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "#Import mathematical functions\n",
    "from random import *\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Get regular expression package\n",
    "import re\n",
    "\n",
    "#Import  Scikit learn framework\n",
    "import sklearn as sk\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the functions built in previous parts\n",
    "from Titanic_Import import *\n",
    "\n",
    "full_set = pd.read_csv('D:/Datasets/Titanic/train.csv')\n",
    "sub_set = pd.read_csv('D:/Datasets/Titanic/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_set = full_set\n",
    "append_set = append_set.append([sub_set], ignore_index =True )\n",
    "clean_set = Cleanse_Data_v3(append_set)\n",
    "X_Train, Y_Train, X_CV, Y_CV, X_Test = dataset_splitter(clean_set, cv_size = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity I will import the linear regression model from SciKit learn, although in principle any model that can perform linear and logistic regression should work for gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "logreg.fit(X_Train, Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat_probs = logreg.predict_proba(X_Train)\n",
    "y_hat_cv_probs = logreg.predict_proba(X_CV)\n",
    "\n",
    "y_hat =  normalize_predictions(y_hat_probs[:, 1])\n",
    "y_hat_cv = normalize_predictions(y_hat_cv_probs[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  83.79160636758321\n",
      "F1 Score =  0.7878787878787878\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "      <th>Actual True</th>\n",
       "      <th>Actual False</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pred True</td>\n",
       "      <td>208.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pred False</td>\n",
       "      <td>58.0</td>\n",
       "      <td>371.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Labels  Actual True  Actual False\n",
       "0   Pred True        208.0          54.0\n",
       "1  Pred False         58.0         371.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc5, score5, conf5 = Calc_Accuracy(Y_Train, y_hat)\n",
    "\n",
    "print(\"Accuracy = \", acc5)\n",
    "print(\"F1 Score = \", score5)\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix\")\n",
    "conf5[[\"Labels\", \"Actual True\", \"Actual False\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  86.5\n",
      "F1 Score =  0.8280254777070064\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "      <th>Actual True</th>\n",
       "      <th>Actual False</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pred True</td>\n",
       "      <td>65.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pred False</td>\n",
       "      <td>11.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Labels  Actual True  Actual False\n",
       "0   Pred True         65.0          16.0\n",
       "1  Pred False         11.0         108.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc5, score5, conf5 = Calc_Accuracy(Y_CV, y_hat_cv)\n",
    "\n",
    "print(\"Accuracy = \", acc5)\n",
    "print(\"F1 Score = \", score5)\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix\")\n",
    "conf5[[\"Labels\", \"Actual True\", \"Actual False\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this seems to work reasonably well, so it seems like a solid start to building a gradient boosting model, the first model will be a logistic regression model as above.  Every subsequent model will then be a SVM regression model to predict the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Gradboost_Mach(object):\n",
    "    def __init__(self, num_models, C = 1e5, max_iter = 100):\n",
    "        self.num_models = num_models\n",
    "        self.models = {}\n",
    "        for i in range(num_models) :\n",
    "            if i == 0 :\n",
    "                self.models['mod' + str(i)] = linear_model.LogisticRegression(C = C, max_iter = max_iter)\n",
    "            else :\n",
    "                self.models['mod' + str(i)] = svm.SVR(kernel= 'rbf')\n",
    "    \n",
    "    def Train_models(self, X_in, Y_in, print_cost = False):\n",
    "        Y_out1 = np.zeros((Y_in.shape[0], 2))\n",
    "        Y_out2 = np.zeros(Y_in.shape)\n",
    "        Y_vec = {}\n",
    "        Residuals = {}\n",
    "        for i in range(self.num_models):\n",
    "            Residuals['R_' + str(i)] = np.zeros(Y_in.shape)\n",
    "            if i == 0 :\n",
    "                self.models['mod' + str(i)].fit(X_in, Y_in)\n",
    "                Y_out1 = self.models['mod' + str(i)].predict_proba(X_in)\n",
    "                Y_vec['Y_' + str(i)] = Y_out1[:, 1]\n",
    "                Residuals['R_' + str(i)] = Y_Train - Y_out1[:, 1]\n",
    "                if print_cost == True :\n",
    "                    Cost = sum(np.square(Residuals['R_' + str(i)]))\n",
    "                    print('Cost' + repr(i) + ' = ' + repr(Cost))\n",
    "            else :\n",
    "                Fit_R = Residuals['R_' + str(i - 1)]\n",
    "                self.models['mod' + str(i)].fit(X_in, Fit_R)\n",
    "                Y_out2 = self.models['mod' + str(i)].predict(X_in)\n",
    "                Y_vec['Y_' + str(i)] = Y_vec['Y_' + str(i - 1)] + Y_out2\n",
    "                Residuals['R_' + str(i)] = Y_Train - Y_vec['Y_' + str(i)]\n",
    "                \n",
    "                if print_cost == True :\n",
    "                    Cost = sum(np.square(Residuals['R_' + str(i)]))\n",
    "                    print('Cost' + repr(i) + ' = ' + repr(Cost))\n",
    "        \n",
    "            if i == (self.num_models - 1) :\n",
    "                Cost = np.square(Residuals['R_' + str(i)])\n",
    "                return(sum(Cost))\n",
    "            \n",
    "    def Predict(self, X_in):\n",
    "        Y_mat = np.zeros((X_in.shape[0], self.num_models))\n",
    "        \n",
    "        Y_out1 = np.zeros((X_in.shape[0], 2))\n",
    "        Y_out2 = np.zeros((X_in.shape[0],))\n",
    "        \n",
    "        for i in range(self.num_models):\n",
    "            if i == 0 :\n",
    "                Y_out1 = self.models['mod' + str(i)].predict_proba(X_in)\n",
    "                Y_mat[:, 0] = Y_out1[:, 1]\n",
    "            else :\n",
    "                Y_out2 = self.models['mod' + str(i)].predict(X_in)\n",
    "                Y_mat[:, i] = Y_out2\n",
    "        \n",
    "        Y_hat = sum(Y_mat.T)\n",
    "        return(Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_gb = Gradboost_Mach(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost0 = 87.92865952304959\n",
      "Cost1 = 90.4744950734984\n",
      "Cost2 = 87.70498111702356\n",
      "Cost3 = 85.61755209853388\n",
      "Cost4 = 83.98881812717252\n",
      "Cost5 = 83.0019243835363\n",
      "Cost6 = 82.51315991667212\n",
      "Cost7 = 82.24552506033704\n",
      "Cost8 = 81.97716104803071\n",
      "Cost9 = 81.62903814225024\n",
      "Cost10 = 81.38150571039424\n",
      "Cost11 = 81.30926023001815\n",
      "Cost12 = 81.13242825607892\n",
      "Cost13 = 81.01977639217549\n",
      "Cost14 = 80.84814940947396\n",
      "Cost15 = 80.67313447187216\n",
      "Cost16 = 80.44432627133946\n",
      "Cost17 = 80.25963559144164\n",
      "Cost18 = 80.08487951065705\n",
      "Cost19 = 79.93956085888048\n",
      "Cost20 = 79.77206679937129\n",
      "Cost21 = 79.61006242006653\n",
      "Cost22 = 79.48575075383445\n",
      "Cost23 = 79.37258964762302\n",
      "Cost24 = 79.26759740287481\n",
      "Cost25 = 79.18222910018264\n",
      "Cost26 = 79.07739076000472\n",
      "Cost27 = 78.92899622775732\n",
      "Cost28 = 78.78947460867097\n",
      "Cost29 = 78.66287874919408\n",
      "Cost30 = 78.51929464224283\n",
      "Cost31 = 78.40608623861706\n",
      "Cost32 = 78.2836686741221\n",
      "Cost33 = 78.15418825752234\n",
      "Cost34 = 78.04790894013358\n",
      "Cost35 = 77.93790880157076\n",
      "Cost36 = 77.83786897481477\n",
      "Cost37 = 77.73919173528691\n",
      "Cost38 = 77.65021426605273\n",
      "Cost39 = 77.569280182867\n",
      "Cost40 = 77.46016064800402\n",
      "Cost41 = 77.36675403122072\n",
      "Cost42 = 77.30012348000223\n",
      "Cost43 = 77.24065132939613\n",
      "Cost44 = 77.16624755071582\n",
      "Cost45 = 77.10450348643381\n",
      "Cost46 = 77.0391700972526\n",
      "Cost47 = 76.98376963328398\n",
      "Cost48 = 76.92541186132506\n",
      "Cost49 = 76.87003057481986\n",
      "Cost50 = 76.81719574067445\n",
      "Cost51 = 76.74400017766544\n",
      "Cost52 = 76.68505230469704\n",
      "Cost53 = 76.62076291658785\n",
      "Cost54 = 76.56298256701636\n",
      "Cost55 = 76.49921841228769\n",
      "Cost56 = 76.45383954034833\n",
      "Cost57 = 76.4048456466256\n",
      "Cost58 = 76.35177882576588\n",
      "Cost59 = 76.29819587645274\n",
      "Cost60 = 76.25619872812561\n",
      "Cost61 = 76.19476256464944\n",
      "Cost62 = 76.12504172181825\n",
      "Cost63 = 76.06072124396817\n",
      "Cost64 = 76.00224626179696\n",
      "Cost65 = 75.93279617102105\n",
      "Cost66 = 75.86494190752926\n",
      "Cost67 = 75.78940451223704\n",
      "Cost68 = 75.71483953169216\n",
      "Cost69 = 75.63952562666887\n",
      "Cost70 = 75.57760447494152\n",
      "Cost71 = 75.50548859799217\n",
      "Cost72 = 75.42942357135223\n",
      "Cost73 = 75.36602896591528\n",
      "Cost74 = 75.29716571769822\n",
      "Cost75 = 75.24044251517945\n",
      "Cost76 = 75.1727654622947\n",
      "Cost77 = 75.12004300299202\n",
      "Cost78 = 75.06085542017033\n",
      "Cost79 = 75.00882161802147\n",
      "Cost80 = 74.95943323909832\n",
      "Cost81 = 74.91064996681123\n",
      "Cost82 = 74.86021308898896\n",
      "Cost83 = 74.79883295021419\n",
      "Cost84 = 74.74372660857847\n",
      "Cost85 = 74.70695292079051\n",
      "Cost86 = 74.64944050511832\n",
      "Cost87 = 74.59813527461796\n",
      "Cost88 = 74.53978169283234\n",
      "Cost89 = 74.49271100464962\n",
      "Cost90 = 74.43492003872187\n",
      "Cost91 = 74.38710923247623\n",
      "Cost92 = 74.33067008235994\n",
      "Cost93 = 74.28113234711486\n",
      "Cost94 = 74.23418218126118\n",
      "Cost95 = 74.16828395949854\n",
      "Cost96 = 74.12784747246795\n",
      "Cost97 = 74.0716116250109\n",
      "Cost98 = 74.02949086570409\n",
      "Cost99 = 73.98228392301557\n",
      "Cost100 = 73.9348498692603\n",
      "Cost101 = 73.89101185804246\n",
      "Cost102 = 73.85902952452372\n",
      "Cost103 = 73.82832970584107\n",
      "Cost104 = 73.78959718530461\n",
      "Cost105 = 73.75667746576559\n",
      "Cost106 = 73.7207431076273\n",
      "Cost107 = 73.68318311263901\n",
      "Cost108 = 73.64343790817954\n",
      "Cost109 = 73.59981713021784\n",
      "Cost110 = 73.56518219208672\n",
      "Cost111 = 73.52413307262127\n",
      "Cost112 = 73.49300270568168\n",
      "Cost113 = 73.45219029405499\n",
      "Cost114 = 73.42218187446888\n",
      "Cost115 = 73.38746345731981\n",
      "Cost116 = 73.34856735313603\n",
      "Cost117 = 73.31141130601962\n",
      "Cost118 = 73.27954166868071\n",
      "Cost119 = 73.24883491913464\n",
      "Cost120 = 73.21927549995584\n",
      "Cost121 = 73.19030347622977\n",
      "Cost122 = 73.15468294650034\n",
      "Cost123 = 73.12316216941366\n",
      "Cost124 = 73.09883917099565\n",
      "Cost125 = 73.06940264165439\n",
      "Cost126 = 73.03752144484703\n",
      "Cost127 = 73.00887499074412\n",
      "Cost128 = 72.9832327299506\n",
      "Cost129 = 72.95575155095143\n",
      "Cost130 = 72.92790048855557\n",
      "Cost131 = 72.8971128158215\n",
      "Cost132 = 72.86742671719206\n",
      "Cost133 = 72.83758122876948\n",
      "Cost134 = 72.81637148620946\n",
      "Cost135 = 72.7852365722737\n",
      "Cost136 = 72.7568056976664\n",
      "Cost137 = 72.73332822559463\n",
      "Cost138 = 72.70729300912227\n",
      "Cost139 = 72.67961808081841\n",
      "Cost140 = 72.65095869573331\n",
      "Cost141 = 72.6292820097405\n",
      "Cost142 = 72.59830659830867\n",
      "Cost143 = 72.57570647242528\n",
      "Cost144 = 72.55021721135128\n",
      "Cost145 = 72.5188003537691\n",
      "Cost146 = 72.50135865912894\n",
      "Cost147 = 72.47700473131334\n",
      "Cost148 = 72.45543030514767\n",
      "Cost149 = 72.42779135729566\n",
      "Cost150 = 72.40194970299068\n",
      "Cost151 = 72.37484970939914\n",
      "Cost152 = 72.34915121873969\n",
      "Cost153 = 72.3320873504535\n",
      "Cost154 = 72.30236625610777\n",
      "Cost155 = 72.27092143214537\n",
      "Cost156 = 72.24760314363195\n",
      "Cost157 = 72.2327947842896\n",
      "Cost158 = 72.19970164330475\n",
      "Cost159 = 72.18120758281056\n",
      "Cost160 = 72.1575296838306\n",
      "Cost161 = 72.12709111737776\n",
      "Cost162 = 72.10261802416193\n",
      "Cost163 = 72.08028280510176\n",
      "Cost164 = 72.05207843802964\n",
      "Cost165 = 72.04013277773271\n",
      "Cost166 = 72.01614217904536\n",
      "Cost167 = 71.99807963621464\n",
      "Cost168 = 71.9808092375601\n",
      "Cost169 = 71.95203867416\n",
      "Cost170 = 71.93051922137803\n",
      "Cost171 = 71.91268293387883\n",
      "Cost172 = 71.89005575036416\n",
      "Cost173 = 71.872369563684\n",
      "Cost174 = 71.84004790755525\n",
      "Cost175 = 71.81862142525023\n",
      "Cost176 = 71.7904834588913\n",
      "Cost177 = 71.76695003047796\n",
      "Cost178 = 71.74341036371386\n",
      "Cost179 = 71.71583251741126\n",
      "Cost180 = 71.69228807566196\n",
      "Cost181 = 71.66674499790598\n",
      "Cost182 = 71.64252253262548\n",
      "Cost183 = 71.61950991140924\n",
      "Cost184 = 71.59141607418374\n",
      "Cost185 = 71.57507112146541\n",
      "Cost186 = 71.5500784169398\n",
      "Cost187 = 71.53403567392988\n",
      "Cost188 = 71.51030908394111\n",
      "Cost189 = 71.48980745780541\n",
      "Cost190 = 71.46773227965213\n",
      "Cost191 = 71.44300603519993\n",
      "Cost192 = 71.4233647078111\n",
      "Cost193 = 71.39776188687354\n",
      "Cost194 = 71.37912348265579\n",
      "Cost195 = 71.3599025273259\n",
      "Cost196 = 71.34315152292014\n",
      "Cost197 = 71.31954547909224\n",
      "Cost198 = 71.29638392578887\n",
      "Cost199 = 71.28094943146677\n",
      "Cost200 = 71.26503451631534\n",
      "Cost201 = 71.24860597636649\n",
      "Cost202 = 71.23350709286143\n",
      "Cost203 = 71.22525361880847\n",
      "Cost204 = 71.20051139914322\n",
      "Cost205 = 71.19007349764077\n",
      "Cost206 = 71.17788237349069\n",
      "Cost207 = 71.14963469555738\n",
      "Cost208 = 71.13300900001488\n",
      "Cost209 = 71.11398188579909\n",
      "Cost210 = 71.09870407021658\n",
      "Cost211 = 71.07917160923225\n",
      "Cost212 = 71.05583867137058\n",
      "Cost213 = 71.04584903555345\n",
      "Cost214 = 71.03150787591936\n",
      "Cost215 = 70.99878731023934\n",
      "Cost216 = 70.98461030431731\n",
      "Cost217 = 70.96996023237301\n",
      "Cost218 = 70.9473011254182\n",
      "Cost219 = 70.93868249253413\n",
      "Cost220 = 70.92241441871447\n",
      "Cost221 = 70.90428254421407\n",
      "Cost222 = 70.89269895287356\n",
      "Cost223 = 70.87602677811424\n",
      "Cost224 = 70.86229072546321\n",
      "Cost225 = 70.84913063128587\n",
      "Cost226 = 70.83244773132112\n",
      "Cost227 = 70.818530701649\n",
      "Cost228 = 70.80277441373246\n",
      "Cost229 = 70.79359402716487\n",
      "Cost230 = 70.78146889484053\n",
      "Cost231 = 70.7728482115108\n",
      "Cost232 = 70.75948366408731\n",
      "Cost233 = 70.7469281639029\n",
      "Cost234 = 70.7321078313828\n",
      "Cost235 = 70.72202925936993\n",
      "Cost236 = 70.71109172223461\n",
      "Cost237 = 70.70321077037451\n",
      "Cost238 = 70.69528687331979\n",
      "Cost239 = 70.68250841020237\n",
      "Cost240 = 70.67387542314451\n",
      "Cost241 = 70.66122800094034\n",
      "Cost242 = 70.65316455166378\n",
      "Cost243 = 70.64206543756117\n",
      "Cost244 = 70.63287435508865\n",
      "Cost245 = 70.63224537109363\n",
      "Cost246 = 70.61687177029167\n",
      "Cost247 = 70.60754175905511\n",
      "Cost248 = 70.59283006628439\n",
      "Cost249 = 70.58774174872167\n",
      "Cost250 = 70.58699907409624\n",
      "Cost251 = 70.57454075430768\n",
      "Cost252 = 70.56775411646709\n",
      "Cost253 = 70.55229135398604\n",
      "Cost254 = 70.54851303923058\n",
      "Cost255 = 70.53891050862761\n",
      "Cost256 = 70.52854778516685\n",
      "Cost257 = 70.5154066514699\n",
      "Cost258 = 70.50606930533155\n",
      "Cost259 = 70.4984861324735\n",
      "Cost260 = 70.48594868155227\n",
      "Cost261 = 70.47742824656754\n",
      "Cost262 = 70.4669608810968\n",
      "Cost263 = 70.45463717871002\n",
      "Cost264 = 70.4533043622599\n",
      "Cost265 = 70.43943044216674\n",
      "Cost266 = 70.42521860947139\n",
      "Cost267 = 70.41596432604587\n",
      "Cost268 = 70.4014799749461\n",
      "Cost269 = 70.39729258729916\n",
      "Cost270 = 70.38271315616987\n",
      "Cost271 = 70.38164368120037\n",
      "Cost272 = 70.37202270901935\n",
      "Cost273 = 70.36223539286551\n",
      "Cost274 = 70.35697558369279\n",
      "Cost275 = 70.34141185828202\n",
      "Cost276 = 70.3384826957394\n",
      "Cost277 = 70.32741452388365\n",
      "Cost278 = 70.31350600625363\n",
      "Cost279 = 70.31108465448918\n",
      "Cost280 = 70.29485991060606\n",
      "Cost281 = 70.28558248855407\n",
      "Cost282 = 70.27833634253632\n",
      "Cost283 = 70.26720705804355\n",
      "Cost284 = 70.26158184037982\n",
      "Cost285 = 70.25376178026931\n",
      "Cost286 = 70.24639456559544\n",
      "Cost287 = 70.23159569600583\n",
      "Cost288 = 70.21656327464592\n",
      "Cost289 = 70.21642252273982\n",
      "Cost290 = 70.20123901102558\n",
      "Cost291 = 70.18703822059513\n",
      "Cost292 = 70.1764510449741\n",
      "Cost293 = 70.16975739755517\n",
      "Cost294 = 70.15770371370488\n",
      "Cost295 = 70.14390045040206\n",
      "Cost296 = 70.13526263995186\n",
      "Cost297 = 70.12757581825934\n",
      "Cost298 = 70.11731477018016\n",
      "Cost299 = 70.1052193862709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost300 = 70.09358497569525\n",
      "Cost301 = 70.07977897814355\n",
      "Cost302 = 70.07471203639832\n",
      "Cost303 = 70.05757846565517\n",
      "Cost304 = 70.05472931836687\n",
      "Cost305 = 70.05127584542203\n",
      "Cost306 = 70.03713996372207\n",
      "Cost307 = 70.03485552278268\n",
      "Cost308 = 70.02506185445027\n",
      "Cost309 = 70.02070563434214\n",
      "Cost310 = 70.0084136885788\n",
      "Cost311 = 70.0045028059789\n",
      "Cost312 = 69.99367919566845\n",
      "Cost313 = 69.98904115845909\n",
      "Cost314 = 69.97861863660289\n",
      "Cost315 = 69.97694324982793\n",
      "Cost316 = 69.97037072829553\n",
      "Cost317 = 69.96304017857669\n",
      "Cost318 = 69.95883205778233\n",
      "Cost319 = 69.95170586687784\n",
      "Cost320 = 69.94215627715494\n",
      "Cost321 = 69.93571756539555\n",
      "Cost322 = 69.93527216181757\n",
      "Cost323 = 69.92759178392494\n",
      "Cost324 = 69.92317689634065\n",
      "Cost325 = 69.9150052836755\n",
      "Cost326 = 69.9118405219583\n",
      "Cost327 = 69.90559585474344\n",
      "Cost328 = 69.8977398289078\n",
      "Cost329 = 69.8908147264205\n",
      "Cost330 = 69.88316175116366\n",
      "Cost331 = 69.88086373237627\n",
      "Cost332 = 69.87477637991815\n",
      "Cost333 = 69.87046800197245\n",
      "Cost334 = 69.86169014854931\n",
      "Cost335 = 69.85640041775041\n",
      "Cost336 = 69.85315876997137\n",
      "Cost337 = 69.84469070246853\n",
      "Cost338 = 69.84309855231069\n",
      "Cost339 = 69.83548863987805\n",
      "Cost340 = 69.83124589692072\n",
      "Cost341 = 69.82414864473255\n",
      "Cost342 = 69.81661114132197\n",
      "Cost343 = 69.81423310643561\n",
      "Cost344 = 69.81185807350842\n",
      "Cost345 = 69.80229772359378\n",
      "Cost346 = 69.79676665000088\n",
      "Cost347 = 69.78838469766424\n",
      "Cost348 = 69.7839773937731\n",
      "Cost349 = 69.77769673706068\n",
      "Cost350 = 69.77851959649684\n",
      "Cost351 = 69.76735193968875\n",
      "Cost352 = 69.76267963123556\n",
      "Cost353 = 69.76381141474636\n",
      "Cost354 = 69.7548812935726\n",
      "Cost355 = 69.74644389540357\n",
      "Cost356 = 69.73889070247614\n",
      "Cost357 = 69.73853190808951\n",
      "Cost358 = 69.73551762316004\n",
      "Cost359 = 69.72416150793525\n",
      "Cost360 = 69.7205425222401\n",
      "Cost361 = 69.71197305179331\n",
      "Cost362 = 69.70660334375904\n",
      "Cost363 = 69.69966577927845\n",
      "Cost364 = 69.69619978002387\n",
      "Cost365 = 69.68967408232812\n",
      "Cost366 = 69.68265649522081\n",
      "Cost367 = 69.67946909922465\n",
      "Cost368 = 69.66837695762011\n",
      "Cost369 = 69.66857853202325\n",
      "Cost370 = 69.66327439304735\n",
      "Cost371 = 69.66104592965675\n",
      "Cost372 = 69.65728629412368\n",
      "Cost373 = 69.64833989424942\n",
      "Cost374 = 69.648835001223\n",
      "Cost375 = 69.64463851737865\n",
      "Cost376 = 69.63809836757382\n",
      "Cost377 = 69.63283725666147\n",
      "Cost378 = 69.63045817269476\n",
      "Cost379 = 69.62712118566871\n",
      "Cost380 = 69.62282649105057\n",
      "Cost381 = 69.61410627393394\n",
      "Cost382 = 69.61050777370372\n",
      "Cost383 = 69.60857106348746\n",
      "Cost384 = 69.61068686951232\n",
      "Cost385 = 69.60048686539821\n",
      "Cost386 = 69.60166465285762\n",
      "Cost387 = 69.58877117555672\n",
      "Cost388 = 69.5914983490454\n",
      "Cost389 = 69.5897698146672\n",
      "Cost390 = 69.58574604978115\n",
      "Cost391 = 69.58072922613523\n",
      "Cost392 = 69.57766064529513\n",
      "Cost393 = 69.58013656423937\n",
      "Cost394 = 69.56587952256143\n",
      "Cost395 = 69.56598360788968\n",
      "Cost396 = 69.56011502111684\n",
      "Cost397 = 69.56133981160623\n",
      "Cost398 = 69.55040504380791\n",
      "Cost399 = 69.55670755977349\n",
      "Cost400 = 69.55131956055135\n",
      "Cost401 = 69.54766359736155\n",
      "Cost402 = 69.54168509272893\n",
      "Cost403 = 69.5402525253841\n",
      "Cost404 = 69.53502080278237\n",
      "Cost405 = 69.52637083450365\n",
      "Cost406 = 69.53011048504615\n",
      "Cost407 = 69.52730614542321\n",
      "Cost408 = 69.52712513698859\n",
      "Cost409 = 69.51635218344268\n",
      "Cost410 = 69.51871092861424\n",
      "Cost411 = 69.51559178202585\n",
      "Cost412 = 69.51266170628733\n",
      "Cost413 = 69.51209021965556\n",
      "Cost414 = 69.50137174875762\n",
      "Cost415 = 69.49785755531578\n",
      "Cost416 = 69.49409508430668\n",
      "Cost417 = 69.48632914047825\n",
      "Cost418 = 69.47497127622651\n",
      "Cost419 = 69.46890097939198\n",
      "Cost420 = 69.46454846293632\n",
      "Cost421 = 69.4581477994976\n",
      "Cost422 = 69.44574243954268\n",
      "Cost423 = 69.43732766960788\n",
      "Cost424 = 69.43070641424059\n",
      "Cost425 = 69.4216845781455\n",
      "Cost426 = 69.42033388239882\n",
      "Cost427 = 69.41088695315156\n",
      "Cost428 = 69.40017037906509\n",
      "Cost429 = 69.39877536123392\n",
      "Cost430 = 69.3934563083868\n",
      "Cost431 = 69.38498972052759\n",
      "Cost432 = 69.38152046434844\n",
      "Cost433 = 69.37484113997654\n",
      "Cost434 = 69.36762194264092\n",
      "Cost435 = 69.35692310674067\n",
      "Cost436 = 69.34616727684536\n",
      "Cost437 = 69.33668422331714\n",
      "Cost438 = 69.33272714956084\n",
      "Cost439 = 69.3258551290446\n",
      "Cost440 = 69.32154800754844\n",
      "Cost441 = 69.32556770319499\n",
      "Cost442 = 69.3124503071802\n",
      "Cost443 = 69.30711900821635\n",
      "Cost444 = 69.30493185553158\n",
      "Cost445 = 69.29023187705413\n",
      "Cost446 = 69.2847103001391\n",
      "Cost447 = 69.28373775370443\n",
      "Cost448 = 69.27832776547541\n",
      "Cost449 = 69.26353743150884\n",
      "Cost450 = 69.2588871735518\n",
      "Cost451 = 69.25142318094926\n",
      "Cost452 = 69.24774305322423\n",
      "Cost453 = 69.24187609941761\n",
      "Cost454 = 69.23672241728372\n",
      "Cost455 = 69.23054341771768\n",
      "Cost456 = 69.22374569849914\n",
      "Cost457 = 69.21918655406104\n",
      "Cost458 = 69.2122960440907\n",
      "Cost459 = 69.20123072516508\n",
      "Cost460 = 69.19426055293863\n",
      "Cost461 = 69.19531113069803\n",
      "Cost462 = 69.18665780091631\n",
      "Cost463 = 69.17589029814502\n",
      "Cost464 = 69.17733049994843\n",
      "Cost465 = 69.17189879965785\n",
      "Cost466 = 69.16650631507592\n",
      "Cost467 = 69.16021861473344\n",
      "Cost468 = 69.14611551856807\n",
      "Cost469 = 69.14907713512454\n",
      "Cost470 = 69.14011690417021\n",
      "Cost471 = 69.13329339844219\n",
      "Cost472 = 69.12765788367362\n",
      "Cost473 = 69.11855826018937\n",
      "Cost474 = 69.11369229665802\n",
      "Cost475 = 69.10719849877728\n",
      "Cost476 = 69.10563016322321\n",
      "Cost477 = 69.09913077597683\n",
      "Cost478 = 69.09161451484054\n",
      "Cost479 = 69.08460724020988\n",
      "Cost480 = 69.08235124139475\n",
      "Cost481 = 69.07028870417041\n",
      "Cost482 = 69.07584322715012\n",
      "Cost483 = 69.06434682264235\n",
      "Cost484 = 69.05817768987157\n",
      "Cost485 = 69.05561214000072\n",
      "Cost486 = 69.04911696933975\n",
      "Cost487 = 69.04461129971122\n",
      "Cost488 = 69.03932014298664\n",
      "Cost489 = 69.03375087958302\n",
      "Cost490 = 69.02541572768294\n",
      "Cost491 = 69.02532615474875\n",
      "Cost492 = 69.0260149407926\n",
      "Cost493 = 69.01886969228393\n",
      "Cost494 = 69.01166281835637\n",
      "Cost495 = 69.00176444833794\n",
      "Cost496 = 69.00463918516323\n",
      "Cost497 = 68.9933267740887\n",
      "Cost498 = 68.98956540710839\n",
      "Cost499 = 68.98266041908876\n",
      "Cost500 = 68.98615109705544\n",
      "Cost501 = 68.97975998342932\n",
      "Cost502 = 68.97291521191755\n",
      "Cost503 = 68.96460435589177\n",
      "Cost504 = 68.96593897696651\n",
      "Cost505 = 68.96328061347765\n",
      "Cost506 = 68.9552448730925\n",
      "Cost507 = 68.95340413509764\n",
      "Cost508 = 68.94895766873577\n",
      "Cost509 = 68.9431232842391\n",
      "Cost510 = 68.9447638429848\n",
      "Cost511 = 68.93767125771288\n",
      "Cost512 = 68.93049474247489\n",
      "Cost513 = 68.93085350725408\n",
      "Cost514 = 68.92410688682436\n",
      "Cost515 = 68.91765414615982\n",
      "Cost516 = 68.91032895988707\n",
      "Cost517 = 68.90550429555215\n",
      "Cost518 = 68.9030294326135\n",
      "Cost519 = 68.9007518945885\n",
      "Cost520 = 68.8990812325262\n",
      "Cost521 = 68.88897777678594\n",
      "Cost522 = 68.89023554651634\n",
      "Cost523 = 68.8831459814914\n",
      "Cost524 = 68.87439271591552\n",
      "Cost525 = 68.87581567874298\n",
      "Cost526 = 68.86874567950035\n",
      "Cost527 = 68.86471254501626\n",
      "Cost528 = 68.86361933809697\n",
      "Cost529 = 68.85485846285425\n",
      "Cost530 = 68.8544571304756\n",
      "Cost531 = 68.84753045221476\n",
      "Cost532 = 68.84643062601967\n",
      "Cost533 = 68.83462026556278\n",
      "Cost534 = 68.83496178095098\n",
      "Cost535 = 68.82894480219082\n",
      "Cost536 = 68.82997541316082\n",
      "Cost537 = 68.82318614563357\n",
      "Cost538 = 68.82201566866796\n",
      "Cost539 = 68.82011575367453\n",
      "Cost540 = 68.80720008575793\n",
      "Cost541 = 68.80927364276947\n",
      "Cost542 = 68.80575596899622\n",
      "Cost543 = 68.80578186001834\n",
      "Cost544 = 68.7988915104174\n",
      "Cost545 = 68.79673704335582\n",
      "Cost546 = 68.79244583058538\n",
      "Cost547 = 68.77773074466603\n",
      "Cost548 = 68.78894482623039\n",
      "Cost549 = 68.77450436616316\n",
      "Cost550 = 68.77757944540876\n",
      "Cost551 = 68.76936955422582\n",
      "Cost552 = 68.76479809293535\n",
      "Cost553 = 68.76018115460457\n",
      "Cost554 = 68.7556654262368\n",
      "Cost555 = 68.75685919026296\n",
      "Cost556 = 68.75738338105452\n",
      "Cost557 = 68.75208741724968\n",
      "Cost558 = 68.7402618758403\n",
      "Cost559 = 68.73900231314366\n",
      "Cost560 = 68.73522585994218\n",
      "Cost561 = 68.73422753088855\n",
      "Cost562 = 68.73160656429461\n",
      "Cost563 = 68.7324091998012\n",
      "Cost564 = 68.73165950694694\n",
      "Cost565 = 68.731300788729\n",
      "Cost566 = 68.72866705599395\n",
      "Cost567 = 68.72263199913449\n",
      "Cost568 = 68.72358300820093\n",
      "Cost569 = 68.71942596089796\n",
      "Cost570 = 68.71618540725824\n",
      "Cost571 = 68.71319193248654\n",
      "Cost572 = 68.71015275887105\n",
      "Cost573 = 68.70302253974201\n",
      "Cost574 = 68.7065785257896\n",
      "Cost575 = 68.70368139973118\n",
      "Cost576 = 68.69808527089597\n",
      "Cost577 = 68.69658404358167\n",
      "Cost578 = 68.69924821905046\n",
      "Cost579 = 68.69142216463013\n",
      "Cost580 = 68.69445173686785\n",
      "Cost581 = 68.69428923108813\n",
      "Cost582 = 68.68143493469313\n",
      "Cost583 = 68.68174877829112\n",
      "Cost584 = 68.6767036319797\n",
      "Cost585 = 68.67142901717769\n",
      "Cost586 = 68.67339473393363\n",
      "Cost587 = 68.67030804292166\n",
      "Cost588 = 68.66890239655591\n",
      "Cost589 = 68.66694671603489\n",
      "Cost590 = 68.66450068900136\n",
      "Cost591 = 68.65787488664834\n",
      "Cost592 = 68.6572088092161\n",
      "Cost593 = 68.65032196231866\n",
      "Cost594 = 68.6523413363839\n",
      "Cost595 = 68.64614145713654\n",
      "Cost596 = 68.64260826567276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost597 = 68.6442830940107\n",
      "Cost598 = 68.64283428841584\n",
      "Cost599 = 68.63517159908851\n",
      "Cost600 = 68.62748697008831\n",
      "Cost601 = 68.62935768470503\n",
      "Cost602 = 68.62924261568568\n",
      "Cost603 = 68.62072568908029\n",
      "Cost604 = 68.6168080602314\n",
      "Cost605 = 68.61339632109083\n",
      "Cost606 = 68.61114349486746\n",
      "Cost607 = 68.60592962601095\n",
      "Cost608 = 68.5997685465876\n",
      "Cost609 = 68.59878327076505\n",
      "Cost610 = 68.5964866861469\n",
      "Cost611 = 68.5879754369437\n",
      "Cost612 = 68.58878762433947\n",
      "Cost613 = 68.58343657852119\n",
      "Cost614 = 68.57513338221123\n",
      "Cost615 = 68.57163205287107\n",
      "Cost616 = 68.5724813161223\n",
      "Cost617 = 68.5622761530038\n",
      "Cost618 = 68.56015775100606\n",
      "Cost619 = 68.55712434279344\n",
      "Cost620 = 68.543286278423\n",
      "Cost621 = 68.5482015611161\n",
      "Cost622 = 68.54524583195354\n",
      "Cost623 = 68.53808675371204\n",
      "Cost624 = 68.53048355065071\n",
      "Cost625 = 68.52088589079732\n",
      "Cost626 = 68.51887164815919\n",
      "Cost627 = 68.51649685046982\n",
      "Cost628 = 68.51092946676619\n",
      "Cost629 = 68.50485522219985\n",
      "Cost630 = 68.5012567066731\n",
      "Cost631 = 68.49399246336306\n",
      "Cost632 = 68.49563524790742\n",
      "Cost633 = 68.48661626010967\n",
      "Cost634 = 68.48484804846903\n",
      "Cost635 = 68.47566291240086\n",
      "Cost636 = 68.46785177018579\n",
      "Cost637 = 68.47121275083619\n",
      "Cost638 = 68.46134036692341\n",
      "Cost639 = 68.45742001995708\n",
      "Cost640 = 68.4551509323986\n",
      "Cost641 = 68.44758973838402\n",
      "Cost642 = 68.44275089489739\n",
      "Cost643 = 68.44046595718953\n",
      "Cost644 = 68.43171353024803\n",
      "Cost645 = 68.43232175844008\n",
      "Cost646 = 68.42247253491838\n",
      "Cost647 = 68.4194150433089\n",
      "Cost648 = 68.41540319459016\n",
      "Cost649 = 68.40980713034818\n",
      "Cost650 = 68.40744990614603\n",
      "Cost651 = 68.40647523135095\n",
      "Cost652 = 68.3969949121731\n",
      "Cost653 = 68.39185722871655\n",
      "Cost654 = 68.38440051366845\n",
      "Cost655 = 68.37901564809705\n",
      "Cost656 = 68.37878427317074\n",
      "Cost657 = 68.37330779452633\n",
      "Cost658 = 68.37029600034771\n",
      "Cost659 = 68.36681540012908\n",
      "Cost660 = 68.36100409555287\n",
      "Cost661 = 68.35228486803942\n",
      "Cost662 = 68.34689325113469\n",
      "Cost663 = 68.34162667295789\n",
      "Cost664 = 68.34012191001686\n",
      "Cost665 = 68.33415201334483\n",
      "Cost666 = 68.33339993268848\n",
      "Cost667 = 68.33258951148738\n",
      "Cost668 = 68.32318732371913\n",
      "Cost669 = 68.31612922802289\n",
      "Cost670 = 68.31492697019279\n",
      "Cost671 = 68.30655659409322\n",
      "Cost672 = 68.30746715737247\n",
      "Cost673 = 68.29891972731214\n",
      "Cost674 = 68.294825805602\n",
      "Cost675 = 68.29086870042146\n",
      "Cost676 = 68.28954347152278\n",
      "Cost677 = 68.2822697365678\n",
      "Cost678 = 68.27368634160726\n",
      "Cost679 = 68.2782225769587\n",
      "Cost680 = 68.272641617291\n",
      "Cost681 = 68.26696594143392\n",
      "Cost682 = 68.2633649819462\n",
      "Cost683 = 68.25464224435325\n",
      "Cost684 = 68.25825140607377\n",
      "Cost685 = 68.24630230580333\n",
      "Cost686 = 68.25251658580446\n",
      "Cost687 = 68.24201376430584\n",
      "Cost688 = 68.24235615700546\n",
      "Cost689 = 68.24302879457821\n",
      "Cost690 = 68.23810183641261\n",
      "Cost691 = 68.2310222538366\n",
      "Cost692 = 68.22792252593315\n",
      "Cost693 = 68.22417763956282\n",
      "Cost694 = 68.22300773723484\n",
      "Cost695 = 68.22067315297662\n",
      "Cost696 = 68.21576564725889\n",
      "Cost697 = 68.20898341972307\n",
      "Cost698 = 68.20239145972288\n",
      "Cost699 = 68.20246587567797\n",
      "Cost700 = 68.1954211634035\n",
      "Cost701 = 68.19977176470519\n",
      "Cost702 = 68.19584391950345\n",
      "Cost703 = 68.1941406656295\n",
      "Cost704 = 68.19021845447276\n",
      "Cost705 = 68.19193487323692\n",
      "Cost706 = 68.18366323631739\n",
      "Cost707 = 68.17922095360953\n",
      "Cost708 = 68.17927450863614\n",
      "Cost709 = 68.17764194930338\n",
      "Cost710 = 68.17159078257903\n",
      "Cost711 = 68.16910626701265\n",
      "Cost712 = 68.16715726988846\n",
      "Cost713 = 68.16064524692071\n",
      "Cost714 = 68.16409967908739\n",
      "Cost715 = 68.16069424443732\n",
      "Cost716 = 68.15168809089253\n",
      "Cost717 = 68.15083752447522\n",
      "Cost718 = 68.15055882328544\n",
      "Cost719 = 68.14189602970805\n",
      "Cost720 = 68.1382857810219\n",
      "Cost721 = 68.14112114099134\n",
      "Cost722 = 68.13489619777225\n",
      "Cost723 = 68.1291511752485\n",
      "Cost724 = 68.12771793746614\n",
      "Cost725 = 68.13028830279214\n",
      "Cost726 = 68.1264490466685\n",
      "Cost727 = 68.12271391231575\n",
      "Cost728 = 68.1140318682584\n",
      "Cost729 = 68.11564890625735\n",
      "Cost730 = 68.11108781898847\n",
      "Cost731 = 68.11247735665546\n",
      "Cost732 = 68.10239696279021\n",
      "Cost733 = 68.10779458606054\n",
      "Cost734 = 68.09984176801574\n",
      "Cost735 = 68.09621440624262\n",
      "Cost736 = 68.09677125362374\n",
      "Cost737 = 68.09189919442838\n",
      "Cost738 = 68.09124613704098\n",
      "Cost739 = 68.08571231811895\n",
      "Cost740 = 68.08105507393087\n",
      "Cost741 = 68.08191672268646\n",
      "Cost742 = 68.0771493953778\n",
      "Cost743 = 68.07373366547706\n",
      "Cost744 = 68.07473209171111\n",
      "Cost745 = 68.0666700260924\n",
      "Cost746 = 68.06740529336298\n",
      "Cost747 = 68.06232358381502\n",
      "Cost748 = 68.06054328609383\n",
      "Cost749 = 68.05679182625856\n",
      "Cost750 = 68.05626920975169\n",
      "Cost751 = 68.05053445595347\n",
      "Cost752 = 68.0503044309237\n",
      "Cost753 = 68.0492942242039\n",
      "Cost754 = 68.04786679440923\n",
      "Cost755 = 68.03761122737768\n",
      "Cost756 = 68.03339186310748\n",
      "Cost757 = 68.03478796810913\n",
      "Cost758 = 68.03111151857054\n",
      "Cost759 = 68.0323076228658\n",
      "Cost760 = 68.02368169109896\n",
      "Cost761 = 68.01962773819766\n",
      "Cost762 = 68.01951572532379\n",
      "Cost763 = 68.01501979816715\n",
      "Cost764 = 68.01202246311551\n",
      "Cost765 = 68.01069621364448\n",
      "Cost766 = 68.00362529171402\n",
      "Cost767 = 68.00327227040468\n",
      "Cost768 = 67.9989331277363\n",
      "Cost769 = 67.99188591027665\n",
      "Cost770 = 67.99297855759852\n",
      "Cost771 = 67.99164571219099\n",
      "Cost772 = 67.99045771378273\n",
      "Cost773 = 67.98601192241344\n",
      "Cost774 = 67.98504396429023\n",
      "Cost775 = 67.98184308960109\n",
      "Cost776 = 67.97780966573426\n",
      "Cost777 = 67.97756982481623\n",
      "Cost778 = 67.97197762847163\n",
      "Cost779 = 67.97220283229436\n",
      "Cost780 = 67.96979546778284\n",
      "Cost781 = 67.96461519756714\n",
      "Cost782 = 67.96182614061495\n",
      "Cost783 = 67.9600371933704\n",
      "Cost784 = 67.95928420309292\n",
      "Cost785 = 67.950083279438\n",
      "Cost786 = 67.95200013858103\n",
      "Cost787 = 67.94679674897448\n",
      "Cost788 = 67.94165325921611\n",
      "Cost789 = 67.9405534417369\n",
      "Cost790 = 67.936697079702\n",
      "Cost791 = 67.93473192553205\n",
      "Cost792 = 67.93330386963058\n",
      "Cost793 = 67.92731052514348\n",
      "Cost794 = 67.92648017020356\n",
      "Cost795 = 67.92511435078852\n",
      "Cost796 = 67.92348547043353\n",
      "Cost797 = 67.91627943025597\n",
      "Cost798 = 67.91903356890853\n",
      "Cost799 = 67.9142974562905\n",
      "Cost800 = 67.91437882145684\n",
      "Cost801 = 67.91024148714298\n",
      "Cost802 = 67.90246642454704\n",
      "Cost803 = 67.9049425481763\n",
      "Cost804 = 67.90016432662686\n",
      "Cost805 = 67.89646166404191\n",
      "Cost806 = 67.9006209945563\n",
      "Cost807 = 67.88924855146242\n",
      "Cost808 = 67.88818238840409\n",
      "Cost809 = 67.88545887377968\n",
      "Cost810 = 67.88269761584154\n",
      "Cost811 = 67.87817260002835\n",
      "Cost812 = 67.87886630389264\n",
      "Cost813 = 67.87363828780445\n",
      "Cost814 = 67.87604813997294\n",
      "Cost815 = 67.87205118146505\n",
      "Cost816 = 67.8700169668856\n",
      "Cost817 = 67.87043855732077\n",
      "Cost818 = 67.8709928713297\n",
      "Cost819 = 67.86594717101399\n",
      "Cost820 = 67.86671164314345\n",
      "Cost821 = 67.8645349042298\n",
      "Cost822 = 67.85926809494968\n",
      "Cost823 = 67.86174603231943\n",
      "Cost824 = 67.85806643983729\n",
      "Cost825 = 67.84839751214254\n",
      "Cost826 = 67.85052929654985\n",
      "Cost827 = 67.85298270843676\n",
      "Cost828 = 67.84663854365523\n",
      "Cost829 = 67.84705790275557\n",
      "Cost830 = 67.84276571494965\n",
      "Cost831 = 67.84081638364431\n",
      "Cost832 = 67.84188948856921\n",
      "Cost833 = 67.84242804202557\n",
      "Cost834 = 67.8299606304523\n",
      "Cost835 = 67.83117910516722\n",
      "Cost836 = 67.83357239094404\n",
      "Cost837 = 67.83084706707918\n",
      "Cost838 = 67.8271638061054\n",
      "Cost839 = 67.82039221796457\n",
      "Cost840 = 67.81860754286245\n",
      "Cost841 = 67.8186751093604\n",
      "Cost842 = 67.81602182101318\n",
      "Cost843 = 67.81419412632698\n",
      "Cost844 = 67.81267114266515\n",
      "Cost845 = 67.8055002472538\n",
      "Cost846 = 67.80379078643668\n",
      "Cost847 = 67.80381715655213\n",
      "Cost848 = 67.80771273036812\n",
      "Cost849 = 67.80264900178264\n",
      "Cost850 = 67.79800018827682\n",
      "Cost851 = 67.79977903479853\n",
      "Cost852 = 67.79075290197278\n",
      "Cost853 = 67.78593713228379\n",
      "Cost854 = 67.78350521883401\n",
      "Cost855 = 67.78474055515171\n",
      "Cost856 = 67.78838147539916\n",
      "Cost857 = 67.77939135115082\n",
      "Cost858 = 67.78140004555178\n",
      "Cost859 = 67.7801315209573\n",
      "Cost860 = 67.77971065994679\n",
      "Cost861 = 67.78208638870547\n",
      "Cost862 = 67.77717454637414\n",
      "Cost863 = 67.76998815798065\n",
      "Cost864 = 67.77587239994523\n",
      "Cost865 = 67.76622349039333\n",
      "Cost866 = 67.7672482335065\n",
      "Cost867 = 67.76652022683328\n",
      "Cost868 = 67.76792973124482\n",
      "Cost869 = 67.75871724810996\n",
      "Cost870 = 67.75494634930213\n",
      "Cost871 = 67.75431079892037\n",
      "Cost872 = 67.75891657648611\n",
      "Cost873 = 67.75103326175588\n",
      "Cost874 = 67.75439483218386\n",
      "Cost875 = 67.753530687005\n",
      "Cost876 = 67.74780548267448\n",
      "Cost877 = 67.74280548352593\n",
      "Cost878 = 67.73938754075478\n",
      "Cost879 = 67.73800237323171\n",
      "Cost880 = 67.73566551615262\n",
      "Cost881 = 67.7323305003215\n",
      "Cost882 = 67.72967886639339\n",
      "Cost883 = 67.7290786331954\n",
      "Cost884 = 67.72934020622598\n",
      "Cost885 = 67.72439192417876\n",
      "Cost886 = 67.72532417555941\n",
      "Cost887 = 67.71929390752638\n",
      "Cost888 = 67.7150287094067\n",
      "Cost889 = 67.71904659979192\n",
      "Cost890 = 67.71570205776501\n",
      "Cost891 = 67.70808974175799\n",
      "Cost892 = 67.71424300388801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost893 = 67.70503095839571\n",
      "Cost894 = 67.71055978713201\n",
      "Cost895 = 67.7026730095712\n",
      "Cost896 = 67.70628886264343\n",
      "Cost897 = 67.70232114736474\n",
      "Cost898 = 67.69819594141387\n",
      "Cost899 = 67.69657960675387\n",
      "Cost900 = 67.69391487331728\n",
      "Cost901 = 67.69531335303319\n",
      "Cost902 = 67.68901836219398\n",
      "Cost903 = 67.69077606586106\n",
      "Cost904 = 67.68732066985278\n",
      "Cost905 = 67.68400213017009\n",
      "Cost906 = 67.68705219442099\n",
      "Cost907 = 67.68097214755464\n",
      "Cost908 = 67.67600517909763\n",
      "Cost909 = 67.68028999649269\n",
      "Cost910 = 67.6711948131373\n",
      "Cost911 = 67.6780455171605\n",
      "Cost912 = 67.67506290563003\n",
      "Cost913 = 67.67274305693631\n",
      "Cost914 = 67.66272272733437\n",
      "Cost915 = 67.66565651402374\n",
      "Cost916 = 67.66300715404488\n",
      "Cost917 = 67.65506417286352\n",
      "Cost918 = 67.66185868656807\n",
      "Cost919 = 67.65934330564515\n",
      "Cost920 = 67.65097064724198\n",
      "Cost921 = 67.65139156776878\n",
      "Cost922 = 67.65480544626827\n",
      "Cost923 = 67.6418330086496\n",
      "Cost924 = 67.64429437735875\n",
      "Cost925 = 67.64251672904035\n",
      "Cost926 = 67.6409393970556\n",
      "Cost927 = 67.63844263993165\n",
      "Cost928 = 67.63519262327355\n",
      "Cost929 = 67.63475461244565\n",
      "Cost930 = 67.63528645787859\n",
      "Cost931 = 67.63084698568942\n",
      "Cost932 = 67.63110039827764\n",
      "Cost933 = 67.63143265559235\n",
      "Cost934 = 67.62598150811314\n",
      "Cost935 = 67.62173834822045\n",
      "Cost936 = 67.6254360271041\n",
      "Cost937 = 67.62129896939636\n",
      "Cost938 = 67.61843077331477\n",
      "Cost939 = 67.62032904495909\n",
      "Cost940 = 67.61277549577281\n",
      "Cost941 = 67.6108566485693\n",
      "Cost942 = 67.61206260175277\n",
      "Cost943 = 67.61048125385655\n",
      "Cost944 = 67.60922054487791\n",
      "Cost945 = 67.60383319495529\n",
      "Cost946 = 67.6039756583954\n",
      "Cost947 = 67.59937678330523\n",
      "Cost948 = 67.60239341529896\n",
      "Cost949 = 67.59568539291685\n",
      "Cost950 = 67.5999387924484\n",
      "Cost951 = 67.59120254294172\n",
      "Cost952 = 67.58793671343692\n",
      "Cost953 = 67.58727561057478\n",
      "Cost954 = 67.57865058091927\n",
      "Cost955 = 67.58142272645406\n",
      "Cost956 = 67.58277416092439\n",
      "Cost957 = 67.57826999063819\n",
      "Cost958 = 67.58158366273335\n",
      "Cost959 = 67.57516727697123\n",
      "Cost960 = 67.57386275110348\n",
      "Cost961 = 67.57968287192493\n",
      "Cost962 = 67.57402477848322\n",
      "Cost963 = 67.56880353921312\n",
      "Cost964 = 67.5756218202992\n",
      "Cost965 = 67.57125150766699\n",
      "Cost966 = 67.56764606239715\n",
      "Cost967 = 67.56828278234599\n",
      "Cost968 = 67.56508558278878\n",
      "Cost969 = 67.5663080871662\n",
      "Cost970 = 67.56537630537541\n",
      "Cost971 = 67.5607232722698\n",
      "Cost972 = 67.55912262109405\n",
      "Cost973 = 67.55723049483127\n",
      "Cost974 = 67.55842005844903\n",
      "Cost975 = 67.55278117065434\n",
      "Cost976 = 67.54898544491368\n",
      "Cost977 = 67.54675796046041\n",
      "Cost978 = 67.55269322659173\n",
      "Cost979 = 67.54683221383986\n",
      "Cost980 = 67.54917758023322\n",
      "Cost981 = 67.53932632959945\n",
      "Cost982 = 67.54183479393409\n",
      "Cost983 = 67.53674348749325\n",
      "Cost984 = 67.5340953501674\n",
      "Cost985 = 67.52983614388653\n",
      "Cost986 = 67.5258096565254\n",
      "Cost987 = 67.52621925765881\n",
      "Cost988 = 67.51934223616506\n",
      "Cost989 = 67.5226377489702\n",
      "Cost990 = 67.51898727544115\n",
      "Cost991 = 67.51734007866615\n",
      "Cost992 = 67.51093289857438\n",
      "Cost993 = 67.5158209722451\n",
      "Cost994 = 67.50660882361402\n",
      "Cost995 = 67.51132073568597\n",
      "Cost996 = 67.50840415437165\n",
      "Cost997 = 67.504320898899\n",
      "Cost998 = 67.50213633434201\n",
      "Cost999 = 67.49977788874068\n",
      "Cost1000 = 67.49721203241309\n",
      "Cost1001 = 67.4951490594871\n",
      "Cost1002 = 67.49460302546744\n",
      "Cost1003 = 67.49802467138966\n",
      "Cost1004 = 67.48935847672008\n",
      "Cost1005 = 67.49054011282034\n",
      "Cost1006 = 67.48955099330253\n",
      "Cost1007 = 67.48482412600532\n",
      "Cost1008 = 67.48591599939343\n",
      "Cost1009 = 67.48349090119052\n",
      "Cost1010 = 67.48705173177353\n",
      "Cost1011 = 67.48293542543945\n",
      "Cost1012 = 67.48207582207466\n",
      "Cost1013 = 67.47650794158974\n",
      "Cost1014 = 67.47078981069208\n",
      "Cost1015 = 67.47304271309477\n",
      "Cost1016 = 67.47412041330584\n",
      "Cost1017 = 67.46599478167488\n",
      "Cost1018 = 67.46454923386302\n",
      "Cost1019 = 67.46374091771224\n",
      "Cost1020 = 67.46279216649313\n",
      "Cost1021 = 67.45625033821491\n",
      "Cost1022 = 67.45768617968648\n",
      "Cost1023 = 67.45807107839016\n",
      "Cost1024 = 67.45652004030373\n",
      "Cost1025 = 67.45570836381492\n",
      "Cost1026 = 67.45266360387096\n",
      "Cost1027 = 67.44446767008243\n",
      "Cost1028 = 67.44756226353132\n",
      "Cost1029 = 67.44577945335493\n",
      "Cost1030 = 67.44327936963133\n",
      "Cost1031 = 67.4492709920815\n",
      "Cost1032 = 67.44212670926284\n",
      "Cost1033 = 67.44787994908054\n",
      "Cost1034 = 67.44026220778123\n",
      "Cost1035 = 67.4352367836864\n",
      "Cost1036 = 67.43456075810086\n",
      "Cost1037 = 67.42831071688192\n",
      "Cost1038 = 67.43156684836136\n",
      "Cost1039 = 67.43098978273791\n",
      "Cost1040 = 67.43655359729243\n",
      "Cost1041 = 67.42785704981195\n",
      "Cost1042 = 67.42748557386217\n",
      "Cost1043 = 67.4200644205122\n",
      "Cost1044 = 67.42005297257812\n",
      "Cost1045 = 67.42201775445038\n",
      "Cost1046 = 67.41925463740931\n",
      "Cost1047 = 67.41890240105701\n",
      "Cost1048 = 67.41611883486532\n",
      "Cost1049 = 67.4127714577719\n",
      "Cost1050 = 67.41250115904688\n",
      "Cost1051 = 67.40774203607033\n",
      "Cost1052 = 67.41338916170136\n",
      "Cost1053 = 67.40837181665817\n",
      "Cost1054 = 67.40227013537469\n",
      "Cost1055 = 67.39856800114045\n",
      "Cost1056 = 67.40285997387294\n",
      "Cost1057 = 67.39815518324225\n",
      "Cost1058 = 67.39720023634102\n",
      "Cost1059 = 67.39263970240748\n",
      "Cost1060 = 67.38979278474605\n",
      "Cost1061 = 67.39694594901628\n",
      "Cost1062 = 67.38907737868965\n",
      "Cost1063 = 67.38707016905\n",
      "Cost1064 = 67.3910455296222\n",
      "Cost1065 = 67.38639029065891\n",
      "Cost1066 = 67.38378488370363\n",
      "Cost1067 = 67.37929440002601\n",
      "Cost1068 = 67.37597476847488\n",
      "Cost1069 = 67.3770605901195\n",
      "Cost1070 = 67.37959833189369\n",
      "Cost1071 = 67.37856451028861\n",
      "Cost1072 = 67.37476544487069\n",
      "Cost1073 = 67.3721783324847\n",
      "Cost1074 = 67.36656235847246\n",
      "Cost1075 = 67.3649634297901\n",
      "Cost1076 = 67.36767800753385\n",
      "Cost1077 = 67.3604069905007\n",
      "Cost1078 = 67.3625947021457\n",
      "Cost1079 = 67.36454416367657\n",
      "Cost1080 = 67.36560088463234\n",
      "Cost1081 = 67.35932619700827\n",
      "Cost1082 = 67.35459803510363\n",
      "Cost1083 = 67.34866598347047\n",
      "Cost1084 = 67.35793842218422\n",
      "Cost1085 = 67.35186919219896\n",
      "Cost1086 = 67.34856684843092\n",
      "Cost1087 = 67.34715003271795\n",
      "Cost1088 = 67.35029686575722\n",
      "Cost1089 = 67.34496646386819\n",
      "Cost1090 = 67.34020404636549\n",
      "Cost1091 = 67.34318918888661\n",
      "Cost1092 = 67.34317749807515\n",
      "Cost1093 = 67.33935993958069\n",
      "Cost1094 = 67.33759576539885\n",
      "Cost1095 = 67.33340625842659\n",
      "Cost1096 = 67.33483324752117\n",
      "Cost1097 = 67.33360941323679\n",
      "Cost1098 = 67.3318837402072\n",
      "Cost1099 = 67.3307116981277\n",
      "Cost1100 = 67.32422093343432\n",
      "Cost1101 = 67.3264693062101\n",
      "Cost1102 = 67.32145066770492\n",
      "Cost1103 = 67.32319400897518\n",
      "Cost1104 = 67.31644965005295\n",
      "Cost1105 = 67.3192748665693\n",
      "Cost1106 = 67.32134186240721\n",
      "Cost1107 = 67.3183933946715\n",
      "Cost1108 = 67.3154634860343\n",
      "Cost1109 = 67.31545123710498\n",
      "Cost1110 = 67.31277776396253\n",
      "Cost1111 = 67.31194370903901\n",
      "Cost1112 = 67.313835458949\n",
      "Cost1113 = 67.30777377215888\n",
      "Cost1114 = 67.31139251283155\n",
      "Cost1115 = 67.3064964845428\n",
      "Cost1116 = 67.30166627111056\n",
      "Cost1117 = 67.2998176252604\n",
      "Cost1118 = 67.30528482429824\n",
      "Cost1119 = 67.30288763237206\n",
      "Cost1120 = 67.2977035267857\n",
      "Cost1121 = 67.2960739211502\n",
      "Cost1122 = 67.2991321412155\n",
      "Cost1123 = 67.29420122063547\n",
      "Cost1124 = 67.29379891350472\n",
      "Cost1125 = 67.29760592700372\n",
      "Cost1126 = 67.28877583277163\n",
      "Cost1127 = 67.28616131497805\n",
      "Cost1128 = 67.28698451943279\n",
      "Cost1129 = 67.28810438650783\n",
      "Cost1130 = 67.2846659783903\n",
      "Cost1131 = 67.28328056402626\n",
      "Cost1132 = 67.28110545053481\n",
      "Cost1133 = 67.27913480798304\n",
      "Cost1134 = 67.27920647091926\n",
      "Cost1135 = 67.2808425108904\n",
      "Cost1136 = 67.27750317799234\n",
      "Cost1137 = 67.27665560223649\n",
      "Cost1138 = 67.27408790879315\n",
      "Cost1139 = 67.2794047774714\n",
      "Cost1140 = 67.2729272495139\n",
      "Cost1141 = 67.27248053755936\n",
      "Cost1142 = 67.26522513522177\n",
      "Cost1143 = 67.27003838738776\n",
      "Cost1144 = 67.2672172126865\n",
      "Cost1145 = 67.26040776816816\n",
      "Cost1146 = 67.26066310906798\n",
      "Cost1147 = 67.2578672728048\n",
      "Cost1148 = 67.26323282192371\n",
      "Cost1149 = 67.26339925198158\n",
      "Cost1150 = 67.25306449538564\n",
      "Cost1151 = 67.25177642110704\n",
      "Cost1152 = 67.25129742691946\n",
      "Cost1153 = 67.24901023762641\n",
      "Cost1154 = 67.24896796316415\n",
      "Cost1155 = 67.24636760656091\n",
      "Cost1156 = 67.24002687133834\n",
      "Cost1157 = 67.24359813139019\n",
      "Cost1158 = 67.23586563835875\n",
      "Cost1159 = 67.24692123156224\n",
      "Cost1160 = 67.2335851516022\n",
      "Cost1161 = 67.23362483288426\n",
      "Cost1162 = 67.23284436893006\n",
      "Cost1163 = 67.23817662091552\n",
      "Cost1164 = 67.2280050396185\n",
      "Cost1165 = 67.23331000659805\n",
      "Cost1166 = 67.23488707900168\n",
      "Cost1167 = 67.22829850562816\n",
      "Cost1168 = 67.2245676565074\n",
      "Cost1169 = 67.22450700314829\n",
      "Cost1170 = 67.21941423970063\n",
      "Cost1171 = 67.2269503625152\n",
      "Cost1172 = 67.22357215266055\n",
      "Cost1173 = 67.21869021640484\n",
      "Cost1174 = 67.21724169126259\n",
      "Cost1175 = 67.2170330987708\n",
      "Cost1176 = 67.21615516979148\n",
      "Cost1177 = 67.21900067546021\n",
      "Cost1178 = 67.21870082140634\n",
      "Cost1179 = 67.20743869568753\n",
      "Cost1180 = 67.21327839326976\n",
      "Cost1181 = 67.2150003346474\n",
      "Cost1182 = 67.20793879363315\n",
      "Cost1183 = 67.21243193892889\n",
      "Cost1184 = 67.20798886668754\n",
      "Cost1185 = 67.20917445013244\n",
      "Cost1186 = 67.20120720072242\n",
      "Cost1187 = 67.19953326316207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost1188 = 67.19816512594664\n",
      "Cost1189 = 67.20461831915492\n",
      "Cost1190 = 67.19898407922453\n",
      "Cost1191 = 67.19655073987745\n",
      "Cost1192 = 67.19724442460702\n",
      "Cost1193 = 67.19011307927491\n",
      "Cost1194 = 67.19626657409721\n",
      "Cost1195 = 67.1898877118277\n",
      "Cost1196 = 67.192477065275\n",
      "Cost1197 = 67.18868208991235\n",
      "Cost1198 = 67.18761429664863\n",
      "Cost1199 = 67.18634801791461\n",
      "Cost1200 = 67.1891794864402\n",
      "Cost1201 = 67.18225178302163\n",
      "Cost1202 = 67.179264297538\n",
      "Cost1203 = 67.18327458662664\n",
      "Cost1204 = 67.17381159166526\n",
      "Cost1205 = 67.17472256002725\n",
      "Cost1206 = 67.16915148249007\n",
      "Cost1207 = 67.16994082914238\n",
      "Cost1208 = 67.1668830727997\n",
      "Cost1209 = 67.1655571710653\n",
      "Cost1210 = 67.16361911504693\n",
      "Cost1211 = 67.16293700379597\n",
      "Cost1212 = 67.16585048008073\n",
      "Cost1213 = 67.16087585631303\n",
      "Cost1214 = 67.16363852310954\n",
      "Cost1215 = 67.16075749651384\n",
      "Cost1216 = 67.15639752593957\n",
      "Cost1217 = 67.1567636529664\n",
      "Cost1218 = 67.15764507613285\n",
      "Cost1219 = 67.15564746166008\n",
      "Cost1220 = 67.1589899090459\n",
      "Cost1221 = 67.15018151581376\n",
      "Cost1222 = 67.15339373029323\n",
      "Cost1223 = 67.14910872836052\n",
      "Cost1224 = 67.15001294513338\n",
      "Cost1225 = 67.1470009928923\n",
      "Cost1226 = 67.14743611454502\n",
      "Cost1227 = 67.14032506598707\n",
      "Cost1228 = 67.1441894867098\n",
      "Cost1229 = 67.14938046674669\n",
      "Cost1230 = 67.14405689309615\n",
      "Cost1231 = 67.14396503765923\n",
      "Cost1232 = 67.14284775410717\n",
      "Cost1233 = 67.13977982856802\n",
      "Cost1234 = 67.13677467982278\n",
      "Cost1235 = 67.13587745610432\n",
      "Cost1236 = 67.13784424951004\n",
      "Cost1237 = 67.13902987547414\n",
      "Cost1238 = 67.13358632303841\n",
      "Cost1239 = 67.13702805615124\n",
      "Cost1240 = 67.127269074745\n",
      "Cost1241 = 67.13636465751992\n",
      "Cost1242 = 67.13334284822952\n",
      "Cost1243 = 67.1306020055338\n",
      "Cost1244 = 67.13090097346637\n",
      "Cost1245 = 67.12813230276744\n",
      "Cost1246 = 67.12863485058675\n",
      "Cost1247 = 67.12383365473013\n",
      "Cost1248 = 67.12747798101132\n",
      "Cost1249 = 67.12683257875698\n",
      "Cost1250 = 67.12539326206871\n",
      "Cost1251 = 67.11853686195771\n",
      "Cost1252 = 67.12423806006316\n",
      "Cost1253 = 67.1182456439875\n",
      "Cost1254 = 67.11609759877722\n",
      "Cost1255 = 67.12123200103335\n",
      "Cost1256 = 67.1227747726076\n",
      "Cost1257 = 67.1168214389845\n",
      "Cost1258 = 67.11426177992699\n",
      "Cost1259 = 67.10729971754496\n",
      "Cost1260 = 67.11771432032484\n",
      "Cost1261 = 67.1116613580313\n",
      "Cost1262 = 67.11142448226147\n",
      "Cost1263 = 67.10965386764902\n",
      "Cost1264 = 67.10999021123372\n",
      "Cost1265 = 67.11015820392697\n",
      "Cost1266 = 67.11184654264066\n",
      "Cost1267 = 67.10617913810987\n",
      "Cost1268 = 67.104552647018\n",
      "Cost1269 = 67.10893253053085\n",
      "Cost1270 = 67.1033366929326\n",
      "Cost1271 = 67.10932859144971\n",
      "Cost1272 = 67.10398878384098\n",
      "Cost1273 = 67.10443010546298\n",
      "Cost1274 = 67.10196846872086\n",
      "Cost1275 = 67.10717170346217\n",
      "Cost1276 = 67.10235628367884\n",
      "Cost1277 = 67.10484088366874\n",
      "Cost1278 = 67.10752862634371\n",
      "Cost1279 = 67.0986213815887\n",
      "Cost1280 = 67.0989873602045\n",
      "Cost1281 = 67.09396922510872\n",
      "Cost1282 = 67.09562979439652\n",
      "Cost1283 = 67.09613068387577\n",
      "Cost1284 = 67.0948879489034\n",
      "Cost1285 = 67.09408768632309\n",
      "Cost1286 = 67.09789533865745\n",
      "Cost1287 = 67.09724327142942\n",
      "Cost1288 = 67.09676386004064\n",
      "Cost1289 = 67.09652888969819\n",
      "Cost1290 = 67.08856345205498\n",
      "Cost1291 = 67.08860245988555\n",
      "Cost1292 = 67.08748959190751\n",
      "Cost1293 = 67.09140346015923\n",
      "Cost1294 = 67.08958444308688\n",
      "Cost1295 = 67.09115948084319\n",
      "Cost1296 = 67.08607057413461\n",
      "Cost1297 = 67.08795928646335\n",
      "Cost1298 = 67.09141186922815\n",
      "Cost1299 = 67.08906558066674\n",
      "Cost1300 = 67.09183318717903\n",
      "Cost1301 = 67.08335546658803\n",
      "Cost1302 = 67.08552397598916\n",
      "Cost1303 = 67.08603111856833\n",
      "Cost1304 = 67.0866272477298\n",
      "Cost1305 = 67.08535563335246\n",
      "Cost1306 = 67.08657809804778\n",
      "Cost1307 = 67.08857145570782\n",
      "Cost1308 = 67.08560560766196\n",
      "Cost1309 = 67.09042676796064\n",
      "Cost1310 = 67.08334741783402\n",
      "Cost1311 = 67.08428871970175\n",
      "Cost1312 = 67.08261403109428\n",
      "Cost1313 = 67.08378181157168\n",
      "Cost1314 = 67.08334784955966\n",
      "Cost1315 = 67.08174196046012\n",
      "Cost1316 = 67.08266310997747\n",
      "Cost1317 = 67.08406181246285\n",
      "Cost1318 = 67.08296578259119\n",
      "Cost1319 = 67.0859191815346\n",
      "Cost1320 = 67.07737264976107\n",
      "Cost1321 = 67.08097390797892\n",
      "Cost1322 = 67.08340658753754\n",
      "Cost1323 = 67.08083898747437\n",
      "Cost1324 = 67.07503977096707\n",
      "Cost1325 = 67.07656498540409\n",
      "Cost1326 = 67.08296708813431\n",
      "Cost1327 = 67.07805620654197\n",
      "Cost1328 = 67.07690957820546\n",
      "Cost1329 = 67.07527798751106\n",
      "Cost1330 = 67.07332787966972\n",
      "Cost1331 = 67.08014816815646\n",
      "Cost1332 = 67.07964238530822\n",
      "Cost1333 = 67.07976976386904\n",
      "Cost1334 = 67.0715280035292\n",
      "Cost1335 = 67.06971330829501\n",
      "Cost1336 = 67.07094842074383\n",
      "Cost1337 = 67.07538752272949\n",
      "Cost1338 = 67.07099239107146\n",
      "Cost1339 = 67.07354542295523\n",
      "Cost1340 = 67.0743001852774\n",
      "Cost1341 = 67.07416754538518\n",
      "Cost1342 = 67.06614065586112\n",
      "Cost1343 = 67.0719893306617\n",
      "Cost1344 = 67.07053536821269\n",
      "Cost1345 = 67.07624420522653\n",
      "Cost1346 = 67.06829363596991\n",
      "Cost1347 = 67.07422944095173\n",
      "Cost1348 = 67.07296587426447\n",
      "Cost1349 = 67.06914012242517\n",
      "Cost1350 = 67.06606270710417\n",
      "Cost1351 = 67.06602122650284\n",
      "Cost1352 = 67.06595441493971\n",
      "Cost1353 = 67.07074547620067\n",
      "Cost1354 = 67.06421786307756\n",
      "Cost1355 = 67.06538203093261\n",
      "Cost1356 = 67.0665852355765\n",
      "Cost1357 = 67.067437277527\n",
      "Cost1358 = 67.06546959315155\n",
      "Cost1359 = 67.06592498604924\n",
      "Cost1360 = 67.06882709856086\n",
      "Cost1361 = 67.06351312339113\n",
      "Cost1362 = 67.06804294189709\n",
      "Cost1363 = 67.06859138423384\n",
      "Cost1364 = 67.06858339769977\n",
      "Cost1365 = 67.06105234565108\n",
      "Cost1366 = 67.05878451114253\n",
      "Cost1367 = 67.06090410854065\n",
      "Cost1368 = 67.06245610545251\n",
      "Cost1369 = 67.06238053604868\n",
      "Cost1370 = 67.06085052456267\n",
      "Cost1371 = 67.05977439631079\n",
      "Cost1372 = 67.06053577052108\n",
      "Cost1373 = 67.056434054611\n",
      "Cost1374 = 67.06256751841474\n",
      "Cost1375 = 67.0568770159677\n",
      "Cost1376 = 67.05529510990735\n",
      "Cost1377 = 67.05918234291914\n",
      "Cost1378 = 67.05519734118988\n",
      "Cost1379 = 67.05503587854429\n",
      "Cost1380 = 67.05395772980037\n",
      "Cost1381 = 67.06013381194106\n",
      "Cost1382 = 67.05610225856316\n",
      "Cost1383 = 67.05823718242117\n",
      "Cost1384 = 67.05704447842524\n",
      "Cost1385 = 67.05580091267441\n",
      "Cost1386 = 67.04870090886642\n",
      "Cost1387 = 67.05646270137035\n",
      "Cost1388 = 67.05728575077075\n",
      "Cost1389 = 67.05624760843929\n",
      "Cost1390 = 67.05279514295363\n",
      "Cost1391 = 67.04997062631111\n",
      "Cost1392 = 67.05499095987139\n",
      "Cost1393 = 67.04663614384253\n",
      "Cost1394 = 67.04844822548934\n",
      "Cost1395 = 67.05500134160573\n",
      "Cost1396 = 67.04773744053284\n",
      "Cost1397 = 67.05206260576922\n",
      "Cost1398 = 67.05261347738109\n",
      "Cost1399 = 67.04777863293633\n",
      "Cost1400 = 67.04961834677839\n",
      "Cost1401 = 67.04583815520353\n",
      "Cost1402 = 67.04572306615158\n",
      "Cost1403 = 67.04524800513964\n",
      "Cost1404 = 67.04150841793772\n",
      "Cost1405 = 67.04636994169184\n",
      "Cost1406 = 67.04875241641174\n",
      "Cost1407 = 67.04447483638742\n",
      "Cost1408 = 67.04477127251187\n",
      "Cost1409 = 67.04228573126879\n",
      "Cost1410 = 67.04217189685627\n",
      "Cost1411 = 67.03755877446673\n",
      "Cost1412 = 67.04478181682161\n",
      "Cost1413 = 67.03625590648423\n",
      "Cost1414 = 67.04217271430463\n",
      "Cost1415 = 67.04323172940161\n",
      "Cost1416 = 67.04133673858954\n",
      "Cost1417 = 67.03665397580369\n",
      "Cost1418 = 67.04147819535777\n",
      "Cost1419 = 67.0425713549655\n",
      "Cost1420 = 67.04326214704264\n",
      "Cost1421 = 67.03524303924141\n",
      "Cost1422 = 67.04393685724392\n",
      "Cost1423 = 67.0393070060899\n",
      "Cost1424 = 67.03900744342275\n",
      "Cost1425 = 67.03733910136204\n",
      "Cost1426 = 67.03875311951677\n",
      "Cost1427 = 67.03431022064483\n",
      "Cost1428 = 67.03774303723682\n",
      "Cost1429 = 67.03647874837209\n",
      "Cost1430 = 67.03349120424792\n",
      "Cost1431 = 67.0331559241245\n",
      "Cost1432 = 67.03374501499282\n",
      "Cost1433 = 67.03794452038971\n",
      "Cost1434 = 67.03116518913271\n",
      "Cost1435 = 67.03867577889731\n",
      "Cost1436 = 67.03428112219753\n",
      "Cost1437 = 67.03289467414972\n",
      "Cost1438 = 67.03597920686514\n",
      "Cost1439 = 67.03884332716062\n",
      "Cost1440 = 67.03710436954368\n",
      "Cost1441 = 67.02970038455842\n",
      "Cost1442 = 67.03405671628813\n",
      "Cost1443 = 67.0311999774262\n",
      "Cost1444 = 67.03662252029494\n",
      "Cost1445 = 67.0354146659958\n",
      "Cost1446 = 67.03363398446407\n",
      "Cost1447 = 67.03134110850543\n",
      "Cost1448 = 67.0282611522991\n",
      "Cost1449 = 67.02948561570645\n",
      "Cost1450 = 67.02465126386718\n",
      "Cost1451 = 67.02822525801339\n",
      "Cost1452 = 67.03170452367293\n",
      "Cost1453 = 67.02675544393848\n",
      "Cost1454 = 67.0265659378141\n",
      "Cost1455 = 67.02472990275211\n",
      "Cost1456 = 67.02681610352784\n",
      "Cost1457 = 67.02150883843957\n",
      "Cost1458 = 67.02609674256036\n",
      "Cost1459 = 67.02385292544248\n",
      "Cost1460 = 67.02445827441164\n",
      "Cost1461 = 67.02116913600706\n",
      "Cost1462 = 67.02326307853251\n",
      "Cost1463 = 67.02324818580374\n",
      "Cost1464 = 67.02242973054983\n",
      "Cost1465 = 67.02333511185441\n",
      "Cost1466 = 67.02092827920862\n",
      "Cost1467 = 67.01798714764469\n",
      "Cost1468 = 67.02342240194781\n",
      "Cost1469 = 67.01756177184048\n",
      "Cost1470 = 67.0213977484339\n",
      "Cost1471 = 67.01881331831652\n",
      "Cost1472 = 67.01978116569478\n",
      "Cost1473 = 67.0146437219596\n",
      "Cost1474 = 67.0167082851944\n",
      "Cost1475 = 67.01475067310899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost1476 = 67.0127149512676\n",
      "Cost1477 = 67.01344278229448\n",
      "Cost1478 = 67.01439005351511\n",
      "Cost1479 = 67.01769979235753\n",
      "Cost1480 = 67.01400427321622\n",
      "Cost1481 = 67.01489361343432\n",
      "Cost1482 = 67.01922960032753\n",
      "Cost1483 = 67.01323516970129\n",
      "Cost1484 = 67.01016710301278\n",
      "Cost1485 = 67.01114525265208\n",
      "Cost1486 = 67.01196282831005\n",
      "Cost1487 = 67.01086245662613\n",
      "Cost1488 = 67.00944560728101\n",
      "Cost1489 = 67.01098896485503\n",
      "Cost1490 = 67.00642803639145\n",
      "Cost1491 = 67.0115837304996\n",
      "Cost1492 = 67.00549217267348\n",
      "Cost1493 = 67.00990477455879\n",
      "Cost1494 = 67.00516134460102\n",
      "Cost1495 = 67.01137734979469\n",
      "Cost1496 = 67.00864274626412\n",
      "Cost1497 = 67.00517139158192\n",
      "Cost1498 = 67.00529144632155\n",
      "Cost1499 = 67.00701247189362\n",
      "Cost1500 = 67.00733514208194\n",
      "Cost1501 = 67.00491546521872\n",
      "Cost1502 = 67.00149024851191\n",
      "Cost1503 = 67.00299493250532\n",
      "Cost1504 = 67.00419453087245\n",
      "Cost1505 = 67.00553697311952\n",
      "Cost1506 = 66.99975902509127\n",
      "Cost1507 = 67.00600603090255\n",
      "Cost1508 = 67.00398674544937\n",
      "Cost1509 = 67.00947116659377\n",
      "Cost1510 = 66.99718841961122\n",
      "Cost1511 = 67.00004702449348\n",
      "Cost1512 = 67.00268522771982\n",
      "Cost1513 = 67.00316655258861\n",
      "Cost1514 = 67.00451887524918\n",
      "Cost1515 = 67.00531471872814\n",
      "Cost1516 = 66.99964906319337\n",
      "Cost1517 = 66.99953839826064\n",
      "Cost1518 = 66.99759940689718\n",
      "Cost1519 = 66.99975005964853\n",
      "Cost1520 = 66.99936668710973\n",
      "Cost1521 = 66.99214411915963\n",
      "Cost1522 = 66.99762047502331\n",
      "Cost1523 = 66.99300366654928\n",
      "Cost1524 = 67.00163982676597\n",
      "Cost1525 = 66.99314007623953\n",
      "Cost1526 = 66.99217423116585\n",
      "Cost1527 = 66.99914614538069\n",
      "Cost1528 = 66.99802957599827\n",
      "Cost1529 = 66.99519459711964\n",
      "Cost1530 = 66.99775683636666\n",
      "Cost1531 = 66.98950947724211\n",
      "Cost1532 = 66.99079315726074\n",
      "Cost1533 = 66.99389732114248\n",
      "Cost1534 = 66.99380875559197\n",
      "Cost1535 = 66.99672716564666\n",
      "Cost1536 = 66.99116233316394\n",
      "Cost1537 = 66.99508997132409\n",
      "Cost1538 = 66.99818078947054\n",
      "Cost1539 = 66.98887663896254\n",
      "Cost1540 = 66.99277231603911\n",
      "Cost1541 = 66.99004993089433\n",
      "Cost1542 = 66.98844513699439\n",
      "Cost1543 = 66.99670122269517\n",
      "Cost1544 = 66.99096364907105\n",
      "Cost1545 = 66.98986157798758\n",
      "Cost1546 = 66.98793864779164\n",
      "Cost1547 = 66.98592721638026\n",
      "Cost1548 = 66.9911578717866\n",
      "Cost1549 = 66.98899642483697\n",
      "Cost1550 = 66.99066165468298\n",
      "Cost1551 = 66.99453450095822\n",
      "Cost1552 = 66.99418266301114\n",
      "Cost1553 = 66.9945655456376\n",
      "Cost1554 = 66.99022465288155\n",
      "Cost1555 = 66.99057085612755\n",
      "Cost1556 = 66.98745082144974\n",
      "Cost1557 = 66.99228743009154\n",
      "Cost1558 = 66.9846020442633\n",
      "Cost1559 = 66.9860530403917\n",
      "Cost1560 = 66.98825544860823\n",
      "Cost1561 = 66.98575773511271\n",
      "Cost1562 = 66.98867469796075\n",
      "Cost1563 = 66.98153250802598\n",
      "Cost1564 = 66.99062694695313\n",
      "Cost1565 = 66.98652020984417\n",
      "Cost1566 = 66.98791931498559\n",
      "Cost1567 = 66.98634888374201\n",
      "Cost1568 = 66.98831433986055\n",
      "Cost1569 = 66.98688054814144\n",
      "Cost1570 = 66.98579453509576\n",
      "Cost1571 = 66.98024026506302\n",
      "Cost1572 = 66.98628889614538\n",
      "Cost1573 = 66.98839506213228\n",
      "Cost1574 = 66.98529310820669\n",
      "Cost1575 = 66.98301365832653\n",
      "Cost1576 = 66.98023475545737\n",
      "Cost1577 = 66.97684979659215\n",
      "Cost1578 = 66.97547931301493\n",
      "Cost1579 = 66.98187310054853\n",
      "Cost1580 = 66.98317441331542\n",
      "Cost1581 = 66.98458312330368\n",
      "Cost1582 = 66.98089661817512\n",
      "Cost1583 = 66.97842037541767\n",
      "Cost1584 = 66.9812568263443\n",
      "Cost1585 = 66.97899617797421\n",
      "Cost1586 = 66.97531758012482\n",
      "Cost1587 = 66.98078323148685\n",
      "Cost1588 = 66.97576443006031\n",
      "Cost1589 = 66.97442868603035\n",
      "Cost1590 = 66.98099388398364\n",
      "Cost1591 = 66.9789071858215\n",
      "Cost1592 = 66.97644611347212\n",
      "Cost1593 = 66.97659699865739\n",
      "Cost1594 = 66.97775374276041\n",
      "Cost1595 = 66.9747849791806\n",
      "Cost1596 = 66.97723524580489\n",
      "Cost1597 = 66.98168884900562\n",
      "Cost1598 = 66.97668564864946\n",
      "Cost1599 = 66.9780531571058\n",
      "Cost1600 = 66.97484355857254\n",
      "Cost1601 = 66.97423786013525\n",
      "Cost1602 = 66.97513655993129\n",
      "Cost1603 = 66.97315870034339\n",
      "Cost1604 = 66.9744193778228\n",
      "Cost1605 = 66.97090901633209\n",
      "Cost1606 = 66.97758588694656\n",
      "Cost1607 = 66.97863053591011\n",
      "Cost1608 = 66.97321187570411\n",
      "Cost1609 = 66.96775292588572\n",
      "Cost1610 = 66.97288842757868\n",
      "Cost1611 = 66.96815529602146\n",
      "Cost1612 = 66.97490691658352\n",
      "Cost1613 = 66.96927386062877\n",
      "Cost1614 = 66.96916472161271\n",
      "Cost1615 = 66.96712425706399\n",
      "Cost1616 = 66.97506352066434\n",
      "Cost1617 = 66.97365405174249\n",
      "Cost1618 = 66.97121221596049\n",
      "Cost1619 = 66.97011499548738\n",
      "Cost1620 = 66.9705059667648\n",
      "Cost1621 = 66.96997372634875\n",
      "Cost1622 = 66.97297560978986\n",
      "Cost1623 = 66.9735462910359\n",
      "Cost1624 = 66.97038467225083\n",
      "Cost1625 = 66.96806949771872\n",
      "Cost1626 = 66.96954899155658\n",
      "Cost1627 = 66.96676814308891\n",
      "Cost1628 = 66.96353582085071\n",
      "Cost1629 = 66.9647761837672\n",
      "Cost1630 = 66.97096421544923\n",
      "Cost1631 = 66.9666479215012\n",
      "Cost1632 = 66.9635681265137\n",
      "Cost1633 = 66.9677466658617\n",
      "Cost1634 = 66.96441553739776\n",
      "Cost1635 = 66.96556494867853\n",
      "Cost1636 = 66.96394296284869\n",
      "Cost1637 = 66.96480881230805\n",
      "Cost1638 = 66.9650605301285\n",
      "Cost1639 = 66.96479860118973\n",
      "Cost1640 = 66.96436094550658\n",
      "Cost1641 = 66.96086677068016\n",
      "Cost1642 = 66.96021646882721\n",
      "Cost1643 = 66.96567651238533\n",
      "Cost1644 = 66.96321116785757\n",
      "Cost1645 = 66.96711056012182\n",
      "Cost1646 = 66.96025440227005\n",
      "Cost1647 = 66.96448452516688\n",
      "Cost1648 = 66.96557123173922\n",
      "Cost1649 = 66.96659588580773\n",
      "Cost1650 = 66.96548245826233\n",
      "Cost1651 = 66.95889260915251\n",
      "Cost1652 = 66.96166765349197\n",
      "Cost1653 = 66.96048440933697\n",
      "Cost1654 = 66.96534278409891\n",
      "Cost1655 = 66.96666718781965\n",
      "Cost1656 = 66.96345854125707\n",
      "Cost1657 = 66.96534588263266\n",
      "Cost1658 = 66.95819975831728\n",
      "Cost1659 = 66.9617912597495\n",
      "Cost1660 = 66.96444271023256\n",
      "Cost1661 = 66.96170161845532\n",
      "Cost1662 = 66.95930964496738\n",
      "Cost1663 = 66.9555930138915\n",
      "Cost1664 = 66.95953938280911\n",
      "Cost1665 = 66.95177267628556\n",
      "Cost1666 = 66.96055312508594\n",
      "Cost1667 = 66.9594369800064\n",
      "Cost1668 = 66.95553445445212\n",
      "Cost1669 = 66.95565272618423\n",
      "Cost1670 = 66.96123488440438\n",
      "Cost1671 = 66.95758229284222\n",
      "Cost1672 = 66.9565995599155\n",
      "Cost1673 = 66.9596070903484\n",
      "Cost1674 = 66.95752313432835\n",
      "Cost1675 = 66.95627976875834\n",
      "Cost1676 = 66.95587774023532\n",
      "Cost1677 = 66.95436906515846\n",
      "Cost1678 = 66.95820171001291\n",
      "Cost1679 = 66.95218785783165\n",
      "Cost1680 = 66.95718330415568\n",
      "Cost1681 = 66.95937706110716\n",
      "Cost1682 = 66.95265956526033\n",
      "Cost1683 = 66.95257013171361\n",
      "Cost1684 = 66.95502289199545\n",
      "Cost1685 = 66.9561775355183\n",
      "Cost1686 = 66.95892470635559\n",
      "Cost1687 = 66.95925599846468\n",
      "Cost1688 = 66.95172544958257\n",
      "Cost1689 = 66.95388021067777\n",
      "Cost1690 = 66.95587815304599\n",
      "Cost1691 = 66.95291176762656\n",
      "Cost1692 = 66.95726039994165\n",
      "Cost1693 = 66.95158865558352\n",
      "Cost1694 = 66.94608251128146\n",
      "Cost1695 = 66.9554030513667\n",
      "Cost1696 = 66.96032972033271\n",
      "Cost1697 = 66.94957822770489\n",
      "Cost1698 = 66.94753660248062\n",
      "Cost1699 = 66.95468730343006\n",
      "Cost1700 = 66.95279357361625\n",
      "Cost1701 = 66.95444291098002\n",
      "Cost1702 = 66.95590257873818\n",
      "Cost1703 = 66.9544334261067\n",
      "Cost1704 = 66.9499879751824\n",
      "Cost1705 = 66.9513722663012\n",
      "Cost1706 = 66.95255833592424\n",
      "Cost1707 = 66.95431666191912\n",
      "Cost1708 = 66.94888628216043\n",
      "Cost1709 = 66.95318261980393\n",
      "Cost1710 = 66.9510032931644\n",
      "Cost1711 = 66.94956244949654\n",
      "Cost1712 = 66.95388440812721\n",
      "Cost1713 = 66.95268242491521\n",
      "Cost1714 = 66.9527544294283\n",
      "Cost1715 = 66.94807514967488\n",
      "Cost1716 = 66.95419415400869\n",
      "Cost1717 = 66.95197002068778\n",
      "Cost1718 = 66.94792395889273\n",
      "Cost1719 = 66.94899325098727\n",
      "Cost1720 = 66.94669642936528\n",
      "Cost1721 = 66.945496610792\n",
      "Cost1722 = 66.94631295906484\n",
      "Cost1723 = 66.94654226002626\n",
      "Cost1724 = 66.94546715418025\n",
      "Cost1725 = 66.94541041970912\n",
      "Cost1726 = 66.94595453222963\n",
      "Cost1727 = 66.94512850169528\n",
      "Cost1728 = 66.95260843431815\n",
      "Cost1729 = 66.94358358877103\n",
      "Cost1730 = 66.9445320836875\n",
      "Cost1731 = 66.94666969368993\n",
      "Cost1732 = 66.94618947730089\n",
      "Cost1733 = 66.94132312803083\n",
      "Cost1734 = 66.9430691227753\n",
      "Cost1735 = 66.94302271885793\n",
      "Cost1736 = 66.93857690179392\n",
      "Cost1737 = 66.94460725376686\n",
      "Cost1738 = 66.9429992650882\n",
      "Cost1739 = 66.94133384522145\n",
      "Cost1740 = 66.94050661648087\n",
      "Cost1741 = 66.93887146916587\n",
      "Cost1742 = 66.93769121050218\n",
      "Cost1743 = 66.93513668460459\n",
      "Cost1744 = 66.94323438110634\n",
      "Cost1745 = 66.93494430198119\n",
      "Cost1746 = 66.93784634077046\n",
      "Cost1747 = 66.9381356210537\n",
      "Cost1748 = 66.93711709947728\n",
      "Cost1749 = 66.93935745716296\n",
      "Cost1750 = 66.93830109873214\n",
      "Cost1751 = 66.94085414523518\n",
      "Cost1752 = 66.94056282767158\n",
      "Cost1753 = 66.93894404832437\n",
      "Cost1754 = 66.93341371235282\n",
      "Cost1755 = 66.93728414029214\n",
      "Cost1756 = 66.94201294518831\n",
      "Cost1757 = 66.9319009891101\n",
      "Cost1758 = 66.93571297371723\n",
      "Cost1759 = 66.93623180057963\n",
      "Cost1760 = 66.93201835282503\n",
      "Cost1761 = 66.93332957736627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost1762 = 66.93788458306473\n",
      "Cost1763 = 66.92822840842082\n",
      "Cost1764 = 66.92954509655\n",
      "Cost1765 = 66.93764987323478\n",
      "Cost1766 = 66.93370588471849\n",
      "Cost1767 = 66.93219451592513\n",
      "Cost1768 = 66.9322128597692\n",
      "Cost1769 = 66.92876104903038\n",
      "Cost1770 = 66.93088881246182\n",
      "Cost1771 = 66.93375749833871\n",
      "Cost1772 = 66.92758096410668\n",
      "Cost1773 = 66.92979832109944\n",
      "Cost1774 = 66.92558129335599\n",
      "Cost1775 = 66.9357409852634\n",
      "Cost1776 = 66.9300833975671\n",
      "Cost1777 = 66.92918098520751\n",
      "Cost1778 = 66.92541039351836\n",
      "Cost1779 = 66.93059110234\n",
      "Cost1780 = 66.9313312781719\n",
      "Cost1781 = 66.93096745097948\n",
      "Cost1782 = 66.93327241502146\n",
      "Cost1783 = 66.93276833330204\n",
      "Cost1784 = 66.9309602849209\n",
      "Cost1785 = 66.93113596817783\n",
      "Cost1786 = 66.92960636381963\n",
      "Cost1787 = 66.92777376621981\n",
      "Cost1788 = 66.93024457300788\n",
      "Cost1789 = 66.92615834391043\n",
      "Cost1790 = 66.92406432170186\n",
      "Cost1791 = 66.93195290872468\n",
      "Cost1792 = 66.92678383214196\n",
      "Cost1793 = 66.92795110170219\n",
      "Cost1794 = 66.92655295141103\n",
      "Cost1795 = 66.92473472278043\n",
      "Cost1796 = 66.92605837286364\n",
      "Cost1797 = 66.92648021115312\n",
      "Cost1798 = 66.92329492240681\n",
      "Cost1799 = 66.92733051104089\n",
      "Cost1800 = 66.92928508494155\n",
      "Cost1801 = 66.92655330349434\n",
      "Cost1802 = 66.9263528975558\n",
      "Cost1803 = 66.9247383519436\n",
      "Cost1804 = 66.92581425429978\n",
      "Cost1805 = 66.92354956323727\n",
      "Cost1806 = 66.92443506019823\n",
      "Cost1807 = 66.92411566377217\n",
      "Cost1808 = 66.92354113834993\n",
      "Cost1809 = 66.92673135452003\n",
      "Cost1810 = 66.9262816560671\n",
      "Cost1811 = 66.9320376041203\n",
      "Cost1812 = 66.92190579326706\n",
      "Cost1813 = 66.92312080933127\n",
      "Cost1814 = 66.92711055203577\n",
      "Cost1815 = 66.92060296641746\n",
      "Cost1816 = 66.92637355182687\n",
      "Cost1817 = 66.92161731831231\n",
      "Cost1818 = 66.92444758923067\n",
      "Cost1819 = 66.92436269131738\n",
      "Cost1820 = 66.91989732254983\n",
      "Cost1821 = 66.92046449114864\n",
      "Cost1822 = 66.924869177265\n",
      "Cost1823 = 66.91589294329619\n",
      "Cost1824 = 66.92383934640567\n",
      "Cost1825 = 66.92151118997539\n",
      "Cost1826 = 66.91777733106998\n",
      "Cost1827 = 66.92060677303576\n",
      "Cost1828 = 66.9144240613783\n",
      "Cost1829 = 66.92014360473831\n",
      "Cost1830 = 66.91765006756124\n",
      "Cost1831 = 66.91258272902597\n",
      "Cost1832 = 66.9195462835372\n",
      "Cost1833 = 66.91944528086061\n",
      "Cost1834 = 66.91470799904886\n",
      "Cost1835 = 66.91341887250292\n",
      "Cost1836 = 66.91476340198732\n",
      "Cost1837 = 66.9082401374709\n",
      "Cost1838 = 66.91328827279284\n",
      "Cost1839 = 66.91083876238434\n",
      "Cost1840 = 66.91434227310502\n",
      "Cost1841 = 66.90943801372548\n",
      "Cost1842 = 66.90984889247368\n",
      "Cost1843 = 66.90870367973194\n",
      "Cost1844 = 66.91469127873121\n",
      "Cost1845 = 66.91052337601269\n",
      "Cost1846 = 66.9133497879648\n",
      "Cost1847 = 66.91105280695412\n",
      "Cost1848 = 66.9101123409235\n",
      "Cost1849 = 66.90492629046983\n",
      "Cost1850 = 66.90691062191794\n",
      "Cost1851 = 66.90356809845778\n",
      "Cost1852 = 66.90448532735785\n",
      "Cost1853 = 66.90235276256406\n",
      "Cost1854 = 66.90488027475956\n",
      "Cost1855 = 66.90730935043841\n",
      "Cost1856 = 66.90860894622769\n",
      "Cost1857 = 66.90282468805667\n",
      "Cost1858 = 66.90833005623391\n",
      "Cost1859 = 66.9049728446686\n",
      "Cost1860 = 66.89729385030353\n",
      "Cost1861 = 66.9015854355288\n",
      "Cost1862 = 66.90133266294377\n",
      "Cost1863 = 66.90368631945266\n",
      "Cost1864 = 66.90115616286515\n",
      "Cost1865 = 66.90397985381642\n",
      "Cost1866 = 66.9079053078263\n",
      "Cost1867 = 66.90212643050776\n",
      "Cost1868 = 66.89603810078829\n",
      "Cost1869 = 66.9012601773289\n",
      "Cost1870 = 66.90213210497086\n",
      "Cost1871 = 66.90017007288509\n",
      "Cost1872 = 66.89635658446133\n",
      "Cost1873 = 66.89889853237693\n",
      "Cost1874 = 66.89888184256152\n",
      "Cost1875 = 66.90188635249443\n",
      "Cost1876 = 66.898555068615\n",
      "Cost1877 = 66.89368795303959\n",
      "Cost1878 = 66.90048230219524\n",
      "Cost1879 = 66.8951974188293\n",
      "Cost1880 = 66.89622952781835\n",
      "Cost1881 = 66.89864951460697\n",
      "Cost1882 = 66.89627864521438\n",
      "Cost1883 = 66.89864978524567\n",
      "Cost1884 = 66.8920429557142\n",
      "Cost1885 = 66.89080640580221\n",
      "Cost1886 = 66.894749715637\n",
      "Cost1887 = 66.8936379939739\n",
      "Cost1888 = 66.89291907312115\n",
      "Cost1889 = 66.88673875250801\n",
      "Cost1890 = 66.89217224472921\n",
      "Cost1891 = 66.89209025638688\n",
      "Cost1892 = 66.88761845179116\n",
      "Cost1893 = 66.884340871876\n",
      "Cost1894 = 66.88179406100689\n",
      "Cost1895 = 66.88602104427108\n",
      "Cost1896 = 66.8863279450501\n",
      "Cost1897 = 66.88644765085523\n",
      "Cost1898 = 66.88458308291753\n",
      "Cost1899 = 66.88198113621301\n",
      "Cost1900 = 66.8836964246008\n",
      "Cost1901 = 66.87918257683383\n",
      "Cost1902 = 66.88067382056735\n",
      "Cost1903 = 66.8832240530236\n",
      "Cost1904 = 66.88794524375152\n",
      "Cost1905 = 66.88295603298394\n",
      "Cost1906 = 66.88028320328615\n",
      "Cost1907 = 66.88424824046062\n",
      "Cost1908 = 66.87968646431655\n",
      "Cost1909 = 66.87529710258953\n",
      "Cost1910 = 66.88285494342975\n",
      "Cost1911 = 66.87989238101316\n",
      "Cost1912 = 66.87639925784575\n",
      "Cost1913 = 66.87572931094765\n",
      "Cost1914 = 66.87752429265491\n",
      "Cost1915 = 66.8771207432892\n",
      "Cost1916 = 66.87162825977983\n",
      "Cost1917 = 66.88332789268547\n",
      "Cost1918 = 66.87338848649624\n",
      "Cost1919 = 66.87722180711856\n",
      "Cost1920 = 66.87896843191349\n",
      "Cost1921 = 66.8687666459833\n",
      "Cost1922 = 66.87031398084798\n",
      "Cost1923 = 66.86784623923329\n",
      "Cost1924 = 66.86728316769394\n",
      "Cost1925 = 66.86906865006925\n",
      "Cost1926 = 66.87086935854899\n",
      "Cost1927 = 66.86504104487445\n",
      "Cost1928 = 66.86424263694316\n",
      "Cost1929 = 66.86583926449707\n",
      "Cost1930 = 66.86223269247222\n",
      "Cost1931 = 66.86160402956646\n",
      "Cost1932 = 66.86232039855796\n",
      "Cost1933 = 66.85727908019203\n",
      "Cost1934 = 66.86453186398694\n",
      "Cost1935 = 66.86119264666415\n",
      "Cost1936 = 66.85601064019554\n",
      "Cost1937 = 66.8570008567919\n",
      "Cost1938 = 66.85226731533683\n",
      "Cost1939 = 66.85560747658295\n",
      "Cost1940 = 66.85026822179653\n",
      "Cost1941 = 66.84985268614598\n",
      "Cost1942 = 66.85343871800326\n",
      "Cost1943 = 66.85272527969119\n",
      "Cost1944 = 66.85101718533954\n",
      "Cost1945 = 66.85239001411547\n",
      "Cost1946 = 66.85135050332423\n",
      "Cost1947 = 66.84835961965217\n",
      "Cost1948 = 66.85034251285187\n",
      "Cost1949 = 66.84596757096656\n",
      "Cost1950 = 66.84361178135732\n",
      "Cost1951 = 66.84541693308198\n",
      "Cost1952 = 66.84450515132039\n",
      "Cost1953 = 66.84728903388331\n",
      "Cost1954 = 66.84227856230903\n",
      "Cost1955 = 66.84247351812024\n",
      "Cost1956 = 66.83937458223551\n",
      "Cost1957 = 66.83888054680234\n",
      "Cost1958 = 66.83924590190851\n",
      "Cost1959 = 66.8419510511871\n",
      "Cost1960 = 66.84197902938057\n",
      "Cost1961 = 66.83557282042786\n",
      "Cost1962 = 66.83495471819577\n",
      "Cost1963 = 66.83261902476902\n",
      "Cost1964 = 66.83681794441954\n",
      "Cost1965 = 66.83557156381406\n",
      "Cost1966 = 66.82965265124099\n",
      "Cost1967 = 66.8309444639137\n",
      "Cost1968 = 66.83294543912484\n",
      "Cost1969 = 66.83606797926966\n",
      "Cost1970 = 66.83118728476916\n",
      "Cost1971 = 66.83140276970312\n",
      "Cost1972 = 66.83287228264423\n",
      "Cost1973 = 66.82574195997009\n",
      "Cost1974 = 66.8276316064458\n",
      "Cost1975 = 66.8256322979283\n",
      "Cost1976 = 66.82415363596044\n",
      "Cost1977 = 66.82232908091149\n",
      "Cost1978 = 66.82155287434703\n",
      "Cost1979 = 66.82229855147165\n",
      "Cost1980 = 66.8182810629374\n",
      "Cost1981 = 66.82105866642641\n",
      "Cost1982 = 66.8232050892011\n",
      "Cost1983 = 66.81712037224028\n",
      "Cost1984 = 66.81846757826744\n",
      "Cost1985 = 66.81488918260057\n",
      "Cost1986 = 66.81882966962414\n",
      "Cost1987 = 66.81898685258169\n",
      "Cost1988 = 66.81380810480299\n",
      "Cost1989 = 66.81218097663081\n",
      "Cost1990 = 66.80945543860962\n",
      "Cost1991 = 66.81503057490808\n",
      "Cost1992 = 66.81166538722442\n",
      "Cost1993 = 66.81658511770607\n",
      "Cost1994 = 66.81224066966706\n",
      "Cost1995 = 66.80627842796036\n",
      "Cost1996 = 66.81064170651844\n",
      "Cost1997 = 66.81128534422358\n",
      "Cost1998 = 66.80882097870816\n",
      "Cost1999 = 66.8100184345293\n",
      "Cost2000 = 66.80869651272954\n",
      "Cost2001 = 66.8071816940304\n",
      "Cost2002 = 66.80787016750023\n",
      "Cost2003 = 66.80477415769505\n",
      "Cost2004 = 66.80049186577465\n",
      "Cost2005 = 66.80663660438113\n",
      "Cost2006 = 66.80237471309152\n",
      "Cost2007 = 66.80563454493434\n",
      "Cost2008 = 66.80153135037195\n",
      "Cost2009 = 66.8016460599264\n",
      "Cost2010 = 66.7965618348674\n",
      "Cost2011 = 66.79767201448882\n",
      "Cost2012 = 66.79975192019441\n",
      "Cost2013 = 66.79848626585975\n",
      "Cost2014 = 66.80046786701158\n",
      "Cost2015 = 66.79931076441495\n",
      "Cost2016 = 66.79228957225749\n",
      "Cost2017 = 66.79553133783155\n",
      "Cost2018 = 66.79718554536008\n",
      "Cost2019 = 66.78803120643228\n",
      "Cost2020 = 66.79049375988632\n",
      "Cost2021 = 66.78887094697569\n",
      "Cost2022 = 66.79043862447541\n",
      "Cost2023 = 66.79416889940515\n",
      "Cost2024 = 66.79099904523402\n",
      "Cost2025 = 66.79001408491527\n",
      "Cost2026 = 66.78751502010542\n",
      "Cost2027 = 66.7854770461779\n",
      "Cost2028 = 66.78981684080773\n",
      "Cost2029 = 66.78478904458684\n",
      "Cost2030 = 66.78542779727628\n",
      "Cost2031 = 66.77926855845038\n",
      "Cost2032 = 66.7868321745135\n",
      "Cost2033 = 66.78524865881171\n",
      "Cost2034 = 66.78091350841926\n",
      "Cost2035 = 66.77613267881856\n",
      "Cost2036 = 66.77693901514513\n",
      "Cost2037 = 66.77395317413189\n",
      "Cost2038 = 66.77537135903138\n",
      "Cost2039 = 66.77994306622254\n",
      "Cost2040 = 66.77376458209729\n",
      "Cost2041 = 66.77269759452741\n",
      "Cost2042 = 66.7757372206526\n",
      "Cost2043 = 66.77404334377049\n",
      "Cost2044 = 66.76967801876076\n",
      "Cost2045 = 66.76715782169124\n",
      "Cost2046 = 66.76612890110573\n",
      "Cost2047 = 66.76692399606905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost2048 = 66.76732209523762\n",
      "Cost2049 = 66.77201123538157\n",
      "Cost2050 = 66.76919530915423\n",
      "Cost2051 = 66.76679473077559\n",
      "Cost2052 = 66.76535884454677\n",
      "Cost2053 = 66.76647846811814\n",
      "Cost2054 = 66.7653584769589\n",
      "Cost2055 = 66.76369246306514\n",
      "Cost2056 = 66.75892840849806\n",
      "Cost2057 = 66.75982578367945\n",
      "Cost2058 = 66.76056255106433\n",
      "Cost2059 = 66.75917010519353\n",
      "Cost2060 = 66.75971594915946\n",
      "Cost2061 = 66.7533813044938\n",
      "Cost2062 = 66.75411420494761\n",
      "Cost2063 = 66.75131568983716\n",
      "Cost2064 = 66.75716574145922\n",
      "Cost2065 = 66.7566944297197\n",
      "Cost2066 = 66.75259424138832\n",
      "Cost2067 = 66.75139800360151\n",
      "Cost2068 = 66.75187671665078\n",
      "Cost2069 = 66.75096279218648\n",
      "Cost2070 = 66.74710090987107\n",
      "Cost2071 = 66.74612077526821\n",
      "Cost2072 = 66.75067614641448\n",
      "Cost2073 = 66.74034341594252\n",
      "Cost2074 = 66.74521200115846\n",
      "Cost2075 = 66.74390458635048\n",
      "Cost2076 = 66.73920490269465\n",
      "Cost2077 = 66.7422160822328\n",
      "Cost2078 = 66.74275835929237\n",
      "Cost2079 = 66.74375072856391\n",
      "Cost2080 = 66.73855661097073\n",
      "Cost2081 = 66.74318939324824\n",
      "Cost2082 = 66.7369482655784\n",
      "Cost2083 = 66.73293494659848\n",
      "Cost2084 = 66.73064291844011\n",
      "Cost2085 = 66.73606099625079\n",
      "Cost2086 = 66.74076944135363\n",
      "Cost2087 = 66.73355496535069\n",
      "Cost2088 = 66.73140336342104\n",
      "Cost2089 = 66.72550293486505\n",
      "Cost2090 = 66.72999773214327\n",
      "Cost2091 = 66.72378967780175\n",
      "Cost2092 = 66.7296435372924\n",
      "Cost2093 = 66.72907900560219\n",
      "Cost2094 = 66.72890405565562\n",
      "Cost2095 = 66.72691515221896\n",
      "Cost2096 = 66.7214952268267\n",
      "Cost2097 = 66.7244682891077\n",
      "Cost2098 = 66.72535707443099\n",
      "Cost2099 = 66.72014549442659\n",
      "Cost2100 = 66.71965705443112\n",
      "Cost2101 = 66.71789012948811\n",
      "Cost2102 = 66.72315821573638\n",
      "Cost2103 = 66.71480775670013\n",
      "Cost2104 = 66.71643283342031\n",
      "Cost2105 = 66.72084787257099\n",
      "Cost2106 = 66.71609598520246\n",
      "Cost2107 = 66.71373135865238\n",
      "Cost2108 = 66.71645187905628\n",
      "Cost2109 = 66.71748390872654\n",
      "Cost2110 = 66.71545188756416\n",
      "Cost2111 = 66.71496162397196\n",
      "Cost2112 = 66.71408191034371\n",
      "Cost2113 = 66.708511076316\n",
      "Cost2114 = 66.70824373348285\n",
      "Cost2115 = 66.71015265834696\n",
      "Cost2116 = 66.7072684517407\n",
      "Cost2117 = 66.7065349225796\n",
      "Cost2118 = 66.70962908000007\n",
      "Cost2119 = 66.70250932945874\n",
      "Cost2120 = 66.70212458105546\n",
      "Cost2121 = 66.70142239475608\n",
      "Cost2122 = 66.70449574652352\n",
      "Cost2123 = 66.70003376695163\n",
      "Cost2124 = 66.69946141714631\n",
      "Cost2125 = 66.69842761640072\n",
      "Cost2126 = 66.70071926400112\n",
      "Cost2127 = 66.696296493273\n",
      "Cost2128 = 66.69712846068349\n",
      "Cost2129 = 66.69748400099728\n",
      "Cost2130 = 66.69235007341796\n",
      "Cost2131 = 66.6948117742314\n",
      "Cost2132 = 66.69811235525233\n",
      "Cost2133 = 66.69693852083327\n",
      "Cost2134 = 66.68708531222036\n",
      "Cost2135 = 66.69151664519217\n",
      "Cost2136 = 66.69471332499931\n",
      "Cost2137 = 66.69004528453226\n",
      "Cost2138 = 66.690730399969\n",
      "Cost2139 = 66.69211413887358\n",
      "Cost2140 = 66.68432009919336\n",
      "Cost2141 = 66.69315333490941\n",
      "Cost2142 = 66.68692653759226\n",
      "Cost2143 = 66.6857433813796\n",
      "Cost2144 = 66.68781917213654\n",
      "Cost2145 = 66.68389321855538\n",
      "Cost2146 = 66.6911754499993\n",
      "Cost2147 = 66.68389866608183\n",
      "Cost2148 = 66.68466475373836\n",
      "Cost2149 = 66.68456262450974\n",
      "Cost2150 = 66.68214808394414\n",
      "Cost2151 = 66.68465361368895\n",
      "Cost2152 = 66.67658322959419\n",
      "Cost2153 = 66.67716368077524\n",
      "Cost2154 = 66.68332099417026\n",
      "Cost2155 = 66.67709055712884\n",
      "Cost2156 = 66.67714838335587\n",
      "Cost2157 = 66.6728411056085\n",
      "Cost2158 = 66.67642197593858\n",
      "Cost2159 = 66.67564377821714\n",
      "Cost2160 = 66.67162577137394\n",
      "Cost2161 = 66.67154338761063\n",
      "Cost2162 = 66.67137739876915\n",
      "Cost2163 = 66.6706967563202\n",
      "Cost2164 = 66.67011403015304\n",
      "Cost2165 = 66.67269510149083\n",
      "Cost2166 = 66.6700520964946\n",
      "Cost2167 = 66.66665138000342\n",
      "Cost2168 = 66.67195783000864\n",
      "Cost2169 = 66.66608786987719\n",
      "Cost2170 = 66.67121621429176\n",
      "Cost2171 = 66.66699451221265\n",
      "Cost2172 = 66.66654493359653\n",
      "Cost2173 = 66.66408023207349\n",
      "Cost2174 = 66.66067708491877\n",
      "Cost2175 = 66.66069715819579\n",
      "Cost2176 = 66.66060564807127\n",
      "Cost2177 = 66.66077738176824\n",
      "Cost2178 = 66.66637090946989\n",
      "Cost2179 = 66.66497945438311\n",
      "Cost2180 = 66.6625191366917\n",
      "Cost2181 = 66.66927355673911\n",
      "Cost2182 = 66.66314272421494\n",
      "Cost2183 = 66.66471063731629\n",
      "Cost2184 = 66.66558272116319\n",
      "Cost2185 = 66.66220121308582\n",
      "Cost2186 = 66.66252196797399\n",
      "Cost2187 = 66.66021221861517\n",
      "Cost2188 = 66.66199393149854\n",
      "Cost2189 = 66.66267459138284\n",
      "Cost2190 = 66.65334357024759\n",
      "Cost2191 = 66.66250991355118\n",
      "Cost2192 = 66.65682240989146\n",
      "Cost2193 = 66.65671010663796\n",
      "Cost2194 = 66.6536165998722\n",
      "Cost2195 = 66.6571458574968\n",
      "Cost2196 = 66.65679155254941\n",
      "Cost2197 = 66.65559860973993\n",
      "Cost2198 = 66.65941133819061\n",
      "Cost2199 = 66.65629265933609\n",
      "Cost2200 = 66.65657962180127\n",
      "Cost2201 = 66.6568529237239\n",
      "Cost2202 = 66.65554757130451\n",
      "Cost2203 = 66.65785524573877\n",
      "Cost2204 = 66.65254409954186\n",
      "Cost2205 = 66.65640863881792\n",
      "Cost2206 = 66.65212790929016\n",
      "Cost2207 = 66.65109932084555\n",
      "Cost2208 = 66.65049811154125\n",
      "Cost2209 = 66.6478265109379\n",
      "Cost2210 = 66.64524956813983\n",
      "Cost2211 = 66.65092406114032\n",
      "Cost2212 = 66.65073587625255\n",
      "Cost2213 = 66.65006382067408\n",
      "Cost2214 = 66.64181414549648\n",
      "Cost2215 = 66.64251881879044\n",
      "Cost2216 = 66.64189499858787\n",
      "Cost2217 = 66.6424583451048\n",
      "Cost2218 = 66.64597261281847\n",
      "Cost2219 = 66.64161862680928\n",
      "Cost2220 = 66.64302555717198\n",
      "Cost2221 = 66.63883266406567\n",
      "Cost2222 = 66.64393351530092\n",
      "Cost2223 = 66.6419032084802\n",
      "Cost2224 = 66.64286254387522\n",
      "Cost2225 = 66.64722633320784\n",
      "Cost2226 = 66.64728482403383\n",
      "Cost2227 = 66.64547167732381\n",
      "Cost2228 = 66.64844197003048\n",
      "Cost2229 = 66.64321297356425\n",
      "Cost2230 = 66.64529606687658\n",
      "Cost2231 = 66.64572016802259\n",
      "Cost2232 = 66.63883761639356\n",
      "Cost2233 = 66.63585066102269\n",
      "Cost2234 = 66.64100768252577\n",
      "Cost2235 = 66.63972855709949\n",
      "Cost2236 = 66.63956510575694\n",
      "Cost2237 = 66.64084432337853\n",
      "Cost2238 = 66.64127111999862\n",
      "Cost2239 = 66.63962212380177\n",
      "Cost2240 = 66.63501124459702\n",
      "Cost2241 = 66.63607280518416\n",
      "Cost2242 = 66.6347509703116\n",
      "Cost2243 = 66.63414594683213\n",
      "Cost2244 = 66.63423347891431\n",
      "Cost2245 = 66.63782815110197\n",
      "Cost2246 = 66.6373346836886\n",
      "Cost2247 = 66.63884536614816\n",
      "Cost2248 = 66.63972519099889\n",
      "Cost2249 = 66.63693406647675\n",
      "Cost2250 = 66.63293223778534\n",
      "Cost2251 = 66.62616453581259\n",
      "Cost2252 = 66.63011510250178\n",
      "Cost2253 = 66.63967690788606\n",
      "Cost2254 = 66.62881603862535\n",
      "Cost2255 = 66.63229752556092\n",
      "Cost2256 = 66.63446898004825\n",
      "Cost2257 = 66.62723812637225\n",
      "Cost2258 = 66.62756375672417\n",
      "Cost2259 = 66.63022431852932\n",
      "Cost2260 = 66.62729617610319\n",
      "Cost2261 = 66.62462728272052\n",
      "Cost2262 = 66.62914622798178\n",
      "Cost2263 = 66.62679784348138\n",
      "Cost2264 = 66.62429347202257\n",
      "Cost2265 = 66.62225744344377\n",
      "Cost2266 = 66.6296340959798\n",
      "Cost2267 = 66.62484997431511\n",
      "Cost2268 = 66.62139710178529\n",
      "Cost2269 = 66.63096180223025\n",
      "Cost2270 = 66.61869352064392\n",
      "Cost2271 = 66.62593654232205\n",
      "Cost2272 = 66.62717318556669\n",
      "Cost2273 = 66.62224343666594\n",
      "Cost2274 = 66.62412340213449\n",
      "Cost2275 = 66.61998924863931\n",
      "Cost2276 = 66.62187167188958\n",
      "Cost2277 = 66.61698676637772\n",
      "Cost2278 = 66.61768453424129\n",
      "Cost2279 = 66.6188090465874\n",
      "Cost2280 = 66.61983637588386\n",
      "Cost2281 = 66.61922562824523\n",
      "Cost2282 = 66.61690550854341\n",
      "Cost2283 = 66.61742823600099\n",
      "Cost2284 = 66.61746240285606\n",
      "Cost2285 = 66.62057270517202\n",
      "Cost2286 = 66.62205724785782\n",
      "Cost2287 = 66.62066396549439\n",
      "Cost2288 = 66.61440483766334\n",
      "Cost2289 = 66.61108556467417\n",
      "Cost2290 = 66.60800130704898\n",
      "Cost2291 = 66.61203165153351\n",
      "Cost2292 = 66.61111549148924\n",
      "Cost2293 = 66.60719860808092\n",
      "Cost2294 = 66.61068360803375\n",
      "Cost2295 = 66.61220640800465\n",
      "Cost2296 = 66.61019966184827\n",
      "Cost2297 = 66.61132285325193\n",
      "Cost2298 = 66.60307570192346\n",
      "Cost2299 = 66.60634434939884\n",
      "Cost2300 = 66.60809492263469\n",
      "Cost2301 = 66.60619133542896\n",
      "Cost2302 = 66.60823609965753\n",
      "Cost2303 = 66.60135981470526\n",
      "Cost2304 = 66.60503447801662\n",
      "Cost2305 = 66.603246026558\n",
      "Cost2306 = 66.60382049196834\n",
      "Cost2307 = 66.6023849509102\n",
      "Cost2308 = 66.59892572653249\n",
      "Cost2309 = 66.60097760379207\n",
      "Cost2310 = 66.60177466358749\n",
      "Cost2311 = 66.60110690698487\n",
      "Cost2312 = 66.60203707013702\n",
      "Cost2313 = 66.60251697142998\n",
      "Cost2314 = 66.59839198452202\n",
      "Cost2315 = 66.59927616227947\n",
      "Cost2316 = 66.59696006406138\n",
      "Cost2317 = 66.59482819021218\n",
      "Cost2318 = 66.59927031080879\n",
      "Cost2319 = 66.59991309199751\n",
      "Cost2320 = 66.59231944058959\n",
      "Cost2321 = 66.59563993235543\n",
      "Cost2322 = 66.59434487402069\n",
      "Cost2323 = 66.59726611596133\n",
      "Cost2324 = 66.59390880595402\n",
      "Cost2325 = 66.59334166483083\n",
      "Cost2326 = 66.58892636679164\n",
      "Cost2327 = 66.59291670631472\n",
      "Cost2328 = 66.58733391356346\n",
      "Cost2329 = 66.5958378244554\n",
      "Cost2330 = 66.58965260586194\n",
      "Cost2331 = 66.58887480919488\n",
      "Cost2332 = 66.59590749135329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost2333 = 66.59242002904857\n",
      "Cost2334 = 66.58519050568903\n",
      "Cost2335 = 66.58887519190233\n",
      "Cost2336 = 66.58867063349686\n",
      "Cost2337 = 66.58633131972198\n",
      "Cost2338 = 66.59142097413526\n",
      "Cost2339 = 66.58959027015355\n",
      "Cost2340 = 66.58659186271633\n",
      "Cost2341 = 66.58159783938184\n",
      "Cost2342 = 66.58306462838783\n",
      "Cost2343 = 66.58606070849761\n",
      "Cost2344 = 66.57857000788083\n",
      "Cost2345 = 66.57982416057611\n",
      "Cost2346 = 66.58267434695783\n",
      "Cost2347 = 66.58019642168584\n",
      "Cost2348 = 66.57606953795764\n",
      "Cost2349 = 66.57719576859193\n",
      "Cost2350 = 66.57748293629865\n",
      "Cost2351 = 66.57777475580941\n",
      "Cost2352 = 66.57542834164637\n",
      "Cost2353 = 66.57799753924061\n",
      "Cost2354 = 66.5776629003819\n",
      "Cost2355 = 66.57987108131152\n",
      "Cost2356 = 66.57415063570176\n",
      "Cost2357 = 66.57377062231798\n",
      "Cost2358 = 66.57697341278387\n",
      "Cost2359 = 66.57008652169593\n",
      "Cost2360 = 66.57444236806994\n",
      "Cost2361 = 66.57131745071173\n",
      "Cost2362 = 66.56695944691688\n",
      "Cost2363 = 66.5677346849932\n",
      "Cost2364 = 66.56818173300377\n",
      "Cost2365 = 66.56680016426387\n",
      "Cost2366 = 66.56486090387374\n",
      "Cost2367 = 66.56835876907328\n",
      "Cost2368 = 66.57175786643442\n",
      "Cost2369 = 66.57012052528084\n",
      "Cost2370 = 66.56690084686403\n",
      "Cost2371 = 66.56715003391612\n",
      "Cost2372 = 66.565077090575\n",
      "Cost2373 = 66.56156020518657\n",
      "Cost2374 = 66.56632496706723\n",
      "Cost2375 = 66.56016387471426\n",
      "Cost2376 = 66.56686657027375\n",
      "Cost2377 = 66.56430256326793\n",
      "Cost2378 = 66.56684908035983\n",
      "Cost2379 = 66.56266182467839\n",
      "Cost2380 = 66.56192891088224\n",
      "Cost2381 = 66.56272352229304\n",
      "Cost2382 = 66.5608433566461\n",
      "Cost2383 = 66.56247902123428\n",
      "Cost2384 = 66.55949462303698\n",
      "Cost2385 = 66.5580025095492\n",
      "Cost2386 = 66.5591847910372\n",
      "Cost2387 = 66.5550612560518\n",
      "Cost2388 = 66.56265027177965\n",
      "Cost2389 = 66.55802521372922\n",
      "Cost2390 = 66.55830667363213\n",
      "Cost2391 = 66.55656324695222\n",
      "Cost2392 = 66.56177443885255\n",
      "Cost2393 = 66.55134894175106\n",
      "Cost2394 = 66.5502759463498\n",
      "Cost2395 = 66.55295504974477\n",
      "Cost2396 = 66.55772898199156\n",
      "Cost2397 = 66.55208698380417\n",
      "Cost2398 = 66.54914058131632\n",
      "Cost2399 = 66.5450999078738\n",
      "Cost2400 = 66.55342951300737\n",
      "Cost2401 = 66.55541943201966\n",
      "Cost2402 = 66.5473079762113\n",
      "Cost2403 = 66.54305218707854\n",
      "Cost2404 = 66.55181411888715\n",
      "Cost2405 = 66.54717144354315\n",
      "Cost2406 = 66.54170252673993\n",
      "Cost2407 = 66.54846707585779\n",
      "Cost2408 = 66.54522108817669\n",
      "Cost2409 = 66.54161927824408\n",
      "Cost2410 = 66.54524635321144\n",
      "Cost2411 = 66.54534542119755\n",
      "Cost2412 = 66.54935443010939\n",
      "Cost2413 = 66.53848936971019\n",
      "Cost2414 = 66.54273666880694\n",
      "Cost2415 = 66.53777544155146\n",
      "Cost2416 = 66.54274976216716\n",
      "Cost2417 = 66.53491097178605\n",
      "Cost2418 = 66.53533321789861\n",
      "Cost2419 = 66.53988807429337\n",
      "Cost2420 = 66.5416992875144\n",
      "Cost2421 = 66.53883818697973\n",
      "Cost2422 = 66.53419800382576\n",
      "Cost2423 = 66.53678410441165\n",
      "Cost2424 = 66.53808162521901\n",
      "Cost2425 = 66.53466972026492\n",
      "Cost2426 = 66.53297929114967\n",
      "Cost2427 = 66.53658292518467\n",
      "Cost2428 = 66.53090261382683\n",
      "Cost2429 = 66.53415400542329\n",
      "Cost2430 = 66.52725846903164\n",
      "Cost2431 = 66.53286465274716\n",
      "Cost2432 = 66.52881064858961\n",
      "Cost2433 = 66.53493437656131\n",
      "Cost2434 = 66.5287175506878\n",
      "Cost2435 = 66.52894537440041\n",
      "Cost2436 = 66.52779054727459\n",
      "Cost2437 = 66.53416377671563\n",
      "Cost2438 = 66.52366745024715\n",
      "Cost2439 = 66.5252005184252\n",
      "Cost2440 = 66.52895923469373\n",
      "Cost2441 = 66.52555681836556\n",
      "Cost2442 = 66.52272541157748\n",
      "Cost2443 = 66.52736142516805\n",
      "Cost2444 = 66.52515390722502\n",
      "Cost2445 = 66.5216312408527\n",
      "Cost2446 = 66.5228827286102\n",
      "Cost2447 = 66.52587892562853\n",
      "Cost2448 = 66.5256437599101\n",
      "Cost2449 = 66.52304835921663\n",
      "Cost2450 = 66.51979486562949\n",
      "Cost2451 = 66.5197541145805\n",
      "Cost2452 = 66.51896519042332\n",
      "Cost2453 = 66.52288491659563\n",
      "Cost2454 = 66.52166079597872\n",
      "Cost2455 = 66.51862725906409\n",
      "Cost2456 = 66.51691968373308\n",
      "Cost2457 = 66.51457376924189\n",
      "Cost2458 = 66.51432220392579\n",
      "Cost2459 = 66.51749110129161\n",
      "Cost2460 = 66.51832806060662\n",
      "Cost2461 = 66.51810376741885\n",
      "Cost2462 = 66.51491789752183\n",
      "Cost2463 = 66.51259969723576\n",
      "Cost2464 = 66.50881911724613\n",
      "Cost2465 = 66.51195942062657\n",
      "Cost2466 = 66.50924603412257\n",
      "Cost2467 = 66.51378906409828\n",
      "Cost2468 = 66.51232345839492\n",
      "Cost2469 = 66.51238273522732\n",
      "Cost2470 = 66.5097322042283\n",
      "Cost2471 = 66.50940950562698\n",
      "Cost2472 = 66.50544901470674\n",
      "Cost2473 = 66.5055498072962\n",
      "Cost2474 = 66.50595295720561\n",
      "Cost2475 = 66.50095170839194\n",
      "Cost2476 = 66.50397117615846\n",
      "Cost2477 = 66.50738653397244\n",
      "Cost2478 = 66.50219411525066\n",
      "Cost2479 = 66.50093374703533\n",
      "Cost2480 = 66.50170702200546\n",
      "Cost2481 = 66.50385213670694\n",
      "Cost2482 = 66.49984130320931\n",
      "Cost2483 = 66.50193117142486\n",
      "Cost2484 = 66.50465677536725\n",
      "Cost2485 = 66.50362920843743\n",
      "Cost2486 = 66.50111155404397\n",
      "Cost2487 = 66.49904723185557\n",
      "Cost2488 = 66.49821294798626\n",
      "Cost2489 = 66.49729887151781\n",
      "Cost2490 = 66.49829863021101\n",
      "Cost2491 = 66.4982274064941\n",
      "Cost2492 = 66.49512971390504\n",
      "Cost2493 = 66.4962653926594\n",
      "Cost2494 = 66.4970375513249\n",
      "Cost2495 = 66.49330667147828\n",
      "Cost2496 = 66.49543709911961\n",
      "Cost2497 = 66.48867467320098\n",
      "Cost2498 = 66.49228531677997\n",
      "Cost2499 = 66.4941324139755\n",
      "Cost2500 = 66.49740394813247\n",
      "Cost2501 = 66.49710151474031\n",
      "Cost2502 = 66.4976609255838\n",
      "Cost2503 = 66.49420658294859\n",
      "Cost2504 = 66.49501401230486\n",
      "Cost2505 = 66.4890979493377\n",
      "Cost2506 = 66.49413592604509\n",
      "Cost2507 = 66.48587975936837\n",
      "Cost2508 = 66.4921153805276\n",
      "Cost2509 = 66.48877209514323\n",
      "Cost2510 = 66.4913535651711\n",
      "Cost2511 = 66.4882365655009\n",
      "Cost2512 = 66.48935815209117\n",
      "Cost2513 = 66.48442105775321\n",
      "Cost2514 = 66.4851617092957\n",
      "Cost2515 = 66.48192271965975\n",
      "Cost2516 = 66.48300696699953\n",
      "Cost2517 = 66.477053216559\n",
      "Cost2518 = 66.48738217790287\n",
      "Cost2519 = 66.48332074902729\n",
      "Cost2520 = 66.48565408768054\n",
      "Cost2521 = 66.47836672671428\n",
      "Cost2522 = 66.48461401949146\n",
      "Cost2523 = 66.48010205555674\n",
      "Cost2524 = 66.48042796344134\n",
      "Cost2525 = 66.4796374588524\n",
      "Cost2526 = 66.47744430839818\n",
      "Cost2527 = 66.47770632919088\n",
      "Cost2528 = 66.47898757137689\n",
      "Cost2529 = 66.47769190940923\n",
      "Cost2530 = 66.47938496967137\n",
      "Cost2531 = 66.47882110879448\n",
      "Cost2532 = 66.47933039285388\n",
      "Cost2533 = 66.47486475448214\n",
      "Cost2534 = 66.47632997621703\n",
      "Cost2535 = 66.47746717058473\n",
      "Cost2536 = 66.47367380743032\n",
      "Cost2537 = 66.47379602428927\n",
      "Cost2538 = 66.47959825983489\n",
      "Cost2539 = 66.47210169958561\n",
      "Cost2540 = 66.4712426370927\n",
      "Cost2541 = 66.4751885025712\n",
      "Cost2542 = 66.47341190652232\n",
      "Cost2543 = 66.47603408758015\n",
      "Cost2544 = 66.47415395479139\n",
      "Cost2545 = 66.47051575100929\n",
      "Cost2546 = 66.4716934654339\n",
      "Cost2547 = 66.47119298342713\n",
      "Cost2548 = 66.46688026843323\n",
      "Cost2549 = 66.46981452161823\n",
      "Cost2550 = 66.46442968558691\n",
      "Cost2551 = 66.4716076254617\n",
      "Cost2552 = 66.46755027085736\n",
      "Cost2553 = 66.46188163782693\n",
      "Cost2554 = 66.47001845091118\n",
      "Cost2555 = 66.46508490062324\n",
      "Cost2556 = 66.46494592551139\n",
      "Cost2557 = 66.46250871668353\n",
      "Cost2558 = 66.4675259357107\n",
      "Cost2559 = 66.46707508735845\n",
      "Cost2560 = 66.46090742847565\n",
      "Cost2561 = 66.4662360715006\n",
      "Cost2562 = 66.46474719118191\n",
      "Cost2563 = 66.45805452091334\n",
      "Cost2564 = 66.46795026910729\n",
      "Cost2565 = 66.4620288375925\n",
      "Cost2566 = 66.46103231664875\n",
      "Cost2567 = 66.46378722983236\n",
      "Cost2568 = 66.45538384650338\n",
      "Cost2569 = 66.45974752279449\n",
      "Cost2570 = 66.46078291579434\n",
      "Cost2571 = 66.45785037586201\n",
      "Cost2572 = 66.45867885870993\n",
      "Cost2573 = 66.45665299897871\n",
      "Cost2574 = 66.4566924987212\n",
      "Cost2575 = 66.45568518299888\n",
      "Cost2576 = 66.45845884067509\n",
      "Cost2577 = 66.4604429577705\n",
      "Cost2578 = 66.45588615178852\n",
      "Cost2579 = 66.45738625770159\n",
      "Cost2580 = 66.45589197441055\n",
      "Cost2581 = 66.45510235074387\n",
      "Cost2582 = 66.45478245839348\n",
      "Cost2583 = 66.45345794489701\n",
      "Cost2584 = 66.45226934162443\n",
      "Cost2585 = 66.45049097934258\n",
      "Cost2586 = 66.45210747719588\n",
      "Cost2587 = 66.4529334326729\n",
      "Cost2588 = 66.44885204548387\n",
      "Cost2589 = 66.45607201663668\n",
      "Cost2590 = 66.45102141004799\n",
      "Cost2591 = 66.4519380963057\n",
      "Cost2592 = 66.44679680107575\n",
      "Cost2593 = 66.44598005524601\n",
      "Cost2594 = 66.4449464291466\n",
      "Cost2595 = 66.44870355701202\n",
      "Cost2596 = 66.44950440382546\n",
      "Cost2597 = 66.45103506807357\n",
      "Cost2598 = 66.44452349155229\n",
      "Cost2599 = 66.44538711628614\n",
      "Cost2600 = 66.44590303330423\n",
      "Cost2601 = 66.44540656704051\n",
      "Cost2602 = 66.44094794026901\n",
      "Cost2603 = 66.44231635968067\n",
      "Cost2604 = 66.44308336357084\n",
      "Cost2605 = 66.44391369659857\n",
      "Cost2606 = 66.44428042845391\n",
      "Cost2607 = 66.44565826048921\n",
      "Cost2608 = 66.43882064500487\n",
      "Cost2609 = 66.44330608867878\n",
      "Cost2610 = 66.44153303593247\n",
      "Cost2611 = 66.444313089897\n",
      "Cost2612 = 66.43968754671236\n",
      "Cost2613 = 66.44345055122554\n",
      "Cost2614 = 66.43822246892208\n",
      "Cost2615 = 66.44605570336577\n",
      "Cost2616 = 66.44345650499224\n",
      "Cost2617 = 66.43793659516417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost2618 = 66.43872669264458\n",
      "Cost2619 = 66.44036211359908\n",
      "Cost2620 = 66.43830759026679\n",
      "Cost2621 = 66.43450194338966\n",
      "Cost2622 = 66.43691765481893\n",
      "Cost2623 = 66.43609051177003\n",
      "Cost2624 = 66.4347602489451\n",
      "Cost2625 = 66.43684734363163\n",
      "Cost2626 = 66.43392285109121\n",
      "Cost2627 = 66.43614937721193\n",
      "Cost2628 = 66.43491504292126\n",
      "Cost2629 = 66.43382928302492\n",
      "Cost2630 = 66.43201122886448\n",
      "Cost2631 = 66.42810663387806\n",
      "Cost2632 = 66.43018844897789\n",
      "Cost2633 = 66.43259411945913\n",
      "Cost2634 = 66.43285490669639\n",
      "Cost2635 = 66.43478568490009\n",
      "Cost2636 = 66.43336856077892\n",
      "Cost2637 = 66.43036372402113\n",
      "Cost2638 = 66.43476507380039\n",
      "Cost2639 = 66.4279006246381\n",
      "Cost2640 = 66.4298085270058\n",
      "Cost2641 = 66.42819477838698\n",
      "Cost2642 = 66.4253214107125\n",
      "Cost2643 = 66.42879510206326\n",
      "Cost2644 = 66.43156046006365\n",
      "Cost2645 = 66.42538865446646\n",
      "Cost2646 = 66.42883076480364\n",
      "Cost2647 = 66.42208161743461\n",
      "Cost2648 = 66.42895034270978\n",
      "Cost2649 = 66.42835393037011\n",
      "Cost2650 = 66.42284350576784\n",
      "Cost2651 = 66.42690576763304\n",
      "Cost2652 = 66.42352085600861\n",
      "Cost2653 = 66.42452584919467\n",
      "Cost2654 = 66.4226718000274\n",
      "Cost2655 = 66.4245600274684\n",
      "Cost2656 = 66.41867000683034\n",
      "Cost2657 = 66.43186139620305\n",
      "Cost2658 = 66.42915508718433\n",
      "Cost2659 = 66.42365076364301\n",
      "Cost2660 = 66.42163400640574\n",
      "Cost2661 = 66.42154802077647\n",
      "Cost2662 = 66.42405238142142\n",
      "Cost2663 = 66.41761945167829\n",
      "Cost2664 = 66.42071953916128\n",
      "Cost2665 = 66.42388358482125\n",
      "Cost2666 = 66.41934577300528\n",
      "Cost2667 = 66.41571053375205\n",
      "Cost2668 = 66.42047145283382\n",
      "Cost2669 = 66.41915069783906\n",
      "Cost2670 = 66.4155341070245\n",
      "Cost2671 = 66.41630333008568\n",
      "Cost2672 = 66.42065127230761\n",
      "Cost2673 = 66.41712440184675\n",
      "Cost2674 = 66.41419905966681\n",
      "Cost2675 = 66.41622673419296\n",
      "Cost2676 = 66.41636113704698\n",
      "Cost2677 = 66.417272586385\n",
      "Cost2678 = 66.416264630287\n",
      "Cost2679 = 66.42068219059075\n",
      "Cost2680 = 66.4145700329467\n",
      "Cost2681 = 66.41283496750145\n",
      "Cost2682 = 66.41431488489407\n",
      "Cost2683 = 66.41911284873117\n",
      "Cost2684 = 66.41462916771253\n",
      "Cost2685 = 66.40679774106594\n",
      "Cost2686 = 66.40998725953474\n",
      "Cost2687 = 66.41088912297448\n",
      "Cost2688 = 66.41211192799575\n",
      "Cost2689 = 66.41323489807237\n",
      "Cost2690 = 66.4095653375768\n",
      "Cost2691 = 66.41091971203822\n",
      "Cost2692 = 66.4082562894863\n",
      "Cost2693 = 66.40819428303872\n",
      "Cost2694 = 66.4111398378483\n",
      "Cost2695 = 66.40678288790504\n",
      "Cost2696 = 66.40602818086343\n",
      "Cost2697 = 66.41130834599412\n",
      "Cost2698 = 66.41001737073555\n",
      "Cost2699 = 66.40388425901206\n",
      "Cost2700 = 66.41363549028996\n",
      "Cost2701 = 66.4056560759386\n",
      "Cost2702 = 66.40295319432352\n",
      "Cost2703 = 66.41062601574127\n",
      "Cost2704 = 66.4092822070705\n",
      "Cost2705 = 66.41394002723398\n",
      "Cost2706 = 66.40612791490977\n",
      "Cost2707 = 66.40348313849516\n",
      "Cost2708 = 66.40662910115864\n",
      "Cost2709 = 66.40541639477715\n",
      "Cost2710 = 66.40202284176416\n",
      "Cost2711 = 66.4024141939644\n",
      "Cost2712 = 66.40253010684403\n",
      "Cost2713 = 66.40006731530421\n",
      "Cost2714 = 66.40314999813106\n",
      "Cost2715 = 66.40406181936201\n",
      "Cost2716 = 66.40243779310363\n",
      "Cost2717 = 66.40543892237447\n",
      "Cost2718 = 66.3993542986392\n",
      "Cost2719 = 66.40210260138734\n",
      "Cost2720 = 66.40301176013931\n",
      "Cost2721 = 66.39774903222272\n",
      "Cost2722 = 66.39990281161354\n",
      "Cost2723 = 66.4011107037899\n",
      "Cost2724 = 66.40035309635167\n",
      "Cost2725 = 66.39690177352146\n",
      "Cost2726 = 66.39900619199636\n",
      "Cost2727 = 66.40291611936433\n",
      "Cost2728 = 66.39628760136661\n",
      "Cost2729 = 66.3986929359096\n",
      "Cost2730 = 66.39597149534275\n",
      "Cost2731 = 66.39880418002213\n",
      "Cost2732 = 66.39542476546607\n",
      "Cost2733 = 66.39375983423542\n",
      "Cost2734 = 66.39142968179922\n",
      "Cost2735 = 66.3942585721605\n",
      "Cost2736 = 66.39275096874829\n",
      "Cost2737 = 66.3984333814923\n",
      "Cost2738 = 66.39359467424929\n",
      "Cost2739 = 66.39352444069571\n",
      "Cost2740 = 66.38865625134572\n",
      "Cost2741 = 66.38928068105412\n",
      "Cost2742 = 66.39026820098155\n",
      "Cost2743 = 66.39230022597238\n",
      "Cost2744 = 66.38868434012825\n",
      "Cost2745 = 66.39443660729061\n",
      "Cost2746 = 66.38943863062586\n",
      "Cost2747 = 66.39167207178947\n",
      "Cost2748 = 66.38966128125122\n",
      "Cost2749 = 66.39030805253573\n",
      "Cost2750 = 66.38952856363657\n",
      "Cost2751 = 66.38941906055592\n",
      "Cost2752 = 66.39175240713108\n",
      "Cost2753 = 66.38833882028682\n",
      "Cost2754 = 66.38615826538732\n",
      "Cost2755 = 66.39083061523384\n",
      "Cost2756 = 66.38589167911915\n",
      "Cost2757 = 66.39258206575002\n",
      "Cost2758 = 66.38542670623437\n",
      "Cost2759 = 66.38793712477703\n",
      "Cost2760 = 66.3827143059636\n",
      "Cost2761 = 66.38159564872048\n",
      "Cost2762 = 66.38742225624998\n",
      "Cost2763 = 66.38368232355066\n",
      "Cost2764 = 66.38631894181195\n",
      "Cost2765 = 66.3858903912453\n",
      "Cost2766 = 66.3768102453618\n",
      "Cost2767 = 66.3855238329605\n",
      "Cost2768 = 66.38616863488343\n",
      "Cost2769 = 66.38122579828688\n",
      "Cost2770 = 66.38424979214743\n",
      "Cost2771 = 66.38364066075638\n",
      "Cost2772 = 66.38447739978318\n",
      "Cost2773 = 66.38313226359266\n",
      "Cost2774 = 66.37743632124452\n",
      "Cost2775 = 66.37980610351008\n",
      "Cost2776 = 66.3802403531155\n",
      "Cost2777 = 66.38765625736396\n",
      "Cost2778 = 66.37859309101607\n",
      "Cost2779 = 66.37878049794237\n",
      "Cost2780 = 66.38130879890124\n",
      "Cost2781 = 66.38047618304927\n",
      "Cost2782 = 66.3819605568707\n",
      "Cost2783 = 66.37602949074518\n",
      "Cost2784 = 66.37618840277156\n",
      "Cost2785 = 66.37710917782405\n",
      "Cost2786 = 66.3780147949249\n",
      "Cost2787 = 66.37121335994293\n",
      "Cost2788 = 66.37533236783445\n",
      "Cost2789 = 66.37405912101045\n",
      "Cost2790 = 66.37106492301955\n",
      "Cost2791 = 66.37614692499803\n",
      "Cost2792 = 66.3741426583405\n",
      "Cost2793 = 66.3764776173272\n",
      "Cost2794 = 66.37506269202974\n",
      "Cost2795 = 66.3693189041272\n",
      "Cost2796 = 66.37599049474268\n",
      "Cost2797 = 66.36864682116705\n",
      "Cost2798 = 66.37418881542887\n",
      "Cost2799 = 66.37366136955475\n",
      "Cost2800 = 66.36885255999648\n",
      "Cost2801 = 66.37096244712747\n",
      "Cost2802 = 66.36857702152805\n",
      "Cost2803 = 66.37212843463156\n",
      "Cost2804 = 66.36747224812233\n",
      "Cost2805 = 66.36661325657046\n",
      "Cost2806 = 66.3690692173071\n",
      "Cost2807 = 66.37229490739877\n",
      "Cost2808 = 66.37146321915911\n",
      "Cost2809 = 66.37049242512329\n",
      "Cost2810 = 66.36511854705171\n",
      "Cost2811 = 66.36354232477449\n",
      "Cost2812 = 66.37214140803312\n",
      "Cost2813 = 66.36264441080219\n",
      "Cost2814 = 66.36539575480613\n",
      "Cost2815 = 66.36633031560227\n",
      "Cost2816 = 66.36544297041912\n",
      "Cost2817 = 66.36684124120451\n",
      "Cost2818 = 66.36322597187011\n",
      "Cost2819 = 66.36631357622656\n",
      "Cost2820 = 66.36397291116938\n",
      "Cost2821 = 66.36860498739163\n",
      "Cost2822 = 66.36631924709576\n",
      "Cost2823 = 66.36482399434229\n",
      "Cost2824 = 66.36449235272705\n",
      "Cost2825 = 66.36406574388991\n",
      "Cost2826 = 66.3664766597576\n",
      "Cost2827 = 66.36584129591961\n",
      "Cost2828 = 66.36491130152814\n",
      "Cost2829 = 66.36107744453345\n",
      "Cost2830 = 66.36672372032625\n",
      "Cost2831 = 66.36320168478265\n",
      "Cost2832 = 66.36091877545037\n",
      "Cost2833 = 66.36510187572553\n",
      "Cost2834 = 66.36275385627222\n",
      "Cost2835 = 66.36461092982059\n",
      "Cost2836 = 66.36087645081464\n",
      "Cost2837 = 66.35991060809522\n",
      "Cost2838 = 66.35733257898605\n",
      "Cost2839 = 66.3588514584027\n",
      "Cost2840 = 66.35240973559218\n",
      "Cost2841 = 66.35416155154816\n",
      "Cost2842 = 66.35516165127602\n",
      "Cost2843 = 66.35289039992841\n",
      "Cost2844 = 66.35869744239615\n",
      "Cost2845 = 66.3587986655675\n",
      "Cost2846 = 66.35870021795846\n",
      "Cost2847 = 66.35726854666903\n",
      "Cost2848 = 66.35522611327156\n",
      "Cost2849 = 66.3570975133644\n",
      "Cost2850 = 66.35423423398134\n",
      "Cost2851 = 66.35473816151799\n",
      "Cost2852 = 66.35301203071286\n",
      "Cost2853 = 66.35535783432644\n",
      "Cost2854 = 66.35887009850158\n",
      "Cost2855 = 66.35185383621503\n",
      "Cost2856 = 66.35674639379245\n",
      "Cost2857 = 66.34853967547777\n",
      "Cost2858 = 66.35680058048565\n",
      "Cost2859 = 66.35070728335664\n",
      "Cost2860 = 66.35332555529693\n",
      "Cost2861 = 66.35044399778666\n",
      "Cost2862 = 66.35492335769152\n",
      "Cost2863 = 66.34720124773617\n",
      "Cost2864 = 66.35397459838836\n",
      "Cost2865 = 66.35235052703565\n",
      "Cost2866 = 66.35109106222143\n",
      "Cost2867 = 66.34858751582502\n",
      "Cost2868 = 66.35206909763296\n",
      "Cost2869 = 66.35339945612256\n",
      "Cost2870 = 66.34947659030547\n",
      "Cost2871 = 66.35107479713749\n",
      "Cost2872 = 66.35384658895661\n",
      "Cost2873 = 66.34646246174871\n",
      "Cost2874 = 66.34611643199521\n",
      "Cost2875 = 66.34801706377858\n",
      "Cost2876 = 66.34911751907129\n",
      "Cost2877 = 66.34310461250742\n",
      "Cost2878 = 66.34796384966235\n",
      "Cost2879 = 66.35086455828295\n",
      "Cost2880 = 66.34590312452697\n",
      "Cost2881 = 66.34895564842166\n",
      "Cost2882 = 66.34038505317099\n",
      "Cost2883 = 66.34428543175173\n",
      "Cost2884 = 66.34621780904338\n",
      "Cost2885 = 66.34567861832942\n",
      "Cost2886 = 66.34225662398197\n",
      "Cost2887 = 66.34085383609346\n",
      "Cost2888 = 66.34605267156171\n",
      "Cost2889 = 66.33706997460907\n",
      "Cost2890 = 66.33825647500463\n",
      "Cost2891 = 66.33705969648972\n",
      "Cost2892 = 66.34091500406984\n",
      "Cost2893 = 66.3384441465276\n",
      "Cost2894 = 66.33917855723402\n",
      "Cost2895 = 66.34009480793544\n",
      "Cost2896 = 66.33934173038138\n",
      "Cost2897 = 66.3380982618873\n",
      "Cost2898 = 66.33860016830992\n",
      "Cost2899 = 66.33812413940235\n",
      "Cost2900 = 66.3352805432977\n",
      "Cost2901 = 66.33783941651805\n",
      "Cost2902 = 66.33876939435937\n",
      "Cost2903 = 66.33504963990843\n",
      "Cost2904 = 66.33276592337987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost2905 = 66.33294071186577\n",
      "Cost2906 = 66.33266181803715\n",
      "Cost2907 = 66.33512263108416\n",
      "Cost2908 = 66.33611547769367\n",
      "Cost2909 = 66.33523175076466\n",
      "Cost2910 = 66.331107635279\n",
      "Cost2911 = 66.33609862616979\n",
      "Cost2912 = 66.335262249036\n",
      "Cost2913 = 66.33847885976608\n",
      "Cost2914 = 66.33775979418672\n",
      "Cost2915 = 66.33972754546686\n",
      "Cost2916 = 66.33698443911021\n",
      "Cost2917 = 66.33158339619793\n",
      "Cost2918 = 66.33295134734081\n",
      "Cost2919 = 66.33228333824411\n",
      "Cost2920 = 66.32776311736\n",
      "Cost2921 = 66.33340624980488\n",
      "Cost2922 = 66.33413002314039\n",
      "Cost2923 = 66.32951193377748\n",
      "Cost2924 = 66.33051656200291\n",
      "Cost2925 = 66.32826801052951\n",
      "Cost2926 = 66.33400616240043\n",
      "Cost2927 = 66.32829288524229\n",
      "Cost2928 = 66.32774481540028\n",
      "Cost2929 = 66.32430627935611\n",
      "Cost2930 = 66.33236156157338\n",
      "Cost2931 = 66.32653522412002\n",
      "Cost2932 = 66.33000004968467\n",
      "Cost2933 = 66.32478174695665\n",
      "Cost2934 = 66.3258360251089\n",
      "Cost2935 = 66.3254681458603\n",
      "Cost2936 = 66.32806913423768\n",
      "Cost2937 = 66.32772225690559\n",
      "Cost2938 = 66.32590189612309\n",
      "Cost2939 = 66.32615767912075\n",
      "Cost2940 = 66.33283132014034\n",
      "Cost2941 = 66.31798749831718\n",
      "Cost2942 = 66.32747372632367\n",
      "Cost2943 = 66.32504145329804\n",
      "Cost2944 = 66.32087600829618\n",
      "Cost2945 = 66.3242060849878\n",
      "Cost2946 = 66.32685179156867\n",
      "Cost2947 = 66.32832573406613\n",
      "Cost2948 = 66.32710227071621\n",
      "Cost2949 = 66.32584114813865\n",
      "Cost2950 = 66.32461321019412\n",
      "Cost2951 = 66.32332923668422\n",
      "Cost2952 = 66.32704719256387\n",
      "Cost2953 = 66.3219298802287\n",
      "Cost2954 = 66.31998406660352\n",
      "Cost2955 = 66.32317350423783\n",
      "Cost2956 = 66.32363058217915\n",
      "Cost2957 = 66.32472108879719\n",
      "Cost2958 = 66.32230339978496\n",
      "Cost2959 = 66.32105433555807\n",
      "Cost2960 = 66.31566208641357\n",
      "Cost2961 = 66.32624185790036\n",
      "Cost2962 = 66.32414356589042\n",
      "Cost2963 = 66.32814276737051\n",
      "Cost2964 = 66.32013723356918\n",
      "Cost2965 = 66.32845411149515\n",
      "Cost2966 = 66.31987805305697\n",
      "Cost2967 = 66.31729901526504\n",
      "Cost2968 = 66.32239913088607\n",
      "Cost2969 = 66.32213349577938\n",
      "Cost2970 = 66.32066136765852\n",
      "Cost2971 = 66.31777001988723\n",
      "Cost2972 = 66.32028916727778\n",
      "Cost2973 = 66.31324099908774\n",
      "Cost2974 = 66.31746903492513\n",
      "Cost2975 = 66.31501442070592\n",
      "Cost2976 = 66.31270726631053\n",
      "Cost2977 = 66.3136406365594\n",
      "Cost2978 = 66.31361977759121\n",
      "Cost2979 = 66.31306406627627\n",
      "Cost2980 = 66.31571105199359\n",
      "Cost2981 = 66.31543046346232\n",
      "Cost2982 = 66.31118634556083\n",
      "Cost2983 = 66.31809501859384\n",
      "Cost2984 = 66.31653568832404\n",
      "Cost2985 = 66.31337835540855\n",
      "Cost2986 = 66.3127301332814\n",
      "Cost2987 = 66.31140689030919\n",
      "Cost2988 = 66.31644372823054\n",
      "Cost2989 = 66.30793231902611\n",
      "Cost2990 = 66.31069547112494\n",
      "Cost2991 = 66.30666177284117\n",
      "Cost2992 = 66.30864809823011\n",
      "Cost2993 = 66.30874399183129\n",
      "Cost2994 = 66.3104040925817\n",
      "Cost2995 = 66.31098195454615\n",
      "Cost2996 = 66.31459543598099\n",
      "Cost2997 = 66.30814557042956\n",
      "Cost2998 = 66.30758884445714\n",
      "Cost2999 = 66.30626687211264\n",
      "Cost3000 = 66.30592378593745\n",
      "Cost3001 = 66.30893371759201\n",
      "Cost3002 = 66.30412829540289\n",
      "Cost3003 = 66.3033383424838\n",
      "Cost3004 = 66.30546131278686\n",
      "Cost3005 = 66.30486327260805\n",
      "Cost3006 = 66.30059512076055\n",
      "Cost3007 = 66.30460070504428\n",
      "Cost3008 = 66.307443214066\n",
      "Cost3009 = 66.30274632869617\n",
      "Cost3010 = 66.30671011807542\n",
      "Cost3011 = 66.30815096629455\n",
      "Cost3012 = 66.30148565362958\n",
      "Cost3013 = 66.30516444596171\n",
      "Cost3014 = 66.31007323625813\n",
      "Cost3015 = 66.30382886762821\n",
      "Cost3016 = 66.30122170044875\n",
      "Cost3017 = 66.3095567308525\n",
      "Cost3018 = 66.30190609620023\n",
      "Cost3019 = 66.30110654746541\n",
      "Cost3020 = 66.30936201742776\n",
      "Cost3021 = 66.30284906081336\n",
      "Cost3022 = 66.307117018533\n",
      "Cost3023 = 66.3044474021174\n",
      "Cost3024 = 66.30279874976196\n",
      "Cost3025 = 66.30366262522357\n",
      "Cost3026 = 66.30350689801523\n",
      "Cost3027 = 66.29975788580222\n",
      "Cost3028 = 66.30743564286597\n",
      "Cost3029 = 66.30332198489627\n",
      "Cost3030 = 66.30000037091567\n",
      "Cost3031 = 66.30270567159262\n",
      "Cost3032 = 66.2963608494531\n",
      "Cost3033 = 66.299924861094\n",
      "Cost3034 = 66.30152897766763\n",
      "Cost3035 = 66.30274070748501\n",
      "Cost3036 = 66.30197360745403\n",
      "Cost3037 = 66.29545492657023\n",
      "Cost3038 = 66.29339080142219\n",
      "Cost3039 = 66.2929076857723\n",
      "Cost3040 = 66.29258532799865\n",
      "Cost3041 = 66.29199775717333\n",
      "Cost3042 = 66.29250830120341\n",
      "Cost3043 = 66.29342728588624\n",
      "Cost3044 = 66.28863917503558\n",
      "Cost3045 = 66.29330458452836\n",
      "Cost3046 = 66.29225368247707\n",
      "Cost3047 = 66.29856080906387\n",
      "Cost3048 = 66.29024924044637\n",
      "Cost3049 = 66.29395091723408\n",
      "Cost3050 = 66.29157887095528\n",
      "Cost3051 = 66.29552868440176\n",
      "Cost3052 = 66.29013219859142\n",
      "Cost3053 = 66.2893131401214\n",
      "Cost3054 = 66.29227925442139\n",
      "Cost3055 = 66.2881151208339\n",
      "Cost3056 = 66.28823153861237\n",
      "Cost3057 = 66.28948955849629\n",
      "Cost3058 = 66.28985433386303\n",
      "Cost3059 = 66.2864516336906\n",
      "Cost3060 = 66.28888312946754\n",
      "Cost3061 = 66.28991942989022\n",
      "Cost3062 = 66.29295681400625\n",
      "Cost3063 = 66.2869437223093\n",
      "Cost3064 = 66.28591640330734\n",
      "Cost3065 = 66.29003201030793\n",
      "Cost3066 = 66.28311615356809\n",
      "Cost3067 = 66.28342421478692\n",
      "Cost3068 = 66.28578816183895\n",
      "Cost3069 = 66.28669017312836\n",
      "Cost3070 = 66.2873048117911\n",
      "Cost3071 = 66.28331741906972\n",
      "Cost3072 = 66.28346229885238\n",
      "Cost3073 = 66.28306189780089\n",
      "Cost3074 = 66.28277691969616\n",
      "Cost3075 = 66.2827373521618\n",
      "Cost3076 = 66.28368201561696\n",
      "Cost3077 = 66.2803171164326\n",
      "Cost3078 = 66.28118314442175\n",
      "Cost3079 = 66.282689606742\n",
      "Cost3080 = 66.2779994859914\n",
      "Cost3081 = 66.27984842934943\n",
      "Cost3082 = 66.28265841139901\n",
      "Cost3083 = 66.28055400692516\n",
      "Cost3084 = 66.27938343642757\n",
      "Cost3085 = 66.27540606925601\n",
      "Cost3086 = 66.27295423707314\n",
      "Cost3087 = 66.27389643987532\n",
      "Cost3088 = 66.27451467279626\n",
      "Cost3089 = 66.27520878220437\n",
      "Cost3090 = 66.27377120631914\n",
      "Cost3091 = 66.27292254153164\n",
      "Cost3092 = 66.28151197282676\n",
      "Cost3093 = 66.27408073252407\n",
      "Cost3094 = 66.27424448755103\n",
      "Cost3095 = 66.27191651963584\n",
      "Cost3096 = 66.27119958032921\n",
      "Cost3097 = 66.27384716178605\n",
      "Cost3098 = 66.2721693689131\n",
      "Cost3099 = 66.27297782553671\n",
      "Cost3100 = 66.27045120898187\n",
      "Cost3101 = 66.26696565459885\n",
      "Cost3102 = 66.27335776084331\n",
      "Cost3103 = 66.26577134297963\n",
      "Cost3104 = 66.27005919738237\n",
      "Cost3105 = 66.27142528137064\n",
      "Cost3106 = 66.27382145864789\n",
      "Cost3107 = 66.26724065298832\n",
      "Cost3108 = 66.26798333289074\n",
      "Cost3109 = 66.27548559943432\n",
      "Cost3110 = 66.26632125502417\n",
      "Cost3111 = 66.26376640504813\n",
      "Cost3112 = 66.26531883382323\n",
      "Cost3113 = 66.26658820592264\n",
      "Cost3114 = 66.26704120421351\n",
      "Cost3115 = 66.26485269998395\n",
      "Cost3116 = 66.26701530159275\n",
      "Cost3117 = 66.2697489752713\n",
      "Cost3118 = 66.26905763263485\n",
      "Cost3119 = 66.26826982519628\n",
      "Cost3120 = 66.26629272917914\n",
      "Cost3121 = 66.26699531473653\n",
      "Cost3122 = 66.26455402239276\n",
      "Cost3123 = 66.2673177956355\n",
      "Cost3124 = 66.26599208151353\n",
      "Cost3125 = 66.26677290899882\n",
      "Cost3126 = 66.26036507438299\n",
      "Cost3127 = 66.26743018927358\n",
      "Cost3128 = 66.26121479710592\n",
      "Cost3129 = 66.2666038269555\n",
      "Cost3130 = 66.26578864358852\n",
      "Cost3131 = 66.26193495188906\n",
      "Cost3132 = 66.25844243122117\n",
      "Cost3133 = 66.26222883126924\n",
      "Cost3134 = 66.26548058347208\n",
      "Cost3135 = 66.25939435349153\n",
      "Cost3136 = 66.26016136471067\n",
      "Cost3137 = 66.26250292999744\n",
      "Cost3138 = 66.25731741704111\n",
      "Cost3139 = 66.26134359561206\n",
      "Cost3140 = 66.26098994242997\n",
      "Cost3141 = 66.2632358201001\n",
      "Cost3142 = 66.25965286247842\n",
      "Cost3143 = 66.26141235958093\n",
      "Cost3144 = 66.26235769915566\n",
      "Cost3145 = 66.26248513548856\n",
      "Cost3146 = 66.25777721475704\n",
      "Cost3147 = 66.25631007255606\n",
      "Cost3148 = 66.26004604407449\n",
      "Cost3149 = 66.25399513497311\n",
      "Cost3150 = 66.26005264060468\n",
      "Cost3151 = 66.25953201426847\n",
      "Cost3152 = 66.25715071121319\n",
      "Cost3153 = 66.25994069863145\n",
      "Cost3154 = 66.25457798373077\n",
      "Cost3155 = 66.2603358338375\n",
      "Cost3156 = 66.26000937407834\n",
      "Cost3157 = 66.25997583053046\n",
      "Cost3158 = 66.2567312347248\n",
      "Cost3159 = 66.25416284348184\n",
      "Cost3160 = 66.25487981290415\n",
      "Cost3161 = 66.25735289139585\n",
      "Cost3162 = 66.25604675172627\n",
      "Cost3163 = 66.26045547546076\n",
      "Cost3164 = 66.25223297043716\n",
      "Cost3165 = 66.25690349258856\n",
      "Cost3166 = 66.25904624310733\n",
      "Cost3167 = 66.2538722592901\n",
      "Cost3168 = 66.25185397169852\n",
      "Cost3169 = 66.25269737633504\n",
      "Cost3170 = 66.25395275809672\n",
      "Cost3171 = 66.25329471048812\n",
      "Cost3172 = 66.25085600592324\n",
      "Cost3173 = 66.25453035273236\n",
      "Cost3174 = 66.25625343505642\n",
      "Cost3175 = 66.25419434834546\n",
      "Cost3176 = 66.25032339026072\n",
      "Cost3177 = 66.2513796070136\n",
      "Cost3178 = 66.25340231348963\n",
      "Cost3179 = 66.25208840677439\n",
      "Cost3180 = 66.25363371389008\n",
      "Cost3181 = 66.25017213821617\n",
      "Cost3182 = 66.24898485360212\n",
      "Cost3183 = 66.25274019422727\n",
      "Cost3184 = 66.25045259593061\n",
      "Cost3185 = 66.24435583794882\n",
      "Cost3186 = 66.25189601812028\n",
      "Cost3187 = 66.24916155165296\n",
      "Cost3188 = 66.24809151588404\n",
      "Cost3189 = 66.2514863645008\n",
      "Cost3190 = 66.25300426756387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost3191 = 66.24489484770592\n",
      "Cost3192 = 66.24923459216011\n",
      "Cost3193 = 66.24969125827486\n",
      "Cost3194 = 66.24533795788292\n",
      "Cost3195 = 66.24457735931084\n",
      "Cost3196 = 66.24812495938096\n",
      "Cost3197 = 66.24904141299483\n",
      "Cost3198 = 66.24394677308426\n",
      "Cost3199 = 66.24485081754705\n",
      "Cost3200 = 66.24273202998697\n",
      "Cost3201 = 66.2447839939272\n",
      "Cost3202 = 66.24128685793318\n",
      "Cost3203 = 66.24797564247943\n",
      "Cost3204 = 66.2410732022196\n",
      "Cost3205 = 66.24546082078481\n",
      "Cost3206 = 66.24565346978298\n",
      "Cost3207 = 66.24662751419241\n",
      "Cost3208 = 66.24588968614158\n",
      "Cost3209 = 66.23724518707654\n",
      "Cost3210 = 66.24489837184677\n",
      "Cost3211 = 66.24588493844277\n",
      "Cost3212 = 66.23970575189742\n",
      "Cost3213 = 66.24226916415645\n",
      "Cost3214 = 66.24554593633594\n",
      "Cost3215 = 66.24122034519213\n",
      "Cost3216 = 66.24381104823075\n",
      "Cost3217 = 66.23659652033989\n",
      "Cost3218 = 66.24184742245339\n",
      "Cost3219 = 66.23915218588984\n",
      "Cost3220 = 66.24286479368544\n",
      "Cost3221 = 66.23654432324206\n",
      "Cost3222 = 66.23850647131916\n",
      "Cost3223 = 66.2411739168676\n",
      "Cost3224 = 66.24121778624766\n",
      "Cost3225 = 66.24118238233442\n",
      "Cost3226 = 66.23917990625446\n",
      "Cost3227 = 66.23400873416972\n",
      "Cost3228 = 66.23839131686367\n",
      "Cost3229 = 66.24186869856915\n",
      "Cost3230 = 66.23821433756584\n",
      "Cost3231 = 66.24003277738177\n",
      "Cost3232 = 66.23746572854824\n",
      "Cost3233 = 66.2384608185106\n",
      "Cost3234 = 66.23893720034759\n",
      "Cost3235 = 66.23792719686469\n",
      "Cost3236 = 66.23808626998203\n",
      "Cost3237 = 66.23926719894932\n",
      "Cost3238 = 66.23582118394528\n",
      "Cost3239 = 66.23828108335509\n",
      "Cost3240 = 66.23452500080194\n",
      "Cost3241 = 66.23737568162834\n",
      "Cost3242 = 66.23768681002329\n",
      "Cost3243 = 66.22890226722359\n",
      "Cost3244 = 66.2380724792718\n",
      "Cost3245 = 66.2355709746899\n",
      "Cost3246 = 66.23677660374042\n",
      "Cost3247 = 66.23161992911768\n",
      "Cost3248 = 66.2374454629251\n",
      "Cost3249 = 66.23136652794382\n",
      "Cost3250 = 66.23239243618055\n",
      "Cost3251 = 66.23216782480553\n",
      "Cost3252 = 66.22978623349005\n",
      "Cost3253 = 66.23364407207326\n",
      "Cost3254 = 66.22681131684841\n",
      "Cost3255 = 66.2288033078622\n",
      "Cost3256 = 66.22964647824921\n",
      "Cost3257 = 66.2340523396638\n",
      "Cost3258 = 66.22708319212536\n",
      "Cost3259 = 66.22900660814472\n",
      "Cost3260 = 66.22972704815945\n",
      "Cost3261 = 66.22589647768369\n",
      "Cost3262 = 66.22420273126129\n",
      "Cost3263 = 66.2217408656083\n",
      "Cost3264 = 66.22842669311753\n",
      "Cost3265 = 66.22536520044349\n",
      "Cost3266 = 66.2305853965687\n",
      "Cost3267 = 66.22760799027026\n",
      "Cost3268 = 66.22359609182705\n",
      "Cost3269 = 66.22673431657249\n",
      "Cost3270 = 66.22716023397938\n",
      "Cost3271 = 66.22225713667014\n",
      "Cost3272 = 66.21924039888516\n",
      "Cost3273 = 66.22237180702226\n",
      "Cost3274 = 66.22392108963474\n",
      "Cost3275 = 66.21782856692067\n",
      "Cost3276 = 66.2199095974946\n",
      "Cost3277 = 66.22170094237322\n",
      "Cost3278 = 66.22418183554971\n",
      "Cost3279 = 66.22180274759214\n",
      "Cost3280 = 66.22211643929738\n",
      "Cost3281 = 66.22262378507537\n",
      "Cost3282 = 66.22120160987765\n",
      "Cost3283 = 66.21883637850812\n",
      "Cost3284 = 66.21562098358399\n",
      "Cost3285 = 66.21325652990654\n",
      "Cost3286 = 66.2150560175439\n",
      "Cost3287 = 66.22205473999588\n",
      "Cost3288 = 66.21853687691677\n",
      "Cost3289 = 66.21261929522021\n",
      "Cost3290 = 66.21484949674837\n",
      "Cost3291 = 66.21559938702568\n",
      "Cost3292 = 66.2199749567323\n",
      "Cost3293 = 66.21586545816179\n",
      "Cost3294 = 66.2167150557547\n",
      "Cost3295 = 66.21969434187943\n",
      "Cost3296 = 66.21482439416819\n",
      "Cost3297 = 66.2158398139142\n",
      "Cost3298 = 66.22410075783061\n",
      "Cost3299 = 66.21894706907975\n",
      "Cost3300 = 66.22044602737242\n",
      "Cost3301 = 66.21726855728686\n",
      "Cost3302 = 66.21327908533385\n",
      "Cost3303 = 66.21296373674734\n",
      "Cost3304 = 66.21920737373239\n",
      "Cost3305 = 66.21632642394948\n",
      "Cost3306 = 66.21653998475759\n",
      "Cost3307 = 66.21540555598509\n",
      "Cost3308 = 66.21201820444314\n",
      "Cost3309 = 66.21262276984629\n",
      "Cost3310 = 66.21221080186525\n",
      "Cost3311 = 66.21346925277543\n",
      "Cost3312 = 66.2185946994853\n",
      "Cost3313 = 66.21331386960976\n",
      "Cost3314 = 66.20792674668294\n",
      "Cost3315 = 66.21499638346538\n",
      "Cost3316 = 66.21880971989559\n",
      "Cost3317 = 66.211321299468\n",
      "Cost3318 = 66.21193414445366\n",
      "Cost3319 = 66.21234409544006\n",
      "Cost3320 = 66.20943862165724\n",
      "Cost3321 = 66.21159248735661\n",
      "Cost3322 = 66.21637237645268\n",
      "Cost3323 = 66.2136257945742\n",
      "Cost3324 = 66.20959398597049\n",
      "Cost3325 = 66.21107016961531\n",
      "Cost3326 = 66.20767840893417\n",
      "Cost3327 = 66.20409660240261\n",
      "Cost3328 = 66.20733255022499\n",
      "Cost3329 = 66.20567445704461\n",
      "Cost3330 = 66.20732948771062\n",
      "Cost3331 = 66.20802477104242\n",
      "Cost3332 = 66.20717751500986\n",
      "Cost3333 = 66.21221919499376\n",
      "Cost3334 = 66.21089012835091\n",
      "Cost3335 = 66.20755387363337\n",
      "Cost3336 = 66.19784034783173\n",
      "Cost3337 = 66.20776965436995\n",
      "Cost3338 = 66.20733465087628\n",
      "Cost3339 = 66.21055373369497\n",
      "Cost3340 = 66.21038603909547\n",
      "Cost3341 = 66.20419251134327\n",
      "Cost3342 = 66.20274045087832\n",
      "Cost3343 = 66.20724529075365\n",
      "Cost3344 = 66.20077559083406\n",
      "Cost3345 = 66.2027277491509\n",
      "Cost3346 = 66.20638120669761\n",
      "Cost3347 = 66.20223700285652\n",
      "Cost3348 = 66.20208718565571\n",
      "Cost3349 = 66.19939557378609\n",
      "Cost3350 = 66.20343914703747\n",
      "Cost3351 = 66.19788745891388\n",
      "Cost3352 = 66.20279513399606\n",
      "Cost3353 = 66.20383656773654\n",
      "Cost3354 = 66.20268708689713\n",
      "Cost3355 = 66.20261787355403\n",
      "Cost3356 = 66.20420629498187\n",
      "Cost3357 = 66.19975099838747\n",
      "Cost3358 = 66.19750977126107\n",
      "Cost3359 = 66.2030123718437\n",
      "Cost3360 = 66.20179573882172\n",
      "Cost3361 = 66.2015566207141\n",
      "Cost3362 = 66.2015518825291\n",
      "Cost3363 = 66.20086954502862\n",
      "Cost3364 = 66.19835197493359\n",
      "Cost3365 = 66.20055355289875\n",
      "Cost3366 = 66.19874628011584\n",
      "Cost3367 = 66.19326639647171\n",
      "Cost3368 = 66.19766811364586\n",
      "Cost3369 = 66.19728792678389\n",
      "Cost3370 = 66.19828840389822\n",
      "Cost3371 = 66.19659915465506\n",
      "Cost3372 = 66.19437183845643\n",
      "Cost3373 = 66.19162366457229\n",
      "Cost3374 = 66.20023727409178\n",
      "Cost3375 = 66.1936221556509\n",
      "Cost3376 = 66.19237002073363\n",
      "Cost3377 = 66.19429708632374\n",
      "Cost3378 = 66.1922287955567\n",
      "Cost3379 = 66.19358051581911\n",
      "Cost3380 = 66.19355128213745\n",
      "Cost3381 = 66.19268514507415\n",
      "Cost3382 = 66.19151399962355\n",
      "Cost3383 = 66.19518006015649\n",
      "Cost3384 = 66.19875239382922\n",
      "Cost3385 = 66.19154283774591\n",
      "Cost3386 = 66.1899398340944\n",
      "Cost3387 = 66.19443005735185\n",
      "Cost3388 = 66.19347653528406\n",
      "Cost3389 = 66.19465648195343\n",
      "Cost3390 = 66.19206725044845\n",
      "Cost3391 = 66.18985507571306\n",
      "Cost3392 = 66.19233338701397\n",
      "Cost3393 = 66.19099762794109\n",
      "Cost3394 = 66.18972357259341\n",
      "Cost3395 = 66.1964279452818\n",
      "Cost3396 = 66.19430630287411\n",
      "Cost3397 = 66.19252710331986\n",
      "Cost3398 = 66.19355063172628\n",
      "Cost3399 = 66.19184045205394\n",
      "Cost3400 = 66.19117209702806\n",
      "Cost3401 = 66.19456672619773\n",
      "Cost3402 = 66.19498335233354\n",
      "Cost3403 = 66.1902404853298\n",
      "Cost3404 = 66.19398824211703\n",
      "Cost3405 = 66.19226897064078\n",
      "Cost3406 = 66.18904397655912\n",
      "Cost3407 = 66.18572159126138\n",
      "Cost3408 = 66.18722061631618\n",
      "Cost3409 = 66.18513803010255\n",
      "Cost3410 = 66.18556190876383\n",
      "Cost3411 = 66.18531896394971\n",
      "Cost3412 = 66.18674674078815\n",
      "Cost3413 = 66.18216725774838\n",
      "Cost3414 = 66.182100871686\n",
      "Cost3415 = 66.18360456107192\n",
      "Cost3416 = 66.1874439134871\n",
      "Cost3417 = 66.1831469989956\n",
      "Cost3418 = 66.18067186484524\n",
      "Cost3419 = 66.18497623945773\n",
      "Cost3420 = 66.1776344521869\n",
      "Cost3421 = 66.18739587232527\n",
      "Cost3422 = 66.18200293637058\n",
      "Cost3423 = 66.18641747959279\n",
      "Cost3424 = 66.17692799518382\n",
      "Cost3425 = 66.18215095211828\n",
      "Cost3426 = 66.17540328739291\n",
      "Cost3427 = 66.1792631607675\n",
      "Cost3428 = 66.17603523707983\n",
      "Cost3429 = 66.18087347995568\n",
      "Cost3430 = 66.17664775032992\n",
      "Cost3431 = 66.17977799418199\n",
      "Cost3432 = 66.17649464486684\n",
      "Cost3433 = 66.1744416010304\n",
      "Cost3434 = 66.17288452249659\n",
      "Cost3435 = 66.18011510209861\n",
      "Cost3436 = 66.17128017642301\n",
      "Cost3437 = 66.17162108764018\n",
      "Cost3438 = 66.17676386953407\n",
      "Cost3439 = 66.17222534170402\n",
      "Cost3440 = 66.17018933994297\n",
      "Cost3441 = 66.17035378283526\n",
      "Cost3442 = 66.17376095958618\n",
      "Cost3443 = 66.1682316567588\n",
      "Cost3444 = 66.16885457004697\n",
      "Cost3445 = 66.16743965793442\n",
      "Cost3446 = 66.17036979819095\n",
      "Cost3447 = 66.16944775622476\n",
      "Cost3448 = 66.16860856799565\n",
      "Cost3449 = 66.17117952540816\n",
      "Cost3450 = 66.16971003973768\n",
      "Cost3451 = 66.1603341084448\n",
      "Cost3452 = 66.16756395975645\n",
      "Cost3453 = 66.1646320985495\n",
      "Cost3454 = 66.16490624304211\n",
      "Cost3455 = 66.15983968671198\n",
      "Cost3456 = 66.16942491743272\n",
      "Cost3457 = 66.16261971978965\n",
      "Cost3458 = 66.16047419709149\n",
      "Cost3459 = 66.1649467826907\n",
      "Cost3460 = 66.16181198551314\n",
      "Cost3461 = 66.16369849649158\n",
      "Cost3462 = 66.16587190997693\n",
      "Cost3463 = 66.15742631936556\n",
      "Cost3464 = 66.15728047756892\n",
      "Cost3465 = 66.1642767362665\n",
      "Cost3466 = 66.16191108487493\n",
      "Cost3467 = 66.16091005304888\n",
      "Cost3468 = 66.1579996747598\n",
      "Cost3469 = 66.15900115207897\n",
      "Cost3470 = 66.15623212533768\n",
      "Cost3471 = 66.15973560714303\n",
      "Cost3472 = 66.15964571746466\n",
      "Cost3473 = 66.1568359008247\n",
      "Cost3474 = 66.1585672377422\n",
      "Cost3475 = 66.15123197939631\n",
      "Cost3476 = 66.15069820833048\n",
      "Cost3477 = 66.15648511812407\n",
      "Cost3478 = 66.15153059013417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost3479 = 66.1534271640109\n",
      "Cost3480 = 66.1549773244551\n",
      "Cost3481 = 66.15461595099794\n",
      "Cost3482 = 66.14944997817693\n",
      "Cost3483 = 66.15411549847663\n",
      "Cost3484 = 66.15305187712988\n",
      "Cost3485 = 66.15047148828435\n",
      "Cost3486 = 66.15134303541318\n",
      "Cost3487 = 66.1514759217663\n",
      "Cost3488 = 66.15213142932551\n",
      "Cost3489 = 66.15530801972723\n",
      "Cost3490 = 66.14664224861231\n",
      "Cost3491 = 66.15142979725512\n",
      "Cost3492 = 66.14719836955325\n",
      "Cost3493 = 66.14465195898151\n",
      "Cost3494 = 66.14856612329233\n",
      "Cost3495 = 66.14591857280621\n",
      "Cost3496 = 66.14313366827145\n",
      "Cost3497 = 66.14810403249254\n",
      "Cost3498 = 66.1451647588315\n",
      "Cost3499 = 66.14417290756448\n",
      "Cost3500 = 66.14118004760398\n",
      "Cost3501 = 66.1406158242489\n",
      "Cost3502 = 66.1431137843682\n",
      "Cost3503 = 66.1447336378222\n",
      "Cost3504 = 66.13914468161067\n",
      "Cost3505 = 66.14331560940515\n",
      "Cost3506 = 66.1416597993464\n",
      "Cost3507 = 66.14052189709783\n",
      "Cost3508 = 66.13616184671147\n",
      "Cost3509 = 66.13590520711351\n",
      "Cost3510 = 66.1381130763542\n",
      "Cost3511 = 66.13725106925499\n",
      "Cost3512 = 66.13320313449546\n",
      "Cost3513 = 66.13523806540205\n",
      "Cost3514 = 66.1377869760406\n",
      "Cost3515 = 66.13080540913484\n",
      "Cost3516 = 66.13573592585566\n",
      "Cost3517 = 66.13598380993804\n",
      "Cost3518 = 66.13504091559781\n",
      "Cost3519 = 66.13455251199227\n",
      "Cost3520 = 66.13421730599028\n",
      "Cost3521 = 66.1357651954005\n",
      "Cost3522 = 66.13924672532048\n",
      "Cost3523 = 66.12546053328124\n",
      "Cost3524 = 66.13439642455427\n",
      "Cost3525 = 66.13138374148606\n",
      "Cost3526 = 66.13094759527901\n",
      "Cost3527 = 66.13204157169537\n",
      "Cost3528 = 66.13306322435888\n",
      "Cost3529 = 66.12899267081042\n",
      "Cost3530 = 66.12812177667587\n",
      "Cost3531 = 66.12605668259218\n",
      "Cost3532 = 66.13165998912535\n",
      "Cost3533 = 66.12686919330372\n",
      "Cost3534 = 66.13563179164046\n",
      "Cost3535 = 66.1264701491943\n",
      "Cost3536 = 66.12251850082419\n",
      "Cost3537 = 66.12736596259862\n",
      "Cost3538 = 66.12814966537643\n",
      "Cost3539 = 66.13189757479543\n",
      "Cost3540 = 66.12454036088648\n",
      "Cost3541 = 66.12259029179904\n",
      "Cost3542 = 66.1257869129722\n",
      "Cost3543 = 66.12125512146692\n",
      "Cost3544 = 66.1230254617196\n",
      "Cost3545 = 66.12119980015086\n",
      "Cost3546 = 66.11827211401828\n",
      "Cost3547 = 66.12025315738882\n",
      "Cost3548 = 66.12242456888515\n",
      "Cost3549 = 66.11831879818519\n",
      "Cost3550 = 66.1163544581967\n",
      "Cost3551 = 66.12066128113365\n",
      "Cost3552 = 66.11808346826825\n",
      "Cost3553 = 66.11746526380122\n",
      "Cost3554 = 66.1217832102083\n",
      "Cost3555 = 66.11382145268163\n",
      "Cost3556 = 66.11715082998022\n",
      "Cost3557 = 66.11453242382584\n",
      "Cost3558 = 66.11388992760101\n",
      "Cost3559 = 66.11625141702496\n",
      "Cost3560 = 66.11937463033931\n",
      "Cost3561 = 66.11179704338792\n",
      "Cost3562 = 66.11113862518184\n",
      "Cost3563 = 66.11250432015834\n",
      "Cost3564 = 66.11572450275823\n",
      "Cost3565 = 66.10877393894363\n",
      "Cost3566 = 66.11098967023362\n",
      "Cost3567 = 66.11615127258705\n",
      "Cost3568 = 66.11154202184848\n",
      "Cost3569 = 66.1180537148342\n",
      "Cost3570 = 66.11105962838097\n",
      "Cost3571 = 66.11180824786055\n",
      "Cost3572 = 66.10824786108294\n",
      "Cost3573 = 66.10844260741709\n",
      "Cost3574 = 66.10524981723081\n",
      "Cost3575 = 66.11250514044859\n",
      "Cost3576 = 66.10862013598651\n",
      "Cost3577 = 66.1078053986195\n",
      "Cost3578 = 66.1045475046449\n",
      "Cost3579 = 66.10579259893659\n",
      "Cost3580 = 66.1095090499768\n",
      "Cost3581 = 66.10930300828102\n",
      "Cost3582 = 66.10452367861912\n",
      "Cost3583 = 66.10114834317446\n",
      "Cost3584 = 66.10501037255015\n",
      "Cost3585 = 66.10578191857586\n",
      "Cost3586 = 66.10694101857005\n",
      "Cost3587 = 66.10460083158938\n",
      "Cost3588 = 66.10223829163967\n",
      "Cost3589 = 66.09939787197882\n",
      "Cost3590 = 66.10169765008902\n",
      "Cost3591 = 66.10335579984668\n",
      "Cost3592 = 66.10220966274636\n",
      "Cost3593 = 66.1005058722557\n",
      "Cost3594 = 66.10042150563277\n",
      "Cost3595 = 66.09772989283677\n",
      "Cost3596 = 66.0979177024326\n",
      "Cost3597 = 66.09510686624559\n",
      "Cost3598 = 66.1011070600272\n",
      "Cost3599 = 66.09292315113431\n",
      "Cost3600 = 66.0945882908243\n",
      "Cost3601 = 66.10193781380117\n",
      "Cost3602 = 66.09760716484112\n",
      "Cost3603 = 66.09739434882748\n",
      "Cost3604 = 66.0948012968504\n",
      "Cost3605 = 66.0964909787426\n",
      "Cost3606 = 66.09019105071206\n",
      "Cost3607 = 66.09082584242792\n",
      "Cost3608 = 66.09357932029518\n",
      "Cost3609 = 66.0946730054612\n",
      "Cost3610 = 66.09110318500714\n",
      "Cost3611 = 66.09404261790155\n",
      "Cost3612 = 66.09306528638112\n",
      "Cost3613 = 66.09242459422086\n",
      "Cost3614 = 66.09021376315613\n",
      "Cost3615 = 66.09372329052488\n",
      "Cost3616 = 66.08472613219008\n",
      "Cost3617 = 66.08872043114198\n",
      "Cost3618 = 66.08599299261824\n",
      "Cost3619 = 66.08511354005103\n",
      "Cost3620 = 66.08840682122637\n",
      "Cost3621 = 66.09008224329592\n",
      "Cost3622 = 66.08350184457626\n",
      "Cost3623 = 66.08317121078431\n",
      "Cost3624 = 66.0819278674123\n",
      "Cost3625 = 66.08443195521834\n",
      "Cost3626 = 66.08555839064361\n",
      "Cost3627 = 66.08183812603605\n",
      "Cost3628 = 66.08245733387055\n",
      "Cost3629 = 66.08941840825254\n",
      "Cost3630 = 66.07781229303568\n",
      "Cost3631 = 66.08527661708007\n",
      "Cost3632 = 66.08388585552602\n",
      "Cost3633 = 66.08200442290766\n",
      "Cost3634 = 66.08488292340682\n",
      "Cost3635 = 66.07962657954015\n",
      "Cost3636 = 66.08215208486354\n",
      "Cost3637 = 66.07932199541767\n",
      "Cost3638 = 66.08207882741802\n",
      "Cost3639 = 66.07739697386639\n",
      "Cost3640 = 66.0780802152622\n",
      "Cost3641 = 66.07728477382435\n",
      "Cost3642 = 66.07633042339349\n",
      "Cost3643 = 66.07438698987555\n",
      "Cost3644 = 66.07382352349488\n",
      "Cost3645 = 66.07895803871989\n",
      "Cost3646 = 66.07494774482652\n",
      "Cost3647 = 66.07320310984437\n",
      "Cost3648 = 66.07650029842735\n",
      "Cost3649 = 66.0760853356321\n",
      "Cost3650 = 66.07731542767276\n",
      "Cost3651 = 66.07057781408263\n",
      "Cost3652 = 66.07366265123935\n",
      "Cost3653 = 66.07133350059905\n",
      "Cost3654 = 66.07224104487995\n",
      "Cost3655 = 66.06982169141409\n",
      "Cost3656 = 66.07476223009593\n",
      "Cost3657 = 66.070738767451\n",
      "Cost3658 = 66.07212732982043\n",
      "Cost3659 = 66.06782327504467\n",
      "Cost3660 = 66.07094170235811\n",
      "Cost3661 = 66.06719221836866\n",
      "Cost3662 = 66.0726211502927\n",
      "Cost3663 = 66.06785289701905\n",
      "Cost3664 = 66.06637986209002\n",
      "Cost3665 = 66.06749118496646\n",
      "Cost3666 = 66.06749339262727\n",
      "Cost3667 = 66.06941836550531\n",
      "Cost3668 = 66.06824647673096\n",
      "Cost3669 = 66.06341998347278\n",
      "Cost3670 = 66.06689332828624\n",
      "Cost3671 = 66.06462322792932\n",
      "Cost3672 = 66.06021271697628\n",
      "Cost3673 = 66.0621550681865\n",
      "Cost3674 = 66.06215319208704\n",
      "Cost3675 = 66.06180658045291\n",
      "Cost3676 = 66.0637234892305\n",
      "Cost3677 = 66.06282004008267\n",
      "Cost3678 = 66.05737372478066\n",
      "Cost3679 = 66.05724121110808\n",
      "Cost3680 = 66.06212801837795\n",
      "Cost3681 = 66.059197589522\n",
      "Cost3682 = 66.05890890717234\n",
      "Cost3683 = 66.05787297307674\n",
      "Cost3684 = 66.05625334981417\n",
      "Cost3685 = 66.05626722288189\n",
      "Cost3686 = 66.05390550650296\n",
      "Cost3687 = 66.05624928031939\n",
      "Cost3688 = 66.05338007141543\n",
      "Cost3689 = 66.05792668499907\n",
      "Cost3690 = 66.05595985635462\n",
      "Cost3691 = 66.05688963553644\n",
      "Cost3692 = 66.05537239274412\n",
      "Cost3693 = 66.05202112221824\n",
      "Cost3694 = 66.05378618417002\n",
      "Cost3695 = 66.05491000851983\n",
      "Cost3696 = 66.05016469173557\n",
      "Cost3697 = 66.05519299546795\n",
      "Cost3698 = 66.04746476652117\n",
      "Cost3699 = 66.05647541669815\n",
      "Cost3700 = 66.04942430600879\n",
      "Cost3701 = 66.05615376026783\n",
      "Cost3702 = 66.04694120701738\n",
      "Cost3703 = 66.05107092436171\n",
      "Cost3704 = 66.05223668934347\n",
      "Cost3705 = 66.05491174371718\n",
      "Cost3706 = 66.04713586622216\n",
      "Cost3707 = 66.04461295807302\n",
      "Cost3708 = 66.04690352207393\n",
      "Cost3709 = 66.0495264731739\n",
      "Cost3710 = 66.05071238608886\n",
      "Cost3711 = 66.04156442211097\n",
      "Cost3712 = 66.04805698182379\n",
      "Cost3713 = 66.04749662671286\n",
      "Cost3714 = 66.04736278689087\n",
      "Cost3715 = 66.04983061906475\n",
      "Cost3716 = 66.05056013505722\n",
      "Cost3717 = 66.04996934505682\n",
      "Cost3718 = 66.04210081338279\n",
      "Cost3719 = 66.04081155881268\n",
      "Cost3720 = 66.04345006024425\n",
      "Cost3721 = 66.04352479741364\n",
      "Cost3722 = 66.04550921955699\n",
      "Cost3723 = 66.0458669498842\n",
      "Cost3724 = 66.04923322060841\n",
      "Cost3725 = 66.04895378174804\n",
      "Cost3726 = 66.04032239630529\n",
      "Cost3727 = 66.04243924749848\n",
      "Cost3728 = 66.04435590785258\n",
      "Cost3729 = 66.04325844719881\n",
      "Cost3730 = 66.04394998540472\n",
      "Cost3731 = 66.04413736058451\n",
      "Cost3732 = 66.04151930454731\n",
      "Cost3733 = 66.0453594915644\n",
      "Cost3734 = 66.03675804737956\n",
      "Cost3735 = 66.0406288597138\n",
      "Cost3736 = 66.04501725391039\n",
      "Cost3737 = 66.03624415762273\n",
      "Cost3738 = 66.04218431817762\n",
      "Cost3739 = 66.04502152746245\n",
      "Cost3740 = 66.03799196710857\n",
      "Cost3741 = 66.04132921522454\n",
      "Cost3742 = 66.04076404835833\n",
      "Cost3743 = 66.03961430442095\n",
      "Cost3744 = 66.03673721374948\n",
      "Cost3745 = 66.03765647176456\n",
      "Cost3746 = 66.03379364607142\n",
      "Cost3747 = 66.03920224428771\n",
      "Cost3748 = 66.03930784328668\n",
      "Cost3749 = 66.03777682588884\n",
      "Cost3750 = 66.03500541050077\n",
      "Cost3751 = 66.0380795780509\n",
      "Cost3752 = 66.0414720779815\n",
      "Cost3753 = 66.03364111244781\n",
      "Cost3754 = 66.03261651287505\n",
      "Cost3755 = 66.03407750619651\n",
      "Cost3756 = 66.03615357378466\n",
      "Cost3757 = 66.03594153231697\n",
      "Cost3758 = 66.03354488557437\n",
      "Cost3759 = 66.04031537758465\n",
      "Cost3760 = 66.03541838884183\n",
      "Cost3761 = 66.0295877139551\n",
      "Cost3762 = 66.02903858157869\n",
      "Cost3763 = 66.03073520267583\n",
      "Cost3764 = 66.02847447262523\n",
      "Cost3765 = 66.0278748206163\n",
      "Cost3766 = 66.03415259544052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost3767 = 66.0310260974235\n",
      "Cost3768 = 66.0311647958997\n",
      "Cost3769 = 66.03150972906947\n",
      "Cost3770 = 66.03020067092585\n",
      "Cost3771 = 66.02677984225679\n",
      "Cost3772 = 66.03455993575403\n",
      "Cost3773 = 66.0220359335703\n",
      "Cost3774 = 66.02933633020626\n",
      "Cost3775 = 66.02775051515637\n",
      "Cost3776 = 66.02664536176701\n",
      "Cost3777 = 66.02981835322247\n",
      "Cost3778 = 66.02959233265264\n",
      "Cost3779 = 66.02853377092217\n",
      "Cost3780 = 66.02892048644931\n",
      "Cost3781 = 66.02887169976029\n",
      "Cost3782 = 66.02761981866959\n",
      "Cost3783 = 66.03019586077072\n",
      "Cost3784 = 66.01962889932929\n",
      "Cost3785 = 66.02230373826778\n",
      "Cost3786 = 66.02401749039487\n",
      "Cost3787 = 66.02259516958078\n",
      "Cost3788 = 66.02193324415823\n",
      "Cost3789 = 66.02620468321798\n",
      "Cost3790 = 66.01823551921886\n",
      "Cost3791 = 66.02099315286478\n",
      "Cost3792 = 66.02100378055377\n",
      "Cost3793 = 66.0242003921056\n",
      "Cost3794 = 66.02510451487518\n",
      "Cost3795 = 66.02275207273917\n",
      "Cost3796 = 66.01656342014995\n",
      "Cost3797 = 66.01633644344078\n",
      "Cost3798 = 66.02181100232468\n",
      "Cost3799 = 66.0176697450264\n",
      "Cost3800 = 66.01209920422714\n",
      "Cost3801 = 66.02099603875384\n",
      "Cost3802 = 66.01533187962805\n",
      "Cost3803 = 66.01496091855438\n",
      "Cost3804 = 66.01883729938223\n",
      "Cost3805 = 66.018975355368\n",
      "Cost3806 = 66.01373370372917\n",
      "Cost3807 = 66.01547297387985\n",
      "Cost3808 = 66.0164486900621\n",
      "Cost3809 = 66.01835622576154\n",
      "Cost3810 = 66.01771628525077\n",
      "Cost3811 = 66.02220806705844\n",
      "Cost3812 = 66.01067294370168\n",
      "Cost3813 = 66.01324788234194\n",
      "Cost3814 = 66.00980724880095\n",
      "Cost3815 = 66.01221235648416\n",
      "Cost3816 = 66.01528209629147\n",
      "Cost3817 = 66.00847670947601\n",
      "Cost3818 = 66.01399443151597\n",
      "Cost3819 = 66.01704313834784\n",
      "Cost3820 = 66.01154354616463\n",
      "Cost3821 = 66.01244621171296\n",
      "Cost3822 = 66.00678175836724\n",
      "Cost3823 = 66.00820358473207\n",
      "Cost3824 = 66.01424406504651\n",
      "Cost3825 = 66.01144397872041\n",
      "Cost3826 = 66.00918480535552\n",
      "Cost3827 = 66.0119244631057\n",
      "Cost3828 = 66.00489143501599\n",
      "Cost3829 = 66.00862829857454\n",
      "Cost3830 = 66.00693625454745\n",
      "Cost3831 = 66.0077202147115\n",
      "Cost3832 = 66.0069335430502\n",
      "Cost3833 = 66.00493796330615\n",
      "Cost3834 = 66.00924597947805\n",
      "Cost3835 = 66.00565351538604\n",
      "Cost3836 = 66.00864650570519\n",
      "Cost3837 = 66.00684240738457\n",
      "Cost3838 = 66.00432930442284\n",
      "Cost3839 = 66.00397719097474\n",
      "Cost3840 = 66.00407722366502\n",
      "Cost3841 = 66.0051179308769\n",
      "Cost3842 = 66.00627447499153\n",
      "Cost3843 = 66.00330090102095\n",
      "Cost3844 = 66.00129081817974\n",
      "Cost3845 = 65.99620519012053\n",
      "Cost3846 = 66.00314291185036\n",
      "Cost3847 = 66.00031938881203\n",
      "Cost3848 = 66.00032357339401\n",
      "Cost3849 = 65.99948631958473\n",
      "Cost3850 = 66.00350599166481\n",
      "Cost3851 = 66.00236139305753\n",
      "Cost3852 = 66.00027956564739\n",
      "Cost3853 = 66.00208219235836\n",
      "Cost3854 = 66.00079736402762\n",
      "Cost3855 = 66.00273589841757\n",
      "Cost3856 = 65.99498602111088\n",
      "Cost3857 = 65.99808719192399\n",
      "Cost3858 = 65.9955287122872\n",
      "Cost3859 = 65.99276378505374\n",
      "Cost3860 = 65.99302363593203\n",
      "Cost3861 = 65.99141631589237\n",
      "Cost3862 = 65.99590936297626\n",
      "Cost3863 = 66.00236258613262\n",
      "Cost3864 = 65.99236161377539\n",
      "Cost3865 = 65.99782435488554\n",
      "Cost3866 = 65.99915389007113\n",
      "Cost3867 = 65.99668963307845\n",
      "Cost3868 = 65.9850783991475\n",
      "Cost3869 = 65.98843023140951\n",
      "Cost3870 = 65.99118031308072\n",
      "Cost3871 = 65.99415967043777\n",
      "Cost3872 = 65.99658330903885\n",
      "Cost3873 = 65.99036627204013\n",
      "Cost3874 = 65.99200559479426\n",
      "Cost3875 = 65.98615312521697\n",
      "Cost3876 = 65.99218990737774\n",
      "Cost3877 = 65.99306024816462\n",
      "Cost3878 = 65.98714871720375\n",
      "Cost3879 = 65.98865230624665\n",
      "Cost3880 = 65.9857351669772\n",
      "Cost3881 = 65.98823222289731\n",
      "Cost3882 = 65.99038764714659\n",
      "Cost3883 = 65.98650606999905\n",
      "Cost3884 = 65.98772296523961\n",
      "Cost3885 = 65.98782246460256\n",
      "Cost3886 = 65.98680289266815\n",
      "Cost3887 = 65.98793840204203\n",
      "Cost3888 = 65.98505765935066\n",
      "Cost3889 = 65.97994501210017\n",
      "Cost3890 = 65.98401227489921\n",
      "Cost3891 = 65.98740874470731\n",
      "Cost3892 = 65.98415319673934\n",
      "Cost3893 = 65.98555654358987\n",
      "Cost3894 = 65.98400461811455\n",
      "Cost3895 = 65.9861815267483\n",
      "Cost3896 = 65.98719702289026\n",
      "Cost3897 = 65.98281633817597\n",
      "Cost3898 = 65.97946054032684\n",
      "Cost3899 = 65.98186512733562\n",
      "Cost3900 = 65.98164796410728\n",
      "Cost3901 = 65.98063911310317\n",
      "Cost3902 = 65.97919126499694\n",
      "Cost3903 = 65.97734080150214\n",
      "Cost3904 = 65.98036320353596\n",
      "Cost3905 = 65.97946329978433\n",
      "Cost3906 = 65.98203191158154\n",
      "Cost3907 = 65.97526785980955\n",
      "Cost3908 = 65.9778621631267\n",
      "Cost3909 = 65.98055289707193\n",
      "Cost3910 = 65.97758552891476\n",
      "Cost3911 = 65.9774948253054\n",
      "Cost3912 = 65.98045716242608\n",
      "Cost3913 = 65.9797749924112\n",
      "Cost3914 = 65.97633368654574\n",
      "Cost3915 = 65.98016296901983\n",
      "Cost3916 = 65.97251204925296\n",
      "Cost3917 = 65.9783238137284\n",
      "Cost3918 = 65.9773007367128\n",
      "Cost3919 = 65.98286072216858\n",
      "Cost3920 = 65.98000425024878\n",
      "Cost3921 = 65.96928560553854\n",
      "Cost3922 = 65.97521799309804\n",
      "Cost3923 = 65.97197571808915\n",
      "Cost3924 = 65.9761805950456\n",
      "Cost3925 = 65.97157167366784\n",
      "Cost3926 = 65.97238413234258\n",
      "Cost3927 = 65.97210781888315\n",
      "Cost3928 = 65.97361503896644\n",
      "Cost3929 = 65.97245675022448\n",
      "Cost3930 = 65.97072725972934\n",
      "Cost3931 = 65.97191824706708\n",
      "Cost3932 = 65.96770812967755\n",
      "Cost3933 = 65.97026706641704\n",
      "Cost3934 = 65.9778815248361\n",
      "Cost3935 = 65.97045280238292\n",
      "Cost3936 = 65.97403145391988\n",
      "Cost3937 = 65.96984432280557\n",
      "Cost3938 = 65.97425766502406\n",
      "Cost3939 = 65.97372083962682\n",
      "Cost3940 = 65.96824377765165\n",
      "Cost3941 = 65.97045802934137\n",
      "Cost3942 = 65.9707919275398\n",
      "Cost3943 = 65.96871819068802\n",
      "Cost3944 = 65.96852978789207\n",
      "Cost3945 = 65.96875456900487\n",
      "Cost3946 = 65.96835372201848\n",
      "Cost3947 = 65.96320527225267\n",
      "Cost3948 = 65.9658139002894\n",
      "Cost3949 = 65.9682869726223\n",
      "Cost3950 = 65.9707767555829\n",
      "Cost3951 = 65.96683421651186\n",
      "Cost3952 = 65.96471184318204\n",
      "Cost3953 = 65.9644690228595\n",
      "Cost3954 = 65.9676159030212\n",
      "Cost3955 = 65.96936425784253\n",
      "Cost3956 = 65.97107284110797\n",
      "Cost3957 = 65.96679415205543\n",
      "Cost3958 = 65.9610253121818\n",
      "Cost3959 = 65.95731871769345\n",
      "Cost3960 = 65.96734233016161\n",
      "Cost3961 = 65.95931010595858\n",
      "Cost3962 = 65.9665177154694\n",
      "Cost3963 = 65.96104117384438\n",
      "Cost3964 = 65.96281371774046\n",
      "Cost3965 = 65.96134024737582\n",
      "Cost3966 = 65.96207729842779\n",
      "Cost3967 = 65.96225352226864\n",
      "Cost3968 = 65.96199323126513\n",
      "Cost3969 = 65.9635057672295\n",
      "Cost3970 = 65.96654194974097\n",
      "Cost3971 = 65.96336905798009\n",
      "Cost3972 = 65.95856259342973\n",
      "Cost3973 = 65.96180614888975\n",
      "Cost3974 = 65.95863947510784\n",
      "Cost3975 = 65.95966593924214\n",
      "Cost3976 = 65.9579428313972\n",
      "Cost3977 = 65.95568263800155\n",
      "Cost3978 = 65.95742991429842\n",
      "Cost3979 = 65.95444049715061\n",
      "Cost3980 = 65.96238190067824\n",
      "Cost3981 = 65.9570738400845\n",
      "Cost3982 = 65.95045087221685\n",
      "Cost3983 = 65.95797993635496\n",
      "Cost3984 = 65.95127447110447\n",
      "Cost3985 = 65.95940929168916\n",
      "Cost3986 = 65.95800514895224\n",
      "Cost3987 = 65.95243059528691\n",
      "Cost3988 = 65.95397052832206\n",
      "Cost3989 = 65.95284647377463\n",
      "Cost3990 = 65.95386274830128\n",
      "Cost3991 = 65.95364795386661\n",
      "Cost3992 = 65.9564841099513\n",
      "Cost3993 = 65.95944715105382\n",
      "Cost3994 = 65.95261135590363\n",
      "Cost3995 = 65.95020800668607\n",
      "Cost3996 = 65.95271614053678\n",
      "Cost3997 = 65.95525646495314\n",
      "Cost3998 = 65.95192927620717\n",
      "Cost3999 = 65.9531854381491\n",
      "Cost4000 = 65.95050039301067\n",
      "Cost4001 = 65.95113576706363\n",
      "Cost4002 = 65.9529465559422\n",
      "Cost4003 = 65.94726963960298\n",
      "Cost4004 = 65.95458708204062\n",
      "Cost4005 = 65.95117934193557\n",
      "Cost4006 = 65.95548925384482\n",
      "Cost4007 = 65.95016436001902\n",
      "Cost4008 = 65.94894892136821\n",
      "Cost4009 = 65.94925956177613\n",
      "Cost4010 = 65.95193619784085\n",
      "Cost4011 = 65.94512444825527\n",
      "Cost4012 = 65.94629920055289\n",
      "Cost4013 = 65.94699912920134\n",
      "Cost4014 = 65.94763379811099\n",
      "Cost4015 = 65.94537638060656\n",
      "Cost4016 = 65.94880510643314\n",
      "Cost4017 = 65.94387563471608\n",
      "Cost4018 = 65.94932657932138\n",
      "Cost4019 = 65.94943471238315\n",
      "Cost4020 = 65.9455781410859\n",
      "Cost4021 = 65.94320304023644\n",
      "Cost4022 = 65.94838094159006\n",
      "Cost4023 = 65.93905090211351\n",
      "Cost4024 = 65.94203060262465\n",
      "Cost4025 = 65.9446196418261\n",
      "Cost4026 = 65.94544669867723\n",
      "Cost4027 = 65.94250449340285\n",
      "Cost4028 = 65.94284897816405\n",
      "Cost4029 = 65.94150183428985\n",
      "Cost4030 = 65.9442992840315\n",
      "Cost4031 = 65.94365197972601\n",
      "Cost4032 = 65.94008407461513\n",
      "Cost4033 = 65.94012091464879\n",
      "Cost4034 = 65.93920012348111\n",
      "Cost4035 = 65.93848159359786\n",
      "Cost4036 = 65.93798940542294\n",
      "Cost4037 = 65.93459049066345\n",
      "Cost4038 = 65.94256433414368\n",
      "Cost4039 = 65.94116286705179\n",
      "Cost4040 = 65.94051590504965\n",
      "Cost4041 = 65.94068257397312\n",
      "Cost4042 = 65.93892450480132\n",
      "Cost4043 = 65.93705919344698\n",
      "Cost4044 = 65.93856294253811\n",
      "Cost4045 = 65.93585963503266\n",
      "Cost4046 = 65.9403762588595\n",
      "Cost4047 = 65.93625566516155\n",
      "Cost4048 = 65.93988861056972\n",
      "Cost4049 = 65.9367854775215\n",
      "Cost4050 = 65.93454104379018\n",
      "Cost4051 = 65.9340554939067\n",
      "Cost4052 = 65.9372248023857\n",
      "Cost4053 = 65.93427489603523\n",
      "Cost4054 = 65.9349371657492\n",
      "Cost4055 = 65.93195879235054\n",
      "Cost4056 = 65.93150127106021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost4057 = 65.93452343224624\n",
      "Cost4058 = 65.93116353242495\n",
      "Cost4059 = 65.92938642479871\n",
      "Cost4060 = 65.92981072993788\n",
      "Cost4061 = 65.92928281394451\n",
      "Cost4062 = 65.93099998348785\n",
      "Cost4063 = 65.92896449532358\n",
      "Cost4064 = 65.92961567243296\n",
      "Cost4065 = 65.9327124125317\n",
      "Cost4066 = 65.92978151075422\n",
      "Cost4067 = 65.93193133454727\n",
      "Cost4068 = 65.93528226439328\n",
      "Cost4069 = 65.92724785183799\n",
      "Cost4070 = 65.92483618599218\n",
      "Cost4071 = 65.9259373369257\n",
      "Cost4072 = 65.93005445636338\n",
      "Cost4073 = 65.93149317829538\n",
      "Cost4074 = 65.92862822759419\n",
      "Cost4075 = 65.92651961375732\n",
      "Cost4076 = 65.92576584579065\n",
      "Cost4077 = 65.92673393580141\n",
      "Cost4078 = 65.92257936405545\n",
      "Cost4079 = 65.9309188127345\n",
      "Cost4080 = 65.92919660294291\n",
      "Cost4081 = 65.92782576190933\n",
      "Cost4082 = 65.9245157550611\n",
      "Cost4083 = 65.92405616899971\n",
      "Cost4084 = 65.92662060625618\n",
      "Cost4085 = 65.92556358879567\n",
      "Cost4086 = 65.92065527423465\n",
      "Cost4087 = 65.92420647352475\n",
      "Cost4088 = 65.92498392914811\n",
      "Cost4089 = 65.91932007837579\n",
      "Cost4090 = 65.92100911150558\n",
      "Cost4091 = 65.91798473851554\n",
      "Cost4092 = 65.92070772354754\n",
      "Cost4093 = 65.91999712407399\n",
      "Cost4094 = 65.91896089923085\n",
      "Cost4095 = 65.92572995218403\n",
      "Cost4096 = 65.92674040093776\n",
      "Cost4097 = 65.9270734585569\n",
      "Cost4098 = 65.91904336122674\n",
      "Cost4099 = 65.9220991187182\n",
      "Cost4100 = 65.92359603951365\n",
      "Cost4101 = 65.91953923964854\n",
      "Cost4102 = 65.92090476199479\n",
      "Cost4103 = 65.92727691995337\n",
      "Cost4104 = 65.9193669707963\n",
      "Cost4105 = 65.91525013660898\n",
      "Cost4106 = 65.91772970242862\n",
      "Cost4107 = 65.91888898132996\n",
      "Cost4108 = 65.91638221547446\n",
      "Cost4109 = 65.91848160281535\n",
      "Cost4110 = 65.91909087836761\n",
      "Cost4111 = 65.91574697922839\n",
      "Cost4112 = 65.9168623355195\n",
      "Cost4113 = 65.91345536773431\n",
      "Cost4114 = 65.92024306902975\n",
      "Cost4115 = 65.9116781005436\n",
      "Cost4116 = 65.91607072018135\n",
      "Cost4117 = 65.9150170980748\n",
      "Cost4118 = 65.9140931725618\n",
      "Cost4119 = 65.91873653614873\n",
      "Cost4120 = 65.91256116964787\n",
      "Cost4121 = 65.91058608572347\n",
      "Cost4122 = 65.9184038677723\n",
      "Cost4123 = 65.91593856156982\n",
      "Cost4124 = 65.9159162025231\n",
      "Cost4125 = 65.90909899324268\n",
      "Cost4126 = 65.90865879625542\n",
      "Cost4127 = 65.90532068612426\n",
      "Cost4128 = 65.91151059674687\n",
      "Cost4129 = 65.91038905887793\n",
      "Cost4130 = 65.91129304892249\n",
      "Cost4131 = 65.91290282869865\n",
      "Cost4132 = 65.90985148461678\n",
      "Cost4133 = 65.91487317749143\n",
      "Cost4134 = 65.91214004511303\n",
      "Cost4135 = 65.9067954768808\n",
      "Cost4136 = 65.9041328538225\n",
      "Cost4137 = 65.90804391721731\n",
      "Cost4138 = 65.91003632910768\n",
      "Cost4139 = 65.90687589004266\n",
      "Cost4140 = 65.9043438609322\n",
      "Cost4141 = 65.90704277878946\n",
      "Cost4142 = 65.91015893790345\n",
      "Cost4143 = 65.90629798703743\n",
      "Cost4144 = 65.90164224404813\n",
      "Cost4145 = 65.90580172114227\n",
      "Cost4146 = 65.89988616218322\n",
      "Cost4147 = 65.89907822441957\n",
      "Cost4148 = 65.90425879421082\n",
      "Cost4149 = 65.90497310468449\n",
      "Cost4150 = 65.89972672193896\n",
      "Cost4151 = 65.90027487048474\n",
      "Cost4152 = 65.90457603510303\n",
      "Cost4153 = 65.90189965735651\n",
      "Cost4154 = 65.89968078391487\n",
      "Cost4155 = 65.90294101428144\n",
      "Cost4156 = 65.89705491930039\n",
      "Cost4157 = 65.90267738091397\n",
      "Cost4158 = 65.9005662606285\n",
      "Cost4159 = 65.89766087573697\n",
      "Cost4160 = 65.89990698762998\n",
      "Cost4161 = 65.89428715277589\n",
      "Cost4162 = 65.89899119584379\n",
      "Cost4163 = 65.89824336501674\n",
      "Cost4164 = 65.89779718510557\n",
      "Cost4165 = 65.89804837769205\n",
      "Cost4166 = 65.8985061643232\n",
      "Cost4167 = 65.89245099659401\n",
      "Cost4168 = 65.8972908770473\n",
      "Cost4169 = 65.89188842635828\n",
      "Cost4170 = 65.89479663745422\n",
      "Cost4171 = 65.89421410923795\n",
      "Cost4172 = 65.89797703720593\n",
      "Cost4173 = 65.89557294993871\n",
      "Cost4174 = 65.88902333368104\n",
      "Cost4175 = 65.89542422975099\n",
      "Cost4176 = 65.89183759062873\n",
      "Cost4177 = 65.89668635401728\n",
      "Cost4178 = 65.8917383613955\n",
      "Cost4179 = 65.88823099978866\n",
      "Cost4180 = 65.88992302544067\n",
      "Cost4181 = 65.8937186077153\n",
      "Cost4182 = 65.89501927289504\n",
      "Cost4183 = 65.89250492350652\n",
      "Cost4184 = 65.89114359969722\n",
      "Cost4185 = 65.88478808103109\n",
      "Cost4186 = 65.88511145633144\n",
      "Cost4187 = 65.88337933562194\n",
      "Cost4188 = 65.88724462392568\n",
      "Cost4189 = 65.8877568831103\n",
      "Cost4190 = 65.88761565886105\n",
      "Cost4191 = 65.88276793382438\n",
      "Cost4192 = 65.88771788893712\n",
      "Cost4193 = 65.88547049275918\n",
      "Cost4194 = 65.88169212843724\n",
      "Cost4195 = 65.88720211975551\n",
      "Cost4196 = 65.8783012416228\n",
      "Cost4197 = 65.88368157659626\n",
      "Cost4198 = 65.88717245676845\n",
      "Cost4199 = 65.87857192364555\n",
      "Cost4200 = 65.88177045071981\n",
      "Cost4201 = 65.87966362322672\n",
      "Cost4202 = 65.87705146059588\n",
      "Cost4203 = 65.87524120058488\n",
      "Cost4204 = 65.8788146415566\n",
      "Cost4205 = 65.88002322205075\n",
      "Cost4206 = 65.88219282629939\n",
      "Cost4207 = 65.87829631972198\n",
      "Cost4208 = 65.88143507418471\n",
      "Cost4209 = 65.87934705598316\n",
      "Cost4210 = 65.87775315146368\n",
      "Cost4211 = 65.8775442388671\n",
      "Cost4212 = 65.8814730620035\n",
      "Cost4213 = 65.87882000906916\n",
      "Cost4214 = 65.8829866622128\n",
      "Cost4215 = 65.87403499103105\n",
      "Cost4216 = 65.8698293739748\n",
      "Cost4217 = 65.87585367114768\n",
      "Cost4218 = 65.87576274775905\n",
      "Cost4219 = 65.86922478606435\n",
      "Cost4220 = 65.87382733980806\n",
      "Cost4221 = 65.871820878182\n",
      "Cost4222 = 65.87521865381686\n",
      "Cost4223 = 65.86782995861807\n",
      "Cost4224 = 65.87259014281312\n",
      "Cost4225 = 65.87061528636147\n",
      "Cost4226 = 65.87483911543987\n",
      "Cost4227 = 65.87058848697083\n",
      "Cost4228 = 65.86628199201739\n",
      "Cost4229 = 65.86514597830458\n",
      "Cost4230 = 65.86931537998413\n",
      "Cost4231 = 65.86545941218444\n",
      "Cost4232 = 65.86698697209322\n",
      "Cost4233 = 65.87158556175324\n",
      "Cost4234 = 65.86028378267348\n",
      "Cost4235 = 65.87386110596887\n",
      "Cost4236 = 65.86461906285766\n",
      "Cost4237 = 65.86275084867769\n",
      "Cost4238 = 65.8661735817494\n",
      "Cost4239 = 65.86174534234425\n",
      "Cost4240 = 65.86681684809425\n",
      "Cost4241 = 65.86864580950903\n",
      "Cost4242 = 65.86020830249409\n",
      "Cost4243 = 65.86221513368618\n",
      "Cost4244 = 65.85933640037976\n",
      "Cost4245 = 65.86242056031003\n",
      "Cost4246 = 65.85799345216354\n",
      "Cost4247 = 65.85826542468992\n",
      "Cost4248 = 65.85907170304375\n",
      "Cost4249 = 65.8532980114333\n",
      "Cost4250 = 65.85045943267946\n",
      "Cost4251 = 65.85577127072258\n",
      "Cost4252 = 65.85547512397137\n",
      "Cost4253 = 65.85490895966869\n",
      "Cost4254 = 65.86041166442777\n",
      "Cost4255 = 65.85431026400094\n",
      "Cost4256 = 65.8573369466195\n",
      "Cost4257 = 65.85401947336135\n",
      "Cost4258 = 65.84940166120808\n",
      "Cost4259 = 65.85382539942486\n",
      "Cost4260 = 65.85226981231854\n",
      "Cost4261 = 65.85288908303157\n",
      "Cost4262 = 65.85314211316253\n",
      "Cost4263 = 65.84948736604116\n",
      "Cost4264 = 65.84900313922688\n",
      "Cost4265 = 65.84414269675227\n",
      "Cost4266 = 65.85062357278464\n",
      "Cost4267 = 65.84422885589679\n",
      "Cost4268 = 65.85141045929834\n",
      "Cost4269 = 65.84674849691635\n",
      "Cost4270 = 65.85034473407747\n",
      "Cost4271 = 65.84577313967795\n",
      "Cost4272 = 65.84378603463195\n",
      "Cost4273 = 65.84343533659865\n",
      "Cost4274 = 65.84766322717036\n",
      "Cost4275 = 65.84678364603543\n",
      "Cost4276 = 65.84653622758056\n",
      "Cost4277 = 65.83790532017692\n",
      "Cost4278 = 65.8443784741086\n",
      "Cost4279 = 65.84690052152617\n",
      "Cost4280 = 65.84366547735097\n",
      "Cost4281 = 65.84085613152857\n",
      "Cost4282 = 65.8367301574694\n",
      "Cost4283 = 65.835276825626\n",
      "Cost4284 = 65.8385935502029\n",
      "Cost4285 = 65.8363662069146\n",
      "Cost4286 = 65.8386597174184\n",
      "Cost4287 = 65.83744538725755\n",
      "Cost4288 = 65.83799435586704\n",
      "Cost4289 = 65.83684678294857\n",
      "Cost4290 = 65.83507310096235\n",
      "Cost4291 = 65.83037458817765\n",
      "Cost4292 = 65.83668667122463\n",
      "Cost4293 = 65.83337515751036\n",
      "Cost4294 = 65.83491562217986\n",
      "Cost4295 = 65.83238236745558\n",
      "Cost4296 = 65.83994916519806\n",
      "Cost4297 = 65.82892858960248\n",
      "Cost4298 = 65.82850516689888\n",
      "Cost4299 = 65.83534019079538\n",
      "Cost4300 = 65.83129785557539\n",
      "Cost4301 = 65.82765292468163\n",
      "Cost4302 = 65.8280605985209\n",
      "Cost4303 = 65.82571091067345\n",
      "Cost4304 = 65.82353642508869\n",
      "Cost4305 = 65.82137048810603\n",
      "Cost4306 = 65.82449042203972\n",
      "Cost4307 = 65.82437770662779\n",
      "Cost4308 = 65.82780103026211\n",
      "Cost4309 = 65.82440564012144\n",
      "Cost4310 = 65.82418144368216\n",
      "Cost4311 = 65.82651404032052\n",
      "Cost4312 = 65.81772069045616\n",
      "Cost4313 = 65.82226439583323\n",
      "Cost4314 = 65.82154348827696\n",
      "Cost4315 = 65.82339736829704\n",
      "Cost4316 = 65.82364358716276\n",
      "Cost4317 = 65.81745744675443\n",
      "Cost4318 = 65.81722976744406\n",
      "Cost4319 = 65.81618741023152\n",
      "Cost4320 = 65.81780712996411\n",
      "Cost4321 = 65.8170892901124\n",
      "Cost4322 = 65.82119038473084\n",
      "Cost4323 = 65.8175050670553\n",
      "Cost4324 = 65.81730021124939\n",
      "Cost4325 = 65.81720653166265\n",
      "Cost4326 = 65.8146972917756\n",
      "Cost4327 = 65.81336959224548\n",
      "Cost4328 = 65.81654496024603\n",
      "Cost4329 = 65.81039990978515\n",
      "Cost4330 = 65.81108206677604\n",
      "Cost4331 = 65.8131171901696\n",
      "Cost4332 = 65.81497040294234\n",
      "Cost4333 = 65.80957102273588\n",
      "Cost4334 = 65.81262040371685\n",
      "Cost4335 = 65.81371557084135\n",
      "Cost4336 = 65.81219268062344\n",
      "Cost4337 = 65.80778866024646\n",
      "Cost4338 = 65.8133079606541\n",
      "Cost4339 = 65.80859141191542\n",
      "Cost4340 = 65.8049644812771\n",
      "Cost4341 = 65.8119583998704\n",
      "Cost4342 = 65.81185957625954\n",
      "Cost4343 = 65.81167790018903\n",
      "Cost4344 = 65.80538549707363\n",
      "Cost4345 = 65.80688288162338\n",
      "Cost4346 = 65.8068302632891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost4347 = 65.8064117609469\n",
      "Cost4348 = 65.8029344561881\n",
      "Cost4349 = 65.80870023019638\n",
      "Cost4350 = 65.80525640984406\n",
      "Cost4351 = 65.80413346486681\n",
      "Cost4352 = 65.80494167480128\n",
      "Cost4353 = 65.80456521147536\n",
      "Cost4354 = 65.80144242601278\n",
      "Cost4355 = 65.80296678749976\n",
      "Cost4356 = 65.80190458533643\n",
      "Cost4357 = 65.79939380676372\n",
      "Cost4358 = 65.80611443455935\n",
      "Cost4359 = 65.79609605943793\n",
      "Cost4360 = 65.80146374947154\n",
      "Cost4361 = 65.80687314781996\n",
      "Cost4362 = 65.79770522524252\n",
      "Cost4363 = 65.80171315985417\n",
      "Cost4364 = 65.79561158553194\n",
      "Cost4365 = 65.79787298208545\n",
      "Cost4366 = 65.79798214722608\n",
      "Cost4367 = 65.80207317083021\n",
      "Cost4368 = 65.7880023919485\n",
      "Cost4369 = 65.79844731963438\n",
      "Cost4370 = 65.79910783157442\n",
      "Cost4371 = 65.79400847218224\n",
      "Cost4372 = 65.79699010090013\n",
      "Cost4373 = 65.79197915385474\n",
      "Cost4374 = 65.79147485819824\n",
      "Cost4375 = 65.79289835564437\n",
      "Cost4376 = 65.79417897647626\n",
      "Cost4377 = 65.79150894959062\n",
      "Cost4378 = 65.78788860482535\n",
      "Cost4379 = 65.7856619939789\n",
      "Cost4380 = 65.79493622446263\n",
      "Cost4381 = 65.79175446516038\n",
      "Cost4382 = 65.79288367271651\n",
      "Cost4383 = 65.7888153557611\n",
      "Cost4384 = 65.7901267423711\n",
      "Cost4385 = 65.78877985163712\n",
      "Cost4386 = 65.78673006058251\n",
      "Cost4387 = 65.7871242129771\n",
      "Cost4388 = 65.7883202698334\n",
      "Cost4389 = 65.78266016605754\n",
      "Cost4390 = 65.7873431965749\n",
      "Cost4391 = 65.78136816292862\n",
      "Cost4392 = 65.7874597631417\n",
      "Cost4393 = 65.78468470148894\n",
      "Cost4394 = 65.78961806750266\n",
      "Cost4395 = 65.77880593368506\n",
      "Cost4396 = 65.77977458591913\n",
      "Cost4397 = 65.77858339245964\n",
      "Cost4398 = 65.78104888921993\n",
      "Cost4399 = 65.77753265680512\n",
      "Cost4400 = 65.7815656985191\n",
      "Cost4401 = 65.78094244890876\n",
      "Cost4402 = 65.78783416317346\n",
      "Cost4403 = 65.78274113987075\n",
      "Cost4404 = 65.7837421761423\n",
      "Cost4405 = 65.78044921862998\n",
      "Cost4406 = 65.78540034118822\n",
      "Cost4407 = 65.78484513584087\n",
      "Cost4408 = 65.77929901469092\n",
      "Cost4409 = 65.78231137928776\n",
      "Cost4410 = 65.77603742242137\n",
      "Cost4411 = 65.78142014348892\n",
      "Cost4412 = 65.77676037602697\n",
      "Cost4413 = 65.78539309968953\n",
      "Cost4414 = 65.77432732751475\n",
      "Cost4415 = 65.77738491959536\n",
      "Cost4416 = 65.77835954733088\n",
      "Cost4417 = 65.7772240294559\n",
      "Cost4418 = 65.77571874072973\n",
      "Cost4419 = 65.77202852296175\n",
      "Cost4420 = 65.77942015897226\n",
      "Cost4421 = 65.77760950725605\n",
      "Cost4422 = 65.77838067123679\n",
      "Cost4423 = 65.77619354339565\n",
      "Cost4424 = 65.77496440480957\n",
      "Cost4425 = 65.76760446200844\n",
      "Cost4426 = 65.77509747650157\n",
      "Cost4427 = 65.77565908446552\n",
      "Cost4428 = 65.77600274234501\n",
      "Cost4429 = 65.7737540918036\n",
      "Cost4430 = 65.7682727852141\n",
      "Cost4431 = 65.77155385408597\n",
      "Cost4432 = 65.77357689208303\n",
      "Cost4433 = 65.77177695593876\n",
      "Cost4434 = 65.77085545300011\n",
      "Cost4435 = 65.76763050727097\n",
      "Cost4436 = 65.76803384788191\n",
      "Cost4437 = 65.76742524274711\n",
      "Cost4438 = 65.76713774018677\n",
      "Cost4439 = 65.76528369908156\n",
      "Cost4440 = 65.769167159128\n",
      "Cost4441 = 65.76251139037436\n",
      "Cost4442 = 65.76637450024073\n",
      "Cost4443 = 65.76439818276073\n",
      "Cost4444 = 65.76416519929056\n",
      "Cost4445 = 65.76390558302148\n",
      "Cost4446 = 65.76553503369225\n",
      "Cost4447 = 65.76984884152272\n",
      "Cost4448 = 65.76179933076476\n",
      "Cost4449 = 65.75885536778902\n",
      "Cost4450 = 65.76699912309957\n",
      "Cost4451 = 65.76381922534866\n",
      "Cost4452 = 65.76626689857724\n",
      "Cost4453 = 65.76573529760952\n",
      "Cost4454 = 65.76131660235514\n",
      "Cost4455 = 65.75781729938477\n",
      "Cost4456 = 65.76343409281284\n",
      "Cost4457 = 65.75638764494586\n",
      "Cost4458 = 65.76623494406752\n",
      "Cost4459 = 65.75934242454296\n",
      "Cost4460 = 65.76507605158305\n",
      "Cost4461 = 65.75376037408314\n",
      "Cost4462 = 65.75896004967215\n",
      "Cost4463 = 65.75700513160797\n",
      "Cost4464 = 65.76130867020488\n",
      "Cost4465 = 65.75341180565287\n",
      "Cost4466 = 65.75922183797275\n",
      "Cost4467 = 65.75885318555628\n",
      "Cost4468 = 65.75730763147534\n",
      "Cost4469 = 65.75297492424971\n",
      "Cost4470 = 65.75461380887405\n",
      "Cost4471 = 65.75418335460108\n",
      "Cost4472 = 65.75731342743684\n",
      "Cost4473 = 65.75653777973882\n",
      "Cost4474 = 65.75194414393901\n",
      "Cost4475 = 65.75102486971328\n",
      "Cost4476 = 65.75285688100624\n",
      "Cost4477 = 65.74691981681015\n",
      "Cost4478 = 65.7550100619345\n",
      "Cost4479 = 65.75163696844058\n",
      "Cost4480 = 65.75013085106113\n",
      "Cost4481 = 65.7535979832523\n",
      "Cost4482 = 65.75278366437485\n",
      "Cost4483 = 65.74091758798515\n",
      "Cost4484 = 65.74227502751063\n",
      "Cost4485 = 65.7438398044739\n",
      "Cost4486 = 65.745845756792\n",
      "Cost4487 = 65.74654861332631\n",
      "Cost4488 = 65.75251996278371\n",
      "Cost4489 = 65.74931438176382\n",
      "Cost4490 = 65.7415417292631\n",
      "Cost4491 = 65.74531840794461\n",
      "Cost4492 = 65.7465267460245\n",
      "Cost4493 = 65.74507507265494\n",
      "Cost4494 = 65.74249413730816\n",
      "Cost4495 = 65.74111527036416\n",
      "Cost4496 = 65.74547075713673\n",
      "Cost4497 = 65.74756318226018\n",
      "Cost4498 = 65.74325607107646\n",
      "Cost4499 = 65.74716753123354\n",
      "Cost4500 = 65.74207253169764\n",
      "Cost4501 = 65.74318401770675\n",
      "Cost4502 = 65.74089900034902\n",
      "Cost4503 = 65.74033920012585\n",
      "Cost4504 = 65.74188992436265\n",
      "Cost4505 = 65.74342452173394\n",
      "Cost4506 = 65.74443270284226\n",
      "Cost4507 = 65.7409924090595\n",
      "Cost4508 = 65.737189175689\n",
      "Cost4509 = 65.73582354084864\n",
      "Cost4510 = 65.73731132880722\n",
      "Cost4511 = 65.73387734990834\n",
      "Cost4512 = 65.7305612943509\n",
      "Cost4513 = 65.73801498462414\n",
      "Cost4514 = 65.73513750197199\n",
      "Cost4515 = 65.7407488699757\n",
      "Cost4516 = 65.73538181683857\n",
      "Cost4517 = 65.73471468087526\n",
      "Cost4518 = 65.72975876886473\n",
      "Cost4519 = 65.73147204249149\n",
      "Cost4520 = 65.73676018106129\n",
      "Cost4521 = 65.73348652004996\n",
      "Cost4522 = 65.73212750103674\n",
      "Cost4523 = 65.73030909747558\n",
      "Cost4524 = 65.72984574330047\n",
      "Cost4525 = 65.72720474967383\n",
      "Cost4526 = 65.73369580678707\n",
      "Cost4527 = 65.73156688697561\n",
      "Cost4528 = 65.73536105560606\n",
      "Cost4529 = 65.72646412206176\n",
      "Cost4530 = 65.73500849694196\n",
      "Cost4531 = 65.72992540748339\n",
      "Cost4532 = 65.73018852554176\n",
      "Cost4533 = 65.7311784163707\n",
      "Cost4534 = 65.72803374278507\n",
      "Cost4535 = 65.72893223127909\n",
      "Cost4536 = 65.73158390518957\n",
      "Cost4537 = 65.73007341528732\n",
      "Cost4538 = 65.7285282570864\n",
      "Cost4539 = 65.72422889056887\n",
      "Cost4540 = 65.72559389796595\n",
      "Cost4541 = 65.72992961834004\n",
      "Cost4542 = 65.72949285013117\n",
      "Cost4543 = 65.7267819198169\n",
      "Cost4544 = 65.72612813150914\n",
      "Cost4545 = 65.72351345188105\n",
      "Cost4546 = 65.72520547823375\n",
      "Cost4547 = 65.72054251216294\n",
      "Cost4548 = 65.71951413225945\n",
      "Cost4549 = 65.72365827693983\n",
      "Cost4550 = 65.72257535321813\n",
      "Cost4551 = 65.72221564312306\n",
      "Cost4552 = 65.72582177899743\n",
      "Cost4553 = 65.72401502279578\n",
      "Cost4554 = 65.71846534827415\n",
      "Cost4555 = 65.71929396421133\n",
      "Cost4556 = 65.72078474570827\n",
      "Cost4557 = 65.72059949765796\n",
      "Cost4558 = 65.72120799538628\n",
      "Cost4559 = 65.72291173891121\n",
      "Cost4560 = 65.72212164047396\n",
      "Cost4561 = 65.71420109424284\n",
      "Cost4562 = 65.7197669716824\n",
      "Cost4563 = 65.7172957891803\n",
      "Cost4564 = 65.71604399487713\n",
      "Cost4565 = 65.71959517785483\n",
      "Cost4566 = 65.72035637581517\n",
      "Cost4567 = 65.71477247051641\n",
      "Cost4568 = 65.71609362862944\n",
      "Cost4569 = 65.71568795889578\n",
      "Cost4570 = 65.71574638135148\n",
      "Cost4571 = 65.71775370112509\n",
      "Cost4572 = 65.71202043523351\n",
      "Cost4573 = 65.71178923701514\n",
      "Cost4574 = 65.70618718687315\n",
      "Cost4575 = 65.712279649618\n",
      "Cost4576 = 65.70943867283016\n",
      "Cost4577 = 65.71074113777901\n",
      "Cost4578 = 65.71334466900355\n",
      "Cost4579 = 65.71103826194597\n",
      "Cost4580 = 65.70712595833962\n",
      "Cost4581 = 65.71171774300858\n",
      "Cost4582 = 65.70758539297717\n",
      "Cost4583 = 65.7140833706937\n",
      "Cost4584 = 65.70801987028608\n",
      "Cost4585 = 65.71161744771946\n",
      "Cost4586 = 65.70696876072397\n",
      "Cost4587 = 65.70561386061829\n",
      "Cost4588 = 65.70599126669192\n",
      "Cost4589 = 65.71059861625106\n",
      "Cost4590 = 65.70341364436328\n",
      "Cost4591 = 65.70433268150842\n",
      "Cost4592 = 65.70461861660434\n",
      "Cost4593 = 65.70840999968408\n",
      "Cost4594 = 65.70724718017442\n",
      "Cost4595 = 65.7041943985662\n",
      "Cost4596 = 65.70287068574304\n",
      "Cost4597 = 65.69908333333274\n",
      "Cost4598 = 65.70117519836967\n",
      "Cost4599 = 65.70843460707653\n",
      "Cost4600 = 65.70337422415639\n",
      "Cost4601 = 65.70201714186025\n",
      "Cost4602 = 65.70091498002846\n",
      "Cost4603 = 65.7037034527717\n",
      "Cost4604 = 65.69855532972116\n",
      "Cost4605 = 65.70025555564979\n",
      "Cost4606 = 65.70085666574847\n",
      "Cost4607 = 65.70162573690604\n",
      "Cost4608 = 65.69780370202754\n",
      "Cost4609 = 65.69933713939989\n",
      "Cost4610 = 65.69669348053958\n",
      "Cost4611 = 65.69642083379125\n",
      "Cost4612 = 65.69647543648281\n",
      "Cost4613 = 65.70065259551322\n",
      "Cost4614 = 65.69676566100513\n",
      "Cost4615 = 65.69299714963876\n",
      "Cost4616 = 65.69727001478495\n",
      "Cost4617 = 65.69877176373447\n",
      "Cost4618 = 65.69549275601099\n",
      "Cost4619 = 65.6940281375435\n",
      "Cost4620 = 65.69440452282187\n",
      "Cost4621 = 65.69377046802339\n",
      "Cost4622 = 65.69161911023751\n",
      "Cost4623 = 65.69305241351846\n",
      "Cost4624 = 65.69281248693301\n",
      "Cost4625 = 65.69492442309472\n",
      "Cost4626 = 65.69773170238722\n",
      "Cost4627 = 65.69234785515603\n",
      "Cost4628 = 65.69453725571223\n",
      "Cost4629 = 65.69306316892188\n",
      "Cost4630 = 65.69126216375282\n",
      "Cost4631 = 65.68757576080756\n",
      "Cost4632 = 65.68871195244185\n",
      "Cost4633 = 65.68600744388105\n",
      "Cost4634 = 65.69177880182664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost4635 = 65.68540390971411\n",
      "Cost4636 = 65.69133448233042\n",
      "Cost4637 = 65.68986910845001\n",
      "Cost4638 = 65.69232574722726\n",
      "Cost4639 = 65.69344365045657\n",
      "Cost4640 = 65.68731640976321\n",
      "Cost4641 = 65.68871828508264\n",
      "Cost4642 = 65.69141113577277\n",
      "Cost4643 = 65.68775222386783\n",
      "Cost4644 = 65.68804976593313\n",
      "Cost4645 = 65.6865695453186\n",
      "Cost4646 = 65.68289724515505\n",
      "Cost4647 = 65.68490684333356\n",
      "Cost4648 = 65.68799619622308\n",
      "Cost4649 = 65.68142025342858\n",
      "Cost4650 = 65.6854539617172\n",
      "Cost4651 = 65.68239376792256\n",
      "Cost4652 = 65.68288118197619\n",
      "Cost4653 = 65.6861614658426\n",
      "Cost4654 = 65.68816227684655\n",
      "Cost4655 = 65.67739097085115\n",
      "Cost4656 = 65.68131856277101\n",
      "Cost4657 = 65.67560970446021\n",
      "Cost4658 = 65.67881231609503\n",
      "Cost4659 = 65.67953094913156\n",
      "Cost4660 = 65.6735178369366\n",
      "Cost4661 = 65.67884475737421\n",
      "Cost4662 = 65.67871048178552\n",
      "Cost4663 = 65.67782999219503\n",
      "Cost4664 = 65.6803979383423\n",
      "Cost4665 = 65.67395369389841\n",
      "Cost4666 = 65.68395514645078\n",
      "Cost4667 = 65.67610567273552\n",
      "Cost4668 = 65.67393765238141\n",
      "Cost4669 = 65.67129984745755\n",
      "Cost4670 = 65.67453732601\n",
      "Cost4671 = 65.67483167860236\n",
      "Cost4672 = 65.67678226720872\n",
      "Cost4673 = 65.67059343983631\n",
      "Cost4674 = 65.67316806280456\n",
      "Cost4675 = 65.67306558449488\n",
      "Cost4676 = 65.673926906951\n",
      "Cost4677 = 65.67216945764784\n",
      "Cost4678 = 65.6738082702193\n",
      "Cost4679 = 65.67341298656126\n",
      "Cost4680 = 65.6772872548272\n",
      "Cost4681 = 65.66881957975761\n",
      "Cost4682 = 65.67146530228648\n",
      "Cost4683 = 65.6694894154581\n",
      "Cost4684 = 65.67313127601516\n",
      "Cost4685 = 65.66960816287603\n",
      "Cost4686 = 65.66930698069659\n",
      "Cost4687 = 65.66983639185109\n",
      "Cost4688 = 65.66815938574719\n",
      "Cost4689 = 65.66872373766093\n",
      "Cost4690 = 65.66729056065651\n",
      "Cost4691 = 65.66966412254246\n",
      "Cost4692 = 65.67319550251527\n",
      "Cost4693 = 65.66917537149759\n",
      "Cost4694 = 65.66806739095189\n",
      "Cost4695 = 65.66981183758935\n",
      "Cost4696 = 65.66772414541657\n",
      "Cost4697 = 65.66779297676047\n",
      "Cost4698 = 65.66589547174846\n",
      "Cost4699 = 65.66223307703997\n",
      "Cost4700 = 65.66079999599933\n",
      "Cost4701 = 65.66292964590774\n",
      "Cost4702 = 65.66153020789322\n",
      "Cost4703 = 65.66257361237496\n",
      "Cost4704 = 65.66095830300772\n",
      "Cost4705 = 65.66779016107832\n",
      "Cost4706 = 65.66408925802213\n",
      "Cost4707 = 65.664212453671\n",
      "Cost4708 = 65.66517385407467\n",
      "Cost4709 = 65.66233397598327\n",
      "Cost4710 = 65.65948813808392\n",
      "Cost4711 = 65.65821565373867\n",
      "Cost4712 = 65.66145596736756\n",
      "Cost4713 = 65.66083562859403\n",
      "Cost4714 = 65.65992926830017\n",
      "Cost4715 = 65.65646147021496\n",
      "Cost4716 = 65.65785618191269\n",
      "Cost4717 = 65.66076860531362\n",
      "Cost4718 = 65.65808893938794\n",
      "Cost4719 = 65.66061960875389\n",
      "Cost4720 = 65.66072366509111\n",
      "Cost4721 = 65.65772628753359\n",
      "Cost4722 = 65.65694525041646\n",
      "Cost4723 = 65.65491186259278\n",
      "Cost4724 = 65.66057450208372\n",
      "Cost4725 = 65.6608722695717\n",
      "Cost4726 = 65.65439642647878\n",
      "Cost4727 = 65.66123703783572\n",
      "Cost4728 = 65.6520335675931\n",
      "Cost4729 = 65.65672231552152\n",
      "Cost4730 = 65.6582674143889\n",
      "Cost4731 = 65.65392459738874\n",
      "Cost4732 = 65.65020667603441\n",
      "Cost4733 = 65.65512544950019\n",
      "Cost4734 = 65.65354251418191\n",
      "Cost4735 = 65.65469373195691\n",
      "Cost4736 = 65.65131022945074\n",
      "Cost4737 = 65.65056234571956\n",
      "Cost4738 = 65.64808407509155\n",
      "Cost4739 = 65.65056233436164\n",
      "Cost4740 = 65.65556575049234\n",
      "Cost4741 = 65.64880709790724\n",
      "Cost4742 = 65.65135441953034\n",
      "Cost4743 = 65.64735588155168\n",
      "Cost4744 = 65.65055371403382\n",
      "Cost4745 = 65.65115872146598\n",
      "Cost4746 = 65.64488545158072\n",
      "Cost4747 = 65.6428448038541\n",
      "Cost4748 = 65.6464995938177\n",
      "Cost4749 = 65.6434718394184\n",
      "Cost4750 = 65.65137838688624\n",
      "Cost4751 = 65.65166716718103\n",
      "Cost4752 = 65.64558519896683\n",
      "Cost4753 = 65.64513191439956\n",
      "Cost4754 = 65.6446877671175\n",
      "Cost4755 = 65.64349084523185\n",
      "Cost4756 = 65.64469503280245\n",
      "Cost4757 = 65.6439844175614\n",
      "Cost4758 = 65.65053185928822\n",
      "Cost4759 = 65.64649166383971\n",
      "Cost4760 = 65.64635289825978\n",
      "Cost4761 = 65.64000166638941\n",
      "Cost4762 = 65.6415611033153\n",
      "Cost4763 = 65.64459722300762\n",
      "Cost4764 = 65.64546831376495\n",
      "Cost4765 = 65.638328041814\n",
      "Cost4766 = 65.64230703235116\n",
      "Cost4767 = 65.63915625439446\n",
      "Cost4768 = 65.64645161815429\n",
      "Cost4769 = 65.64156302293802\n",
      "Cost4770 = 65.64066165084873\n",
      "Cost4771 = 65.63938487567495\n",
      "Cost4772 = 65.63892037206183\n",
      "Cost4773 = 65.64346060589149\n",
      "Cost4774 = 65.64195195737344\n",
      "Cost4775 = 65.6356399287025\n",
      "Cost4776 = 65.64418812158125\n",
      "Cost4777 = 65.6357328032423\n",
      "Cost4778 = 65.64095882757093\n",
      "Cost4779 = 65.63647908078468\n",
      "Cost4780 = 65.63661411147514\n",
      "Cost4781 = 65.63479652174671\n",
      "Cost4782 = 65.63021557662006\n",
      "Cost4783 = 65.63235951120187\n",
      "Cost4784 = 65.62968517947192\n",
      "Cost4785 = 65.6399860539419\n",
      "Cost4786 = 65.63117534445414\n",
      "Cost4787 = 65.63061520511319\n",
      "Cost4788 = 65.63393153173614\n",
      "Cost4789 = 65.6349279851588\n",
      "Cost4790 = 65.63446567426318\n",
      "Cost4791 = 65.63262729493336\n",
      "Cost4792 = 65.63246603760649\n",
      "Cost4793 = 65.62929881764971\n",
      "Cost4794 = 65.63406186995856\n",
      "Cost4795 = 65.63278427743322\n",
      "Cost4796 = 65.6341374171615\n",
      "Cost4797 = 65.62973118415974\n",
      "Cost4798 = 65.63178942652294\n",
      "Cost4799 = 65.6300610570981\n",
      "Cost4800 = 65.62720239619142\n",
      "Cost4801 = 65.63247424009023\n",
      "Cost4802 = 65.6343505941598\n",
      "Cost4803 = 65.63278527488907\n",
      "Cost4804 = 65.62593189325929\n",
      "Cost4805 = 65.62846969226135\n",
      "Cost4806 = 65.62680062934511\n",
      "Cost4807 = 65.62859414507244\n",
      "Cost4808 = 65.62736634840081\n",
      "Cost4809 = 65.62445327483988\n",
      "Cost4810 = 65.62778404580766\n",
      "Cost4811 = 65.62696260895981\n",
      "Cost4812 = 65.62579800176391\n",
      "Cost4813 = 65.62374507099217\n",
      "Cost4814 = 65.6246043546465\n",
      "Cost4815 = 65.62981532434165\n",
      "Cost4816 = 65.62118468219997\n",
      "Cost4817 = 65.62777603273065\n",
      "Cost4818 = 65.62230157555015\n",
      "Cost4819 = 65.62540708938295\n",
      "Cost4820 = 65.62593565397418\n",
      "Cost4821 = 65.62188291299917\n",
      "Cost4822 = 65.62560465226424\n",
      "Cost4823 = 65.62682365112946\n",
      "Cost4824 = 65.61904934056702\n",
      "Cost4825 = 65.6259411630986\n",
      "Cost4826 = 65.62021895615634\n",
      "Cost4827 = 65.62508462409826\n",
      "Cost4828 = 65.6179632610096\n",
      "Cost4829 = 65.62119053991792\n",
      "Cost4830 = 65.6227699636063\n",
      "Cost4831 = 65.6236033623279\n",
      "Cost4832 = 65.622331126358\n",
      "Cost4833 = 65.62416563089032\n",
      "Cost4834 = 65.62145437959973\n",
      "Cost4835 = 65.62193875051878\n",
      "Cost4836 = 65.6234352160845\n",
      "Cost4837 = 65.61961093527063\n",
      "Cost4838 = 65.6170072702663\n",
      "Cost4839 = 65.61742764711963\n",
      "Cost4840 = 65.61555485390402\n",
      "Cost4841 = 65.61651753340915\n",
      "Cost4842 = 65.61772218758865\n",
      "Cost4843 = 65.61224705556259\n",
      "Cost4844 = 65.61651055804246\n",
      "Cost4845 = 65.61408867969351\n",
      "Cost4846 = 65.61095778574197\n",
      "Cost4847 = 65.61574410042834\n",
      "Cost4848 = 65.61628305988121\n",
      "Cost4849 = 65.61671126060986\n",
      "Cost4850 = 65.60956593500842\n",
      "Cost4851 = 65.61188825290736\n",
      "Cost4852 = 65.61335505346757\n",
      "Cost4853 = 65.60718038450491\n",
      "Cost4854 = 65.61152030959708\n",
      "Cost4855 = 65.61090755445471\n",
      "Cost4856 = 65.6117368584064\n",
      "Cost4857 = 65.61392115307278\n",
      "Cost4858 = 65.60620718335485\n",
      "Cost4859 = 65.60956697579375\n",
      "Cost4860 = 65.60717226165809\n",
      "Cost4861 = 65.61172575826238\n",
      "Cost4862 = 65.61001639456099\n",
      "Cost4863 = 65.61176971673163\n",
      "Cost4864 = 65.61270641165991\n",
      "Cost4865 = 65.61192366287746\n",
      "Cost4866 = 65.61224935255349\n",
      "Cost4867 = 65.6082243731758\n",
      "Cost4868 = 65.6082209423029\n",
      "Cost4869 = 65.6008346679428\n",
      "Cost4870 = 65.60777608850981\n",
      "Cost4871 = 65.61082012956402\n",
      "Cost4872 = 65.60294222941721\n",
      "Cost4873 = 65.60342269544867\n",
      "Cost4874 = 65.60326756887694\n",
      "Cost4875 = 65.60663610723151\n",
      "Cost4876 = 65.60539082296349\n",
      "Cost4877 = 65.60404559595179\n",
      "Cost4878 = 65.6060367435595\n",
      "Cost4879 = 65.60688517169275\n",
      "Cost4880 = 65.60480572914382\n",
      "Cost4881 = 65.60713011325694\n",
      "Cost4882 = 65.6100002625616\n",
      "Cost4883 = 65.60217999949474\n",
      "Cost4884 = 65.61151172854659\n",
      "Cost4885 = 65.60646433761575\n",
      "Cost4886 = 65.60100244108719\n",
      "Cost4887 = 65.60571975556\n",
      "Cost4888 = 65.60678763905672\n",
      "Cost4889 = 65.6065222368456\n",
      "Cost4890 = 65.60345887052128\n",
      "Cost4891 = 65.60941832374984\n",
      "Cost4892 = 65.60523649776405\n",
      "Cost4893 = 65.60701128004025\n",
      "Cost4894 = 65.60639526370109\n",
      "Cost4895 = 65.60516657283335\n",
      "Cost4896 = 65.6015348877786\n",
      "Cost4897 = 65.60341463931748\n",
      "Cost4898 = 65.60417240159816\n",
      "Cost4899 = 65.60023286299516\n",
      "Cost4900 = 65.60208148650622\n",
      "Cost4901 = 65.6018502979322\n",
      "Cost4902 = 65.59768850018939\n",
      "Cost4903 = 65.60499984584364\n",
      "Cost4904 = 65.60118204139631\n",
      "Cost4905 = 65.6001121122472\n",
      "Cost4906 = 65.60390308424145\n",
      "Cost4907 = 65.60152161213531\n",
      "Cost4908 = 65.60449028571146\n",
      "Cost4909 = 65.60108870166752\n",
      "Cost4910 = 65.6045901696768\n",
      "Cost4911 = 65.60175620060272\n",
      "Cost4912 = 65.59828284608435\n",
      "Cost4913 = 65.59795089643161\n",
      "Cost4914 = 65.60047210685914\n",
      "Cost4915 = 65.60222076203009\n",
      "Cost4916 = 65.59941011381062\n",
      "Cost4917 = 65.59951010293072\n",
      "Cost4918 = 65.59549849820274\n",
      "Cost4919 = 65.59772228886057\n",
      "Cost4920 = 65.59720247297479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost4921 = 65.59840262943116\n",
      "Cost4922 = 65.59910809664612\n",
      "Cost4923 = 65.59401454950758\n",
      "Cost4924 = 65.598771036502\n",
      "Cost4925 = 65.59739830317719\n",
      "Cost4926 = 65.59793057555706\n",
      "Cost4927 = 65.60096977524448\n",
      "Cost4928 = 65.59558559139\n",
      "Cost4929 = 65.59634956365225\n",
      "Cost4930 = 65.59587317394072\n",
      "Cost4931 = 65.59868865073933\n",
      "Cost4932 = 65.59439547148209\n",
      "Cost4933 = 65.59364147910678\n",
      "Cost4934 = 65.59681227273761\n",
      "Cost4935 = 65.59720185690323\n",
      "Cost4936 = 65.59824902562619\n",
      "Cost4937 = 65.59614664128746\n",
      "Cost4938 = 65.59644572836383\n",
      "Cost4939 = 65.59515126373776\n",
      "Cost4940 = 65.59481830908855\n",
      "Cost4941 = 65.59540003937182\n",
      "Cost4942 = 65.59685594155951\n",
      "Cost4943 = 65.58829272468954\n",
      "Cost4944 = 65.58765325188594\n",
      "Cost4945 = 65.59068163955276\n",
      "Cost4946 = 65.59506098467268\n",
      "Cost4947 = 65.59197464333188\n",
      "Cost4948 = 65.59019663187779\n",
      "Cost4949 = 65.5903086609086\n",
      "Cost4950 = 65.59445932973793\n",
      "Cost4951 = 65.59142977936634\n",
      "Cost4952 = 65.59490496429991\n",
      "Cost4953 = 65.59146933992731\n",
      "Cost4954 = 65.59257497720026\n",
      "Cost4955 = 65.5892897115424\n",
      "Cost4956 = 65.59937521102688\n",
      "Cost4957 = 65.58691369096795\n",
      "Cost4958 = 65.58745205595349\n",
      "Cost4959 = 65.59294749255585\n",
      "Cost4960 = 65.58951148105926\n",
      "Cost4961 = 65.58795904320982\n",
      "Cost4962 = 65.58910664572588\n",
      "Cost4963 = 65.5899332135937\n",
      "Cost4964 = 65.58793106276318\n",
      "Cost4965 = 65.58991614323656\n",
      "Cost4966 = 65.5896250439298\n",
      "Cost4967 = 65.58679234402285\n",
      "Cost4968 = 65.59056221049057\n",
      "Cost4969 = 65.58843866475561\n",
      "Cost4970 = 65.58594205215769\n",
      "Cost4971 = 65.58183357725844\n",
      "Cost4972 = 65.5833788538752\n",
      "Cost4973 = 65.5856701055701\n",
      "Cost4974 = 65.58541403002786\n",
      "Cost4975 = 65.58610093112809\n",
      "Cost4976 = 65.59376573932065\n",
      "Cost4977 = 65.58808336895461\n",
      "Cost4978 = 65.58782722720086\n",
      "Cost4979 = 65.58709890638143\n",
      "Cost4980 = 65.59297134291894\n",
      "Cost4981 = 65.58491748319706\n",
      "Cost4982 = 65.58149319971088\n",
      "Cost4983 = 65.58316633672985\n",
      "Cost4984 = 65.58826179158842\n",
      "Cost4985 = 65.586325201788\n",
      "Cost4986 = 65.58535101695456\n",
      "Cost4987 = 65.58267143936605\n",
      "Cost4988 = 65.58137942320434\n",
      "Cost4989 = 65.5852424601216\n",
      "Cost4990 = 65.58147890113148\n",
      "Cost4991 = 65.5829734439851\n",
      "Cost4992 = 65.58223645643398\n",
      "Cost4993 = 65.58265689441599\n",
      "Cost4994 = 65.58322647630676\n",
      "Cost4995 = 65.58043763612984\n",
      "Cost4996 = 65.5850444630111\n",
      "Cost4997 = 65.58233143133448\n",
      "Cost4998 = 65.58174187224677\n",
      "Cost4999 = 65.58609588461044\n"
     ]
    }
   ],
   "source": [
    "cost = test_gb.Train_models(X_Train, Y_Train, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_test = normalize_predictions(test_gb.Predict(X_CV))\n",
    "train_test = normalize_predictions(test_gb.Predict(X_Train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Resuts\n",
      "Accuracy =  88.71201157742402\n",
      "F1 Score =  0.8421052631578947\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True        208.0          20.0\n",
      "1  Pred False         58.0         405.0\n",
      "-------------------------------\n",
      "Cross Validation Resuts\n",
      "Accuracy =  82.0\n",
      "F1 Score =  0.7464788732394365\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True         53.0          13.0\n",
      "1  Pred False         23.0         111.0\n"
     ]
    }
   ],
   "source": [
    "#Putting this into function at last\n",
    "#Probably should have done this WAY sooner\n",
    "def show_acc(Y_real, Y_hat) :\n",
    "    acc, score, conf = Calc_Accuracy(Y_real, Y_hat)\n",
    "    print(\"Accuracy = \", acc)\n",
    "    print(\"F1 Score = \", score)\n",
    "    print(\"\")\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(conf[[\"Labels\", \"Actual True\", \"Actual False\"]])\n",
    "    \n",
    "print('Training Resuts')  \n",
    "show_acc(Y_Train, train_test)\n",
    "print('-------------------------------')\n",
    "print('Cross Validation Resuts')  \n",
    "show_acc(Y_CV, cv_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our Gradient Boosting Machine works!\n",
    "\n",
    "Its VERY slow to converge but it does show tangible improvements over the initial logistic regrerssion after enough iterations.  Plus this provides a great skeleton to put different models into later on so overall this is a definite success.\n",
    "\n",
    "There are other boosting methods I may end up implementing later, however for now let's move on to testing out automated feature extraction.\n",
    "\n",
    "## Building an Autoencoder\n",
    "\n",
    "So the idea of an autoencoder is to effectively reduce the number of dimensions of the input dataset and then from this reduced representation, recreate the original dataset.  \n",
    "\n",
    "So for example if there is 5 input features, creating a [5, 3, 5] network where the output layer of 5 nodes is using the original X input as its ground truth from which to perform backpropogation.  In this simple example layer 1 would be the encoder, 2 the representation and 3 the decoder.\n",
    "\n",
    "The variant I will also be implementing, which apparently solid at learning useful data representations is a Denoising Autoencoder which relies upon corrupting the input and learning to recreate the ground truth as the output.\n",
    "\n",
    "Sources used in this section - \n",
    "\n",
    "https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629#250927\n",
    "\n",
    "https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n",
    "\n",
    "So to build an autoencoder the process is as follows - \n",
    "\n",
    "1. Put together a simple Neural network with less nodes in inner layers than the output layers.\n",
    "1. Figure out good method for generating architectures - try a few\n",
    "2. Train neural network using the condition X = Y and verify it works correctly.\n",
    "3. Figure out best method for corrupting X and apply corruption to X as input.\n",
    "4. Train new Denoising Autoencoder on corrupted X as input and original X as output.\n",
    "5. Save encoder weights for first N layers of classification network.\n",
    "6. Build/Train classification network with encoder as the first N untrainable layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(691, 24)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test = Input(shape=(24,))\n",
    "\n",
    "encoder_l1 = Dense(12,activation='relu')(input_test)\n",
    "encoded_l = Dense(6, activation='sigmoid')(encoder_l1)\n",
    "\n",
    "decoder_l1 = Dense(12, activation='relu')(encoded_l)\n",
    "decoded_l = Dense(24, activation='sigmoid')(decoder_l1)\n",
    "\n",
    "autoencoder = Model(input_test, decoded_l)\n",
    "\n",
    "encoder = Model(input_test, encoded_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 691 samples, validate on 200 samples\n",
      "Epoch 1/500\n",
      "691/691 [==============================] - 0s 310us/step - loss: 0.5004 - val_loss: 0.5268\n",
      "Epoch 2/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.4965 - val_loss: 0.5224\n",
      "Epoch 3/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.4920 - val_loss: 0.5176\n",
      "Epoch 4/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.4870 - val_loss: 0.5123\n",
      "Epoch 5/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.4815 - val_loss: 0.5063\n",
      "Epoch 6/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.4754 - val_loss: 0.4997\n",
      "Epoch 7/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.4686 - val_loss: 0.4925\n",
      "Epoch 8/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.4614 - val_loss: 0.4848\n",
      "Epoch 9/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.4535 - val_loss: 0.4765\n",
      "Epoch 10/500\n",
      "691/691 [==============================] - 0s 56us/step - loss: 0.4451 - val_loss: 0.4676\n",
      "Epoch 11/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.4362 - val_loss: 0.4583\n",
      "Epoch 12/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.4269 - val_loss: 0.4485\n",
      "Epoch 13/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.4171 - val_loss: 0.4384\n",
      "Epoch 14/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.4071 - val_loss: 0.4279\n",
      "Epoch 15/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.3969 - val_loss: 0.4172\n",
      "Epoch 16/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.3864 - val_loss: 0.4066\n",
      "Epoch 17/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.3761 - val_loss: 0.3962\n",
      "Epoch 18/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.3660 - val_loss: 0.3861\n",
      "Epoch 19/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.3564 - val_loss: 0.3764\n",
      "Epoch 20/500\n",
      "691/691 [==============================] - 0s 59us/step - loss: 0.3472 - val_loss: 0.3674\n",
      "Epoch 21/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.3386 - val_loss: 0.3591\n",
      "Epoch 22/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.3309 - val_loss: 0.3515\n",
      "Epoch 23/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.3239 - val_loss: 0.3446\n",
      "Epoch 24/500\n",
      "691/691 [==============================] - 0s 56us/step - loss: 0.3175 - val_loss: 0.3386\n",
      "Epoch 25/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.3120 - val_loss: 0.3332\n",
      "Epoch 26/500\n",
      "691/691 [==============================] - 0s 56us/step - loss: 0.3071 - val_loss: 0.3285\n",
      "Epoch 27/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.3028 - val_loss: 0.3245\n",
      "Epoch 28/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2992 - val_loss: 0.3211\n",
      "Epoch 29/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2961 - val_loss: 0.3181\n",
      "Epoch 30/500\n",
      "691/691 [==============================] - 0s 56us/step - loss: 0.2934 - val_loss: 0.3155\n",
      "Epoch 31/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2910 - val_loss: 0.3133\n",
      "Epoch 32/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2891 - val_loss: 0.3114\n",
      "Epoch 33/500\n",
      "691/691 [==============================] - 0s 56us/step - loss: 0.2874 - val_loss: 0.3098\n",
      "Epoch 34/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2859 - val_loss: 0.3084\n",
      "Epoch 35/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2846 - val_loss: 0.3072\n",
      "Epoch 36/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2835 - val_loss: 0.3061\n",
      "Epoch 37/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2825 - val_loss: 0.3051\n",
      "Epoch 38/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2817 - val_loss: 0.3043\n",
      "Epoch 39/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2809 - val_loss: 0.3035\n",
      "Epoch 40/500\n",
      "691/691 [==============================] - 0s 56us/step - loss: 0.2803 - val_loss: 0.3028\n",
      "Epoch 41/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2797 - val_loss: 0.3022\n",
      "Epoch 42/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2791 - val_loss: 0.3017\n",
      "Epoch 43/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2787 - val_loss: 0.3012\n",
      "Epoch 44/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2782 - val_loss: 0.3008\n",
      "Epoch 45/500\n",
      "691/691 [==============================] - 0s 56us/step - loss: 0.2779 - val_loss: 0.3004\n",
      "Epoch 46/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2775 - val_loss: 0.3001\n",
      "Epoch 47/500\n",
      "691/691 [==============================] - 0s 56us/step - loss: 0.2772 - val_loss: 0.2997\n",
      "Epoch 48/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2769 - val_loss: 0.2994\n",
      "Epoch 49/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2766 - val_loss: 0.2991\n",
      "Epoch 50/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2764 - val_loss: 0.2989\n",
      "Epoch 51/500\n",
      "691/691 [==============================] - 0s 56us/step - loss: 0.2761 - val_loss: 0.2986\n",
      "Epoch 52/500\n",
      "691/691 [==============================] - 0s 64us/step - loss: 0.2759 - val_loss: 0.2984\n",
      "Epoch 53/500\n",
      "691/691 [==============================] - 0s 62us/step - loss: 0.2757 - val_loss: 0.2982\n",
      "Epoch 54/500\n",
      "691/691 [==============================] - 0s 58us/step - loss: 0.2755 - val_loss: 0.2980\n",
      "Epoch 55/500\n",
      "691/691 [==============================] - 0s 56us/step - loss: 0.2753 - val_loss: 0.2978\n",
      "Epoch 56/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2751 - val_loss: 0.2976\n",
      "Epoch 57/500\n",
      "691/691 [==============================] - 0s 58us/step - loss: 0.2750 - val_loss: 0.2974\n",
      "Epoch 58/500\n",
      "691/691 [==============================] - 0s 56us/step - loss: 0.2748 - val_loss: 0.2972\n",
      "Epoch 59/500\n",
      "691/691 [==============================] - 0s 58us/step - loss: 0.2746 - val_loss: 0.2970\n",
      "Epoch 60/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2744 - val_loss: 0.2968\n",
      "Epoch 61/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2743 - val_loss: 0.2967\n",
      "Epoch 62/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2741 - val_loss: 0.2965\n",
      "Epoch 63/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2739 - val_loss: 0.2963\n",
      "Epoch 64/500\n",
      "691/691 [==============================] - 0s 61us/step - loss: 0.2737 - val_loss: 0.2961\n",
      "Epoch 65/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2735 - val_loss: 0.2958\n",
      "Epoch 66/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2733 - val_loss: 0.2956\n",
      "Epoch 67/500\n",
      "691/691 [==============================] - 0s 61us/step - loss: 0.2731 - val_loss: 0.2954\n",
      "Epoch 68/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2729 - val_loss: 0.2952\n",
      "Epoch 69/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2727 - val_loss: 0.2950\n",
      "Epoch 70/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2725 - val_loss: 0.2948\n",
      "Epoch 71/500\n",
      "691/691 [==============================] - 0s 56us/step - loss: 0.2723 - val_loss: 0.2946\n",
      "Epoch 72/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2721 - val_loss: 0.2944\n",
      "Epoch 73/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2719 - val_loss: 0.2942\n",
      "Epoch 74/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2717 - val_loss: 0.2940\n",
      "Epoch 75/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2715 - val_loss: 0.2937\n",
      "Epoch 76/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2713 - val_loss: 0.2936\n",
      "Epoch 77/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2711 - val_loss: 0.2934\n",
      "Epoch 78/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2709 - val_loss: 0.2932\n",
      "Epoch 79/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2707 - val_loss: 0.2930\n",
      "Epoch 80/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2706 - val_loss: 0.2928\n",
      "Epoch 81/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2704 - val_loss: 0.2927\n",
      "Epoch 82/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2702 - val_loss: 0.2925\n",
      "Epoch 83/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2700 - val_loss: 0.2923\n",
      "Epoch 84/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2698 - val_loss: 0.2922\n",
      "Epoch 85/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2697 - val_loss: 0.2920\n",
      "Epoch 86/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2695 - val_loss: 0.2918\n",
      "Epoch 87/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2693 - val_loss: 0.2916\n",
      "Epoch 88/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2691 - val_loss: 0.2915\n",
      "Epoch 89/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2689 - val_loss: 0.2913\n",
      "Epoch 90/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2688 - val_loss: 0.2911\n",
      "Epoch 91/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2686 - val_loss: 0.2909\n",
      "Epoch 92/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2684 - val_loss: 0.2908\n",
      "Epoch 93/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2682 - val_loss: 0.2906\n",
      "Epoch 94/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2681 - val_loss: 0.2904\n",
      "Epoch 95/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2679 - val_loss: 0.2902\n",
      "Epoch 96/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2677 - val_loss: 0.2901\n",
      "Epoch 97/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2675 - val_loss: 0.2899\n",
      "Epoch 98/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2674 - val_loss: 0.2897\n",
      "Epoch 99/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2672 - val_loss: 0.2895\n",
      "Epoch 100/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2670 - val_loss: 0.2894\n",
      "Epoch 101/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2668 - val_loss: 0.2892\n",
      "Epoch 102/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2667 - val_loss: 0.2890\n",
      "Epoch 103/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2665 - val_loss: 0.2888\n",
      "Epoch 104/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2663 - val_loss: 0.2887\n",
      "Epoch 105/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2662 - val_loss: 0.2885\n",
      "Epoch 106/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2660 - val_loss: 0.2883\n",
      "Epoch 107/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2658 - val_loss: 0.2882\n",
      "Epoch 108/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2656 - val_loss: 0.2880\n",
      "Epoch 109/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2655 - val_loss: 0.2878\n",
      "Epoch 110/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2653 - val_loss: 0.2876\n",
      "Epoch 111/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2651 - val_loss: 0.2874\n",
      "Epoch 112/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2649 - val_loss: 0.2871\n",
      "Epoch 113/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2646 - val_loss: 0.2867\n",
      "Epoch 114/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2643 - val_loss: 0.2863\n",
      "Epoch 115/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2638 - val_loss: 0.2856\n",
      "Epoch 116/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2632 - val_loss: 0.2850\n",
      "Epoch 117/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2626 - val_loss: 0.2845\n",
      "Epoch 118/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2622 - val_loss: 0.2840\n",
      "Epoch 119/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2617 - val_loss: 0.2835\n",
      "Epoch 120/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2613 - val_loss: 0.2830\n",
      "Epoch 121/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2608 - val_loss: 0.2825\n",
      "Epoch 122/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2604 - val_loss: 0.2821\n",
      "Epoch 123/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2600 - val_loss: 0.2817\n",
      "Epoch 124/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2596 - val_loss: 0.2813\n",
      "Epoch 125/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2592 - val_loss: 0.2809\n",
      "Epoch 126/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2589 - val_loss: 0.2805\n",
      "Epoch 127/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2585 - val_loss: 0.2802\n",
      "Epoch 128/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2582 - val_loss: 0.2798\n",
      "Epoch 129/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2579 - val_loss: 0.2795\n",
      "Epoch 130/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2577 - val_loss: 0.2792\n",
      "Epoch 131/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.2574 - val_loss: 0.2790\n",
      "Epoch 132/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2572 - val_loss: 0.2787\n",
      "Epoch 133/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2569 - val_loss: 0.2784\n",
      "Epoch 134/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2567 - val_loss: 0.2782\n",
      "Epoch 135/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2565 - val_loss: 0.2780\n",
      "Epoch 136/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2563 - val_loss: 0.2778\n",
      "Epoch 137/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2561 - val_loss: 0.2776\n",
      "Epoch 138/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2559 - val_loss: 0.2774\n",
      "Epoch 139/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2557 - val_loss: 0.2772\n",
      "Epoch 140/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2556 - val_loss: 0.2770\n",
      "Epoch 141/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2554 - val_loss: 0.2768\n",
      "Epoch 142/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2552 - val_loss: 0.2767\n",
      "Epoch 143/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2550 - val_loss: 0.2765\n",
      "Epoch 144/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2549 - val_loss: 0.2763\n",
      "Epoch 145/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2547 - val_loss: 0.2761\n",
      "Epoch 146/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2545 - val_loss: 0.2760\n",
      "Epoch 147/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2544 - val_loss: 0.2758\n",
      "Epoch 148/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2542 - val_loss: 0.2756\n",
      "Epoch 149/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2540 - val_loss: 0.2754\n",
      "Epoch 150/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2539 - val_loss: 0.2752\n",
      "Epoch 151/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2537 - val_loss: 0.2751\n",
      "Epoch 152/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2536 - val_loss: 0.2749\n",
      "Epoch 153/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2534 - val_loss: 0.2747\n",
      "Epoch 154/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2532 - val_loss: 0.2746\n",
      "Epoch 155/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2531 - val_loss: 0.2744\n",
      "Epoch 156/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2529 - val_loss: 0.2742\n",
      "Epoch 157/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2527 - val_loss: 0.2740\n",
      "Epoch 158/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2526 - val_loss: 0.2739\n",
      "Epoch 159/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 0s 64us/step - loss: 0.2524 - val_loss: 0.2737\n",
      "Epoch 160/500\n",
      "691/691 [==============================] - 0s 69us/step - loss: 0.2522 - val_loss: 0.2735\n",
      "Epoch 161/500\n",
      "691/691 [==============================] - 0s 71us/step - loss: 0.2521 - val_loss: 0.2733\n",
      "Epoch 162/500\n",
      "691/691 [==============================] - 0s 59us/step - loss: 0.2519 - val_loss: 0.2731\n",
      "Epoch 163/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2517 - val_loss: 0.2729\n",
      "Epoch 164/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2515 - val_loss: 0.2727\n",
      "Epoch 165/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2513 - val_loss: 0.2725\n",
      "Epoch 166/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2510 - val_loss: 0.2722\n",
      "Epoch 167/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2507 - val_loss: 0.2719\n",
      "Epoch 168/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2504 - val_loss: 0.2717\n",
      "Epoch 169/500\n",
      "691/691 [==============================] - 0s 61us/step - loss: 0.2501 - val_loss: 0.2713\n",
      "Epoch 170/500\n",
      "691/691 [==============================] - 0s 71us/step - loss: 0.2497 - val_loss: 0.2710\n",
      "Epoch 171/500\n",
      "691/691 [==============================] - 0s 58us/step - loss: 0.2494 - val_loss: 0.2707\n",
      "Epoch 172/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2490 - val_loss: 0.2703\n",
      "Epoch 173/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2486 - val_loss: 0.2699\n",
      "Epoch 174/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2482 - val_loss: 0.2695\n",
      "Epoch 175/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2477 - val_loss: 0.2691\n",
      "Epoch 176/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2472 - val_loss: 0.2686\n",
      "Epoch 177/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2467 - val_loss: 0.2681\n",
      "Epoch 178/500\n",
      "691/691 [==============================] - 0s 59us/step - loss: 0.2462 - val_loss: 0.2676\n",
      "Epoch 179/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2456 - val_loss: 0.2670\n",
      "Epoch 180/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2450 - val_loss: 0.2664\n",
      "Epoch 181/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2443 - val_loss: 0.2658\n",
      "Epoch 182/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2435 - val_loss: 0.2650\n",
      "Epoch 183/500\n",
      "691/691 [==============================] - 0s 58us/step - loss: 0.2427 - val_loss: 0.2642\n",
      "Epoch 184/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2417 - val_loss: 0.2633\n",
      "Epoch 185/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2406 - val_loss: 0.2623\n",
      "Epoch 186/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2395 - val_loss: 0.2613\n",
      "Epoch 187/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2383 - val_loss: 0.2603\n",
      "Epoch 188/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2370 - val_loss: 0.2592\n",
      "Epoch 189/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2357 - val_loss: 0.2581\n",
      "Epoch 190/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.2345 - val_loss: 0.2571\n",
      "Epoch 191/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2332 - val_loss: 0.2561\n",
      "Epoch 192/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2320 - val_loss: 0.2551\n",
      "Epoch 193/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2309 - val_loss: 0.2542\n",
      "Epoch 194/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2299 - val_loss: 0.2533\n",
      "Epoch 195/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.2290 - val_loss: 0.2525\n",
      "Epoch 196/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2281 - val_loss: 0.2517\n",
      "Epoch 197/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.2272 - val_loss: 0.2509\n",
      "Epoch 198/500\n",
      "691/691 [==============================] - 0s 43us/step - loss: 0.2265 - val_loss: 0.2502\n",
      "Epoch 199/500\n",
      "691/691 [==============================] - 0s 61us/step - loss: 0.2257 - val_loss: 0.2496\n",
      "Epoch 200/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2251 - val_loss: 0.2490\n",
      "Epoch 201/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2244 - val_loss: 0.2484\n",
      "Epoch 202/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2238 - val_loss: 0.2478\n",
      "Epoch 203/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2233 - val_loss: 0.2473\n",
      "Epoch 204/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2227 - val_loss: 0.2468\n",
      "Epoch 205/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2222 - val_loss: 0.2464\n",
      "Epoch 206/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2218 - val_loss: 0.2459\n",
      "Epoch 207/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2213 - val_loss: 0.2455\n",
      "Epoch 208/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.2209 - val_loss: 0.2451\n",
      "Epoch 209/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.2204 - val_loss: 0.2447\n",
      "Epoch 210/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.2200 - val_loss: 0.2443\n",
      "Epoch 211/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2197 - val_loss: 0.2439\n",
      "Epoch 212/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2193 - val_loss: 0.2436\n",
      "Epoch 213/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2189 - val_loss: 0.2432\n",
      "Epoch 214/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2186 - val_loss: 0.2429\n",
      "Epoch 215/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2183 - val_loss: 0.2426\n",
      "Epoch 216/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2179 - val_loss: 0.2423\n",
      "Epoch 217/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2176 - val_loss: 0.2420\n",
      "Epoch 218/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2173 - val_loss: 0.2417\n",
      "Epoch 219/500\n",
      "691/691 [==============================] - 0s 58us/step - loss: 0.2170 - val_loss: 0.2415\n",
      "Epoch 220/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2168 - val_loss: 0.2412\n",
      "Epoch 221/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2165 - val_loss: 0.2409\n",
      "Epoch 222/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2162 - val_loss: 0.2407\n",
      "Epoch 223/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2160 - val_loss: 0.2404\n",
      "Epoch 224/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2157 - val_loss: 0.2402\n",
      "Epoch 225/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2155 - val_loss: 0.2399\n",
      "Epoch 226/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2152 - val_loss: 0.2397\n",
      "Epoch 227/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2150 - val_loss: 0.2395\n",
      "Epoch 228/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2148 - val_loss: 0.2392\n",
      "Epoch 229/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2146 - val_loss: 0.2390\n",
      "Epoch 230/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2143 - val_loss: 0.2388\n",
      "Epoch 231/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2141 - val_loss: 0.2386\n",
      "Epoch 232/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2139 - val_loss: 0.2384\n",
      "Epoch 233/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2137 - val_loss: 0.2382\n",
      "Epoch 234/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2135 - val_loss: 0.2380\n",
      "Epoch 235/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2133 - val_loss: 0.2379\n",
      "Epoch 236/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.2131 - val_loss: 0.2377\n",
      "Epoch 237/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2129 - val_loss: 0.2375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2127 - val_loss: 0.2373\n",
      "Epoch 239/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2125 - val_loss: 0.2371\n",
      "Epoch 240/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2123 - val_loss: 0.2369\n",
      "Epoch 241/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2121 - val_loss: 0.2367\n",
      "Epoch 242/500\n",
      "691/691 [==============================] - ETA: 0s - loss: 0.248 - 0s 49us/step - loss: 0.2119 - val_loss: 0.2365\n",
      "Epoch 243/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2117 - val_loss: 0.2364\n",
      "Epoch 244/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.2115 - val_loss: 0.2362\n",
      "Epoch 245/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.2113 - val_loss: 0.2359\n",
      "Epoch 246/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2110 - val_loss: 0.2357\n",
      "Epoch 247/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2108 - val_loss: 0.2355\n",
      "Epoch 248/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2106 - val_loss: 0.2353\n",
      "Epoch 249/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2104 - val_loss: 0.2351\n",
      "Epoch 250/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.2102 - val_loss: 0.2349\n",
      "Epoch 251/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2100 - val_loss: 0.2347\n",
      "Epoch 252/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2098 - val_loss: 0.2344\n",
      "Epoch 253/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2095 - val_loss: 0.2342\n",
      "Epoch 254/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2093 - val_loss: 0.2340\n",
      "Epoch 255/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2091 - val_loss: 0.2338\n",
      "Epoch 256/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2090 - val_loss: 0.2336\n",
      "Epoch 257/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2088 - val_loss: 0.2334\n",
      "Epoch 258/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2086 - val_loss: 0.2332\n",
      "Epoch 259/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2084 - val_loss: 0.2330\n",
      "Epoch 260/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2082 - val_loss: 0.2328\n",
      "Epoch 261/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2080 - val_loss: 0.2326\n",
      "Epoch 262/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.2078 - val_loss: 0.2324\n",
      "Epoch 263/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2077 - val_loss: 0.2322\n",
      "Epoch 264/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.2075 - val_loss: 0.2320\n",
      "Epoch 265/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2073 - val_loss: 0.2318\n",
      "Epoch 266/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2072 - val_loss: 0.2317\n",
      "Epoch 267/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2070 - val_loss: 0.2315\n",
      "Epoch 268/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2068 - val_loss: 0.2313\n",
      "Epoch 269/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2067 - val_loss: 0.2312\n",
      "Epoch 270/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2065 - val_loss: 0.2310\n",
      "Epoch 271/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.2064 - val_loss: 0.2308\n",
      "Epoch 272/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.2062 - val_loss: 0.2307\n",
      "Epoch 273/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2061 - val_loss: 0.2305\n",
      "Epoch 274/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.2059 - val_loss: 0.2303\n",
      "Epoch 275/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2058 - val_loss: 0.2302\n",
      "Epoch 276/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2057 - val_loss: 0.2301\n",
      "Epoch 277/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2055 - val_loss: 0.2299\n",
      "Epoch 278/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2054 - val_loss: 0.2298\n",
      "Epoch 279/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2053 - val_loss: 0.2296\n",
      "Epoch 280/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2051 - val_loss: 0.2295\n",
      "Epoch 281/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2050 - val_loss: 0.2294\n",
      "Epoch 282/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2049 - val_loss: 0.2293\n",
      "Epoch 283/500\n",
      "691/691 [==============================] - 0s 56us/step - loss: 0.2048 - val_loss: 0.2291\n",
      "Epoch 284/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2047 - val_loss: 0.2290\n",
      "Epoch 285/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2045 - val_loss: 0.2289\n",
      "Epoch 286/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2044 - val_loss: 0.2287\n",
      "Epoch 287/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2043 - val_loss: 0.2286\n",
      "Epoch 288/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2042 - val_loss: 0.2285\n",
      "Epoch 289/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2041 - val_loss: 0.2283\n",
      "Epoch 290/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2039 - val_loss: 0.2282\n",
      "Epoch 291/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2038 - val_loss: 0.2280\n",
      "Epoch 292/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2036 - val_loss: 0.2279\n",
      "Epoch 293/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2035 - val_loss: 0.2277\n",
      "Epoch 294/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2034 - val_loss: 0.2276\n",
      "Epoch 295/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2032 - val_loss: 0.2274\n",
      "Epoch 296/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.2031 - val_loss: 0.2272\n",
      "Epoch 297/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2029 - val_loss: 0.2271\n",
      "Epoch 298/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2028 - val_loss: 0.2269\n",
      "Epoch 299/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2027 - val_loss: 0.2268\n",
      "Epoch 300/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2025 - val_loss: 0.2266\n",
      "Epoch 301/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2024 - val_loss: 0.2265\n",
      "Epoch 302/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2023 - val_loss: 0.2263\n",
      "Epoch 303/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2021 - val_loss: 0.2262\n",
      "Epoch 304/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2020 - val_loss: 0.2260\n",
      "Epoch 305/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2018 - val_loss: 0.2258\n",
      "Epoch 306/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2017 - val_loss: 0.2257\n",
      "Epoch 307/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2016 - val_loss: 0.2256\n",
      "Epoch 308/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2014 - val_loss: 0.2254\n",
      "Epoch 309/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2013 - val_loss: 0.2253\n",
      "Epoch 310/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.2012 - val_loss: 0.2252\n",
      "Epoch 311/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2010 - val_loss: 0.2250\n",
      "Epoch 312/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2009 - val_loss: 0.2249\n",
      "Epoch 313/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.2008 - val_loss: 0.2248\n",
      "Epoch 314/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2007 - val_loss: 0.2246\n",
      "Epoch 315/500\n",
      "691/691 [==============================] - 0s 55us/step - loss: 0.2005 - val_loss: 0.2245\n",
      "Epoch 316/500\n",
      "691/691 [==============================] - 0s 43us/step - loss: 0.2004 - val_loss: 0.2244\n",
      "Epoch 317/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2003 - val_loss: 0.2243\n",
      "Epoch 318/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.2002 - val_loss: 0.2242\n",
      "Epoch 319/500\n",
      "691/691 [==============================] - 0s 56us/step - loss: 0.2001 - val_loss: 0.2240\n",
      "Epoch 320/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.2000 - val_loss: 0.2239\n",
      "Epoch 321/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1999 - val_loss: 0.2238\n",
      "Epoch 322/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1998 - val_loss: 0.2237\n",
      "Epoch 323/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1997 - val_loss: 0.2236\n",
      "Epoch 324/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1996 - val_loss: 0.2235\n",
      "Epoch 325/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1995 - val_loss: 0.2234\n",
      "Epoch 326/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1994 - val_loss: 0.2233\n",
      "Epoch 327/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.1993 - val_loss: 0.2232\n",
      "Epoch 328/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.1992 - val_loss: 0.2231\n",
      "Epoch 329/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.1991 - val_loss: 0.2230\n",
      "Epoch 330/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1990 - val_loss: 0.2229\n",
      "Epoch 331/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.1989 - val_loss: 0.2228\n",
      "Epoch 332/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.1988 - val_loss: 0.2228\n",
      "Epoch 333/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.1987 - val_loss: 0.2227\n",
      "Epoch 334/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1987 - val_loss: 0.2226\n",
      "Epoch 335/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1986 - val_loss: 0.2225\n",
      "Epoch 336/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.1985 - val_loss: 0.2224\n",
      "Epoch 337/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1984 - val_loss: 0.2224\n",
      "Epoch 338/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1983 - val_loss: 0.2223\n",
      "Epoch 339/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1983 - val_loss: 0.2222\n",
      "Epoch 340/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1982 - val_loss: 0.2222\n",
      "Epoch 341/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1981 - val_loss: 0.2221\n",
      "Epoch 342/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.1981 - val_loss: 0.2220\n",
      "Epoch 343/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1980 - val_loss: 0.2219\n",
      "Epoch 344/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1979 - val_loss: 0.2219\n",
      "Epoch 345/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1979 - val_loss: 0.2218\n",
      "Epoch 346/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1978 - val_loss: 0.2218\n",
      "Epoch 347/500\n",
      "691/691 [==============================] - 0s 43us/step - loss: 0.1978 - val_loss: 0.2217\n",
      "Epoch 348/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1977 - val_loss: 0.2217\n",
      "Epoch 349/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1977 - val_loss: 0.2216\n",
      "Epoch 350/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1976 - val_loss: 0.2216\n",
      "Epoch 351/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1975 - val_loss: 0.2215\n",
      "Epoch 352/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.1975 - val_loss: 0.2214\n",
      "Epoch 353/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.1974 - val_loss: 0.2214\n",
      "Epoch 354/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.1974 - val_loss: 0.2213\n",
      "Epoch 355/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.1973 - val_loss: 0.2213\n",
      "Epoch 356/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1973 - val_loss: 0.2213\n",
      "Epoch 357/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1972 - val_loss: 0.2212\n",
      "Epoch 358/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.1972 - val_loss: 0.2211\n",
      "Epoch 359/500\n",
      "691/691 [==============================] - 0s 54us/step - loss: 0.1971 - val_loss: 0.2211\n",
      "Epoch 360/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.1971 - val_loss: 0.2211\n",
      "Epoch 361/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1970 - val_loss: 0.2210\n",
      "Epoch 362/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1970 - val_loss: 0.2209\n",
      "Epoch 363/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1970 - val_loss: 0.2209\n",
      "Epoch 364/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1969 - val_loss: 0.2209\n",
      "Epoch 365/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1969 - val_loss: 0.2208\n",
      "Epoch 366/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1968 - val_loss: 0.2208\n",
      "Epoch 367/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1968 - val_loss: 0.2207\n",
      "Epoch 368/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1967 - val_loss: 0.2207\n",
      "Epoch 369/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.1967 - val_loss: 0.2206\n",
      "Epoch 370/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1967 - val_loss: 0.2206\n",
      "Epoch 371/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1966 - val_loss: 0.2205\n",
      "Epoch 372/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.1966 - val_loss: 0.2205\n",
      "Epoch 373/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1965 - val_loss: 0.2205\n",
      "Epoch 374/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1965 - val_loss: 0.2205\n",
      "Epoch 375/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1964 - val_loss: 0.2204\n",
      "Epoch 376/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1964 - val_loss: 0.2203\n",
      "Epoch 377/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.1964 - val_loss: 0.2203\n",
      "Epoch 378/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.1963 - val_loss: 0.2203\n",
      "Epoch 379/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1963 - val_loss: 0.2203\n",
      "Epoch 380/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1963 - val_loss: 0.2202\n",
      "Epoch 381/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1962 - val_loss: 0.2202\n",
      "Epoch 382/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1962 - val_loss: 0.2201\n",
      "Epoch 383/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1961 - val_loss: 0.2201\n",
      "Epoch 384/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.1961 - val_loss: 0.2200\n",
      "Epoch 385/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.1961 - val_loss: 0.2200\n",
      "Epoch 386/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1960 - val_loss: 0.2200\n",
      "Epoch 387/500\n",
      "691/691 [==============================] - 0s 52us/step - loss: 0.1960 - val_loss: 0.2199\n",
      "Epoch 388/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.1960 - val_loss: 0.2199\n",
      "Epoch 389/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1959 - val_loss: 0.2199\n",
      "Epoch 390/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1959 - val_loss: 0.2198\n",
      "Epoch 391/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1959 - val_loss: 0.2198\n",
      "Epoch 392/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1958 - val_loss: 0.2198\n",
      "Epoch 393/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1958 - val_loss: 0.2197\n",
      "Epoch 394/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1958 - val_loss: 0.2197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1957 - val_loss: 0.2197\n",
      "Epoch 396/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1957 - val_loss: 0.2197\n",
      "Epoch 397/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1957 - val_loss: 0.2196\n",
      "Epoch 398/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1956 - val_loss: 0.2196\n",
      "Epoch 399/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1956 - val_loss: 0.2195\n",
      "Epoch 400/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1956 - val_loss: 0.2195\n",
      "Epoch 401/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1956 - val_loss: 0.2195\n",
      "Epoch 402/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1955 - val_loss: 0.2195\n",
      "Epoch 403/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1955 - val_loss: 0.2194\n",
      "Epoch 404/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1955 - val_loss: 0.2194\n",
      "Epoch 405/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1954 - val_loss: 0.2194\n",
      "Epoch 406/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1954 - val_loss: 0.2194\n",
      "Epoch 407/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1954 - val_loss: 0.2193\n",
      "Epoch 408/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1954 - val_loss: 0.2193\n",
      "Epoch 409/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1953 - val_loss: 0.2193\n",
      "Epoch 410/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1953 - val_loss: 0.2193\n",
      "Epoch 411/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1953 - val_loss: 0.2192\n",
      "Epoch 412/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1952 - val_loss: 0.2192\n",
      "Epoch 413/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1952 - val_loss: 0.2192\n",
      "Epoch 414/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1952 - val_loss: 0.2192\n",
      "Epoch 415/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1952 - val_loss: 0.2191\n",
      "Epoch 416/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1952 - val_loss: 0.2191\n",
      "Epoch 417/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1951 - val_loss: 0.2191\n",
      "Epoch 418/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1951 - val_loss: 0.2190\n",
      "Epoch 419/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1951 - val_loss: 0.2190\n",
      "Epoch 420/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1950 - val_loss: 0.2190\n",
      "Epoch 421/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1950 - val_loss: 0.2190\n",
      "Epoch 422/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1950 - val_loss: 0.2189\n",
      "Epoch 423/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1950 - val_loss: 0.2189\n",
      "Epoch 424/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1949 - val_loss: 0.2189\n",
      "Epoch 425/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1949 - val_loss: 0.2189\n",
      "Epoch 426/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1949 - val_loss: 0.2189\n",
      "Epoch 427/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1949 - val_loss: 0.2188\n",
      "Epoch 428/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1949 - val_loss: 0.2188\n",
      "Epoch 429/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1948 - val_loss: 0.2188\n",
      "Epoch 430/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1948 - val_loss: 0.2187\n",
      "Epoch 431/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1948 - val_loss: 0.2187\n",
      "Epoch 432/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1948 - val_loss: 0.2187\n",
      "Epoch 433/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1947 - val_loss: 0.2187\n",
      "Epoch 434/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1947 - val_loss: 0.2187\n",
      "Epoch 435/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1947 - val_loss: 0.2187\n",
      "Epoch 436/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1947 - val_loss: 0.2186\n",
      "Epoch 437/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1946 - val_loss: 0.2186\n",
      "Epoch 438/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1946 - val_loss: 0.2186\n",
      "Epoch 439/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1946 - val_loss: 0.2186\n",
      "Epoch 440/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1946 - val_loss: 0.2185\n",
      "Epoch 441/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1946 - val_loss: 0.2185\n",
      "Epoch 442/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1945 - val_loss: 0.2185\n",
      "Epoch 443/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1945 - val_loss: 0.2185\n",
      "Epoch 444/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1945 - val_loss: 0.2184\n",
      "Epoch 445/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1945 - val_loss: 0.2184\n",
      "Epoch 446/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1944 - val_loss: 0.2184\n",
      "Epoch 447/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1944 - val_loss: 0.2184\n",
      "Epoch 448/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1944 - val_loss: 0.2183\n",
      "Epoch 449/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1944 - val_loss: 0.2183\n",
      "Epoch 450/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1944 - val_loss: 0.2183\n",
      "Epoch 451/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1943 - val_loss: 0.2183\n",
      "Epoch 452/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1943 - val_loss: 0.2183\n",
      "Epoch 453/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1943 - val_loss: 0.2182\n",
      "Epoch 454/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1943 - val_loss: 0.2182\n",
      "Epoch 455/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1943 - val_loss: 0.2182\n",
      "Epoch 456/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1942 - val_loss: 0.2182\n",
      "Epoch 457/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1942 - val_loss: 0.2182\n",
      "Epoch 458/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1942 - val_loss: 0.2181\n",
      "Epoch 459/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1942 - val_loss: 0.2181\n",
      "Epoch 460/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1942 - val_loss: 0.2181\n",
      "Epoch 461/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1941 - val_loss: 0.2181\n",
      "Epoch 462/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1941 - val_loss: 0.2180\n",
      "Epoch 463/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1941 - val_loss: 0.2180\n",
      "Epoch 464/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1941 - val_loss: 0.2180\n",
      "Epoch 465/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1940 - val_loss: 0.2180\n",
      "Epoch 466/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1940 - val_loss: 0.2179\n",
      "Epoch 467/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1940 - val_loss: 0.2179\n",
      "Epoch 468/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1940 - val_loss: 0.2179\n",
      "Epoch 469/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1940 - val_loss: 0.2179\n",
      "Epoch 470/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1939 - val_loss: 0.2179\n",
      "Epoch 471/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1939 - val_loss: 0.2179\n",
      "Epoch 472/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1939 - val_loss: 0.2178\n",
      "Epoch 473/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1939 - val_loss: 0.2178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 474/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1939 - val_loss: 0.2178\n",
      "Epoch 475/500\n",
      "691/691 [==============================] - 0s 48us/step - loss: 0.1938 - val_loss: 0.2178\n",
      "Epoch 476/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1938 - val_loss: 0.2178\n",
      "Epoch 477/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1938 - val_loss: 0.2177\n",
      "Epoch 478/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1938 - val_loss: 0.2177\n",
      "Epoch 479/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1938 - val_loss: 0.2177\n",
      "Epoch 480/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1937 - val_loss: 0.2177\n",
      "Epoch 481/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1937 - val_loss: 0.2177\n",
      "Epoch 482/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1937 - val_loss: 0.2177\n",
      "Epoch 483/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1937 - val_loss: 0.2176\n",
      "Epoch 484/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1937 - val_loss: 0.2176\n",
      "Epoch 485/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1936 - val_loss: 0.2176\n",
      "Epoch 486/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1936 - val_loss: 0.2176\n",
      "Epoch 487/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1936 - val_loss: 0.2176\n",
      "Epoch 488/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1936 - val_loss: 0.2176\n",
      "Epoch 489/500\n",
      "691/691 [==============================] - 0s 49us/step - loss: 0.1936 - val_loss: 0.2175\n",
      "Epoch 490/500\n",
      "691/691 [==============================] - 0s 46us/step - loss: 0.1935 - val_loss: 0.2175\n",
      "Epoch 491/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1935 - val_loss: 0.2175\n",
      "Epoch 492/500\n",
      "691/691 [==============================] - 0s 51us/step - loss: 0.1935 - val_loss: 0.2175\n",
      "Epoch 493/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1935 - val_loss: 0.2175\n",
      "Epoch 494/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1935 - val_loss: 0.2174\n",
      "Epoch 495/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1934 - val_loss: 0.2174\n",
      "Epoch 496/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1934 - val_loss: 0.2174\n",
      "Epoch 497/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1934 - val_loss: 0.2174\n",
      "Epoch 498/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1934 - val_loss: 0.2174\n",
      "Epoch 499/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1934 - val_loss: 0.2174\n",
      "Epoch 500/500\n",
      "691/691 [==============================] - 0s 45us/step - loss: 0.1934 - val_loss: 0.2173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10252cc0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(X_Train, X_Train,\n",
    "                epochs=500,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_CV, X_CV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = encoder.predict(X_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = encoder.predict(X_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logregtest = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "logregtest.fit(test2, Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enc = logregtest.predict(test2)\n",
    "cv_enc = logregtest.predict(cv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Resuts\n",
      "Accuracy =  79.73950795947901\n",
      "F1 Score =  0.7348484848484849\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True        194.0          67.0\n",
      "1  Pred False         73.0         357.0\n",
      "-------------------------------\n",
      "Cross Validation Resuts\n",
      "Accuracy =  80.0\n",
      "F1 Score =  0.7297297297297296\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True         54.0          19.0\n",
      "1  Pred False         21.0         106.0\n"
     ]
    }
   ],
   "source": [
    "print('Training Resuts')  \n",
    "show_acc(Y_Train, train_enc)\n",
    "print('-------------------------------')\n",
    "print('Cross Validation Resuts')  \n",
    "show_acc(Y_CV, cv_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Data\n",
    "\n",
    "So from some initial experimentation a simple way to measure the effectiveness will be to convert all of my input fields into binary so that I can simply use binary classification accuracy to measure the true accuracy as opposed to some loss/error calculation function.  \n",
    "\n",
    "Plus having every input as binary means the corruption step can be a simple random probability of a bit flip, as opposed to Gaussian noise or some other noise schema.\n",
    "\n",
    "However, this means that some of the fields we have will need some transformation.  For simplicity I will band the name length and fare fields and then apply one hot encodings to these bands, as well as remaining categorical fields.  This will result in a matrix with only binary elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Name</th>\n",
       "      <th>Parch</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Name_length</th>\n",
       "      <th>Company</th>\n",
       "      <th>Norm_age</th>\n",
       "      <th>Norm_fare</th>\n",
       "      <th>canc</th>\n",
       "      <th>Deckstr</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.090286</td>\n",
       "      <td>-0.050800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Mr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>0.110140</td>\n",
       "      <td>0.074185</td>\n",
       "      <td>C85</td>\n",
       "      <td>3</td>\n",
       "      <td>Mrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.040180</td>\n",
       "      <td>-0.049482</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>113803</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0.072560</td>\n",
       "      <td>0.038693</td>\n",
       "      <td>C123</td>\n",
       "      <td>3</td>\n",
       "      <td>Mrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>373450</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072560</td>\n",
       "      <td>-0.049238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Mr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>330877</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.065233</td>\n",
       "      <td>-0.048441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Mr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>54.0</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17463</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.310566</td>\n",
       "      <td>0.036278</td>\n",
       "      <td>E46</td>\n",
       "      <td>5</td>\n",
       "      <td>Mr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>349909</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.340818</td>\n",
       "      <td>-0.023815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Master</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>347742</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.027653</td>\n",
       "      <td>-0.043220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Mrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>237736</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.190499</td>\n",
       "      <td>-0.006257</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Mrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age Cabin Embarked     Fare  \\\n",
       "0  22.0   NaN        S   7.2500   \n",
       "1  38.0   C85        C  71.2833   \n",
       "2  26.0   NaN        S   7.9250   \n",
       "3  35.0  C123        S  53.1000   \n",
       "4  35.0   NaN        S   8.0500   \n",
       "5  24.0   NaN        Q   8.4583   \n",
       "6  54.0   E46        S  51.8625   \n",
       "7   2.0   NaN        S  21.0750   \n",
       "8  27.0   NaN        S  11.1333   \n",
       "9  14.0   NaN        C  30.0708   \n",
       "\n",
       "                                                Name  Parch  PassengerId  \\\n",
       "0                            Braund, Mr. Owen Harris      0            1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...      0            2   \n",
       "2                             Heikkinen, Miss. Laina      0            3   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)      0            4   \n",
       "4                           Allen, Mr. William Henry      0            5   \n",
       "5                                   Moran, Mr. James      0            6   \n",
       "6                            McCarthy, Mr. Timothy J      0            7   \n",
       "7                     Palsson, Master. Gosta Leonard      1            8   \n",
       "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)      2            9   \n",
       "9                Nasser, Mrs. Nicholas (Adele Achem)      0           10   \n",
       "\n",
       "   Pclass  Sex  SibSp  Survived            Ticket  Name_length  Company  \\\n",
       "0       3    1      1       0.0         A/5 21171           23        1   \n",
       "1       1    0      1       1.0          PC 17599           51        1   \n",
       "2       3    0      0       1.0  STON/O2. 3101282           22        0   \n",
       "3       1    0      1       1.0            113803           44        1   \n",
       "4       3    1      0       0.0            373450           24        0   \n",
       "5       3    1      0       0.0            330877           16        0   \n",
       "6       1    1      0       0.0             17463           23        0   \n",
       "7       3    1      3       0.0            349909           30        4   \n",
       "8       3    0      0       1.0            347742           49        2   \n",
       "9       2    0      1       1.0            237736           35        1   \n",
       "\n",
       "   Norm_age  Norm_fare  canc  Deckstr   Title  \n",
       "0 -0.090286  -0.050800   NaN        0      Mr  \n",
       "1  0.110140   0.074185   C85        3     Mrs  \n",
       "2 -0.040180  -0.049482   NaN        0    Miss  \n",
       "3  0.072560   0.038693  C123        3     Mrs  \n",
       "4  0.072560  -0.049238   NaN        0      Mr  \n",
       "5 -0.065233  -0.048441   NaN        0      Mr  \n",
       "6  0.310566   0.036278   E46        5      Mr  \n",
       "7 -0.340818  -0.023815   NaN        0  Master  \n",
       "8 -0.027653  -0.043220   NaN        0     Mrs  \n",
       "9 -0.190499  -0.006257   NaN        0     Mrs  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "append_set.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xf3d1e80>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "append_set['Fare'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above its clear to see there is a massive clustering in the lower fare ranges with a remarkable number at 0.  My best guess as to the reason for this is the staff not having to pay a fare.\n",
    "\n",
    "From this the obvious bands are - \n",
    "1. 0\n",
    "2. 1 - 50\n",
    "3. 51 - 100\n",
    "4. 101 - 200\n",
    "5. 200+\n",
    "\n",
    "### Name Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xf45e860>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "append_set['Name_length'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this seems to be somewhat bell shaped with the majority being somewhere between 20 and 30 characters in length.  Splitting along the clear divides in the above chart yields - \n",
    "\n",
    "1. <19 chars\n",
    "2. 19 - 33 chars\n",
    "3. 33+ chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function for splitting Fare\n",
    "def Split_Fare(row) :\n",
    "    if row['Fare'] == 0 :\n",
    "        return 1\n",
    "    elif row['Fare'] <= 50 :\n",
    "        return 2\n",
    "    elif row['Fare'] <= 100 :\n",
    "        return 3\n",
    "    elif row['Fare'] <= 200 :\n",
    "        return 4\n",
    "    else :\n",
    "        return 5\n",
    "\n",
    "#Create function to split name\n",
    "def Split_Name(row) :\n",
    "    if row['Name_length'] <= 19 :\n",
    "        return 0\n",
    "    elif row['Name_length'] <= 33 :\n",
    "        return 1\n",
    "    else :\n",
    "        return 2\n",
    "    \n",
    "    \n",
    "    \n",
    "#Creating our Training Set\n",
    "def Cleanse_Data_AutoEnc(df_in):\n",
    "    #Put our dataframe into new object to avoid corrupting original dataframe\n",
    "    test_set = df_in\n",
    "    \n",
    "    test_set['Fare_Cat'] = test_set.apply(Split_Fare, axis=1)\n",
    "    test_set['Name_Cat'] = test_set.apply(Split_Name, axis=1)\n",
    "    \n",
    "    emb_set = pd.get_dummies(test_set.Embarked, prefix='Emb', dummy_na = False)\n",
    "    title_set = pd.get_dummies(test_set.Title, prefix='ti', dummy_na = True)\n",
    "    deck_set = pd.get_dummies(test_set.Deckstr, prefix='de', dummy_na = True)\n",
    "    fare_set = pd.get_dummies(test_set.Fare_Cat, prefix='Fare', dummy_na = False)\n",
    "    name_set = pd.get_dummies(test_set.Name_Cat, prefix='Name', dummy_na = False)\n",
    "    class_set = pd.get_dummies(test_set.Pclass, prefix='Class', dummy_na = False)\n",
    "    comp_set = pd.get_dummies(test_set.Company, prefix='Comp', dummy_na = False)\n",
    "    \n",
    "    oh_set = pd.concat([test_set,  \n",
    "                        emb_set, \n",
    "                        title_set, \n",
    "                        deck_set,\n",
    "                        fare_set,\n",
    "                        name_set, \n",
    "                        class_set], axis=1)\n",
    "    \n",
    "    #Create output fully numeric dataframe\n",
    "    out_set = oh_set.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', \n",
    "                             'Cabin', 'canc', 'Embarked', 'Title', 'Norm_age', 'Norm_fare', \n",
    "                           'Company', 'Pclass', 'Fare_Cat', 'Name_Cat',\n",
    "                           'Deckstr', 'Age', 'Fare', 'Name_length'], axis=1)\n",
    "    return out_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_set = Cleanse_Data_AutoEnc(append_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Emb_C</th>\n",
       "      <th>Emb_Q</th>\n",
       "      <th>Emb_S</th>\n",
       "      <th>ti_Master</th>\n",
       "      <th>ti_Miss</th>\n",
       "      <th>ti_Mr</th>\n",
       "      <th>ti_Mrs</th>\n",
       "      <th>ti_Rare</th>\n",
       "      <th>ti_nan</th>\n",
       "      <th>de_0.0</th>\n",
       "      <th>de_1.0</th>\n",
       "      <th>de_2.0</th>\n",
       "      <th>de_3.0</th>\n",
       "      <th>de_4.0</th>\n",
       "      <th>de_5.0</th>\n",
       "      <th>de_6.0</th>\n",
       "      <th>de_7.0</th>\n",
       "      <th>de_8.0</th>\n",
       "      <th>de_nan</th>\n",
       "      <th>Fare_1</th>\n",
       "      <th>Fare_2</th>\n",
       "      <th>Fare_3</th>\n",
       "      <th>Fare_4</th>\n",
       "      <th>Fare_5</th>\n",
       "      <th>Name_0</th>\n",
       "      <th>Name_1</th>\n",
       "      <th>Name_2</th>\n",
       "      <th>Class_1</th>\n",
       "      <th>Class_2</th>\n",
       "      <th>Class_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sex  Survived  Emb_C  Emb_Q  Emb_S  ti_Master  ti_Miss  ti_Mr  ti_Mrs  \\\n",
       "0    1       0.0      0      0      1          0        0      1       0   \n",
       "1    0       1.0      1      0      0          0        0      0       1   \n",
       "2    0       1.0      0      0      1          0        1      0       0   \n",
       "3    0       1.0      0      0      1          0        0      0       1   \n",
       "4    1       0.0      0      0      1          0        0      1       0   \n",
       "5    1       0.0      0      1      0          0        0      1       0   \n",
       "6    1       0.0      0      0      1          0        0      1       0   \n",
       "7    1       0.0      0      0      1          1        0      0       0   \n",
       "8    0       1.0      0      0      1          0        0      0       1   \n",
       "9    0       1.0      1      0      0          0        0      0       1   \n",
       "\n",
       "   ti_Rare  ti_nan  de_0.0  de_1.0  de_2.0  de_3.0  de_4.0  de_5.0  de_6.0  \\\n",
       "0        0       0       1       0       0       0       0       0       0   \n",
       "1        0       0       0       0       0       1       0       0       0   \n",
       "2        0       0       1       0       0       0       0       0       0   \n",
       "3        0       0       0       0       0       1       0       0       0   \n",
       "4        0       0       1       0       0       0       0       0       0   \n",
       "5        0       0       1       0       0       0       0       0       0   \n",
       "6        0       0       0       0       0       0       0       1       0   \n",
       "7        0       0       1       0       0       0       0       0       0   \n",
       "8        0       0       1       0       0       0       0       0       0   \n",
       "9        0       0       1       0       0       0       0       0       0   \n",
       "\n",
       "   de_7.0  de_8.0  de_nan  Fare_1  Fare_2  Fare_3  Fare_4  Fare_5  Name_0  \\\n",
       "0       0       0       0       0       1       0       0       0       0   \n",
       "1       0       0       0       0       0       1       0       0       0   \n",
       "2       0       0       0       0       1       0       0       0       0   \n",
       "3       0       0       0       0       0       1       0       0       0   \n",
       "4       0       0       0       0       1       0       0       0       0   \n",
       "5       0       0       0       0       1       0       0       0       1   \n",
       "6       0       0       0       0       0       1       0       0       0   \n",
       "7       0       0       0       0       1       0       0       0       0   \n",
       "8       0       0       0       0       1       0       0       0       0   \n",
       "9       0       0       0       0       1       0       0       0       0   \n",
       "\n",
       "   Name_1  Name_2  Class_1  Class_2  Class_3  \n",
       "0       1       0        0        0        1  \n",
       "1       0       1        1        0        0  \n",
       "2       1       0        0        0        1  \n",
       "3       0       1        1        0        0  \n",
       "4       1       0        0        0        1  \n",
       "5       0       0        0        0        1  \n",
       "6       1       0        1        0        0  \n",
       "7       1       0        0        0        1  \n",
       "8       0       1        0        0        1  \n",
       "9       0       1        0        1        0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_set.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a fully binary X matrix, meaning we can make a start on a denoising autoencoder to learn a more efficient representation of the above.  \n",
    "\n",
    "And more importantly we can simply and easily see where our autoencoder makes mistakes and verify the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_AE_Train, Y_AE_Train, X_AE_CV, Y_AE_CV, X_AE_Test = dataset_splitter(ae_set, cv_size = 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now to build a simple Autoencoder class to dynamically create autoencoders and enable quick training/testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder_simple(object):\n",
    "    def __init__(self, encoder_layers, decoder_layers, num_params, optimizer = 'Adam', loss = 'mean_squared_error'):\n",
    "        #Initialize variables\n",
    "        self.encoder_layers = encoder_layers\n",
    "        self.decoder_layers = decoder_layers\n",
    "        self.num_params = num_params\n",
    "        \n",
    "        #Define container to take layers\n",
    "        layer_list = {}\n",
    "        \n",
    "        #Get lengths for loops\n",
    "        encoder_len = len(encoder_layers)\n",
    "        decoder_len = len(decoder_layers)\n",
    "        \n",
    "        #Define input\n",
    "        input_layer = Input(shape=(num_params,))\n",
    "        \n",
    "        #Build Encoder\n",
    "        for i in range(encoder_len - 1):\n",
    "            if i == 0 :\n",
    "                layer_list['Encoder' + str(i)] = Dense(encoder_layers[i], activation = 'relu')(input_layer)\n",
    "            else :\n",
    "                layer_list['Encoder' + str(i)] = Dense(encoder_layers[i], activation = 'relu')(layer_list['Encoder' + str(i - 1)])\n",
    "        \n",
    "        layer_list['Encoded_layer'] = Dense(encoder_layers[encoder_len - 1], activation = 'sigmoid')(layer_list['Encoder' + str(encoder_len - 2)])\n",
    "        \n",
    "        #Build Decoder\n",
    "        for j in range(decoder_len):\n",
    "            if j == 0 :\n",
    "                layer_list['Decoder' + str(j)] = Dense(decoder_layers[j], activation = 'relu')(layer_list['Encoded_layer'])\n",
    "            else :\n",
    "                layer_list['Decoder' + str(j)] = Dense(decoder_layers[j], activation = 'relu')(layer_list['Decoder' + str(j - 1)])\n",
    "        \n",
    "        layer_list['Decoded_layer'] = Dense(num_params, activation = 'sigmoid')(layer_list['Decoder' + str(decoder_len - 1)])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #Build Keras models to train/use\n",
    "        self.encoder = Model(input_layer,layer_list['Encoded_layer'])\n",
    "        self.autoencoder = Model(input_layer, layer_list['Decoded_layer'])\n",
    "        \n",
    "        self.autoencoder.compile(optimizer = optimizer, loss = loss)\n",
    "    \n",
    "    def Train_autoencoder(self, X_Train, X_CV, epoch = 500, bat_size = 64):\n",
    "        self.autoencoder.fit(X_Train, X_Train,\n",
    "                epochs=epoch,\n",
    "                batch_size=bat_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_CV, X_CV))\n",
    "    \n",
    "    def Train_Denoise_autoencoder(self,X_in, X_true, X_CV, epoch = 500, bat_size = 64):\n",
    "        self.autoencoder.fit(X_in, X_true,\n",
    "                epochs=epoch,\n",
    "                batch_size=bat_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_CV, X_CV))\n",
    "    \n",
    "    \n",
    "    def Test_Accuracy(self, X_Acc) :\n",
    "        Y_Acc = self.autoencoder.predict(X_Acc)\n",
    "        \n",
    "        \n",
    "        ae_out = normalize_predictions(Y_Acc.flatten())\n",
    "        ae_in = X_Acc.flatten()\n",
    "        \n",
    "        show_acc(ae_in, ae_out)\n",
    "    \n",
    "    def Get_Encoding(self, X_in) :\n",
    "        encode_out = self.encoder.predict(X_in)\n",
    "        \n",
    "        return encode_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_lay = [25, 20, 15, 10]\n",
    "dec_lay = [15, 20, 25]\n",
    "num_params = X_AE_Train.shape[1]\n",
    "\n",
    "\n",
    "test_ae = Autoencoder_simple(enc_lay, dec_lay, num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 741 samples, validate on 150 samples\n",
      "Epoch 1/500\n",
      "741/741 [==============================] - 0s 579us/step - loss: 0.2446 - val_loss: 0.2355\n",
      "Epoch 2/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.2294 - val_loss: 0.2204\n",
      "Epoch 3/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.2116 - val_loss: 0.1984\n",
      "Epoch 4/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.1857 - val_loss: 0.1687\n",
      "Epoch 5/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.1534 - val_loss: 0.1368\n",
      "Epoch 6/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.1236 - val_loss: 0.1143\n",
      "Epoch 7/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.1072 - val_loss: 0.1072\n",
      "Epoch 8/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.1026 - val_loss: 0.1059\n",
      "Epoch 9/500\n",
      "741/741 [==============================] - 0s 140us/step - loss: 0.1012 - val_loss: 0.1050\n",
      "Epoch 10/500\n",
      "741/741 [==============================] - 0s 150us/step - loss: 0.1003 - val_loss: 0.1040\n",
      "Epoch 11/500\n",
      "741/741 [==============================] - 0s 139us/step - loss: 0.0995 - val_loss: 0.1028\n",
      "Epoch 12/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0988 - val_loss: 0.1020\n",
      "Epoch 13/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0980 - val_loss: 0.1010\n",
      "Epoch 14/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.0971 - val_loss: 0.0997\n",
      "Epoch 15/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.0960 - val_loss: 0.0986\n",
      "Epoch 16/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0950 - val_loss: 0.0976\n",
      "Epoch 17/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0939 - val_loss: 0.0964\n",
      "Epoch 18/500\n",
      "741/741 [==============================] - 0s 138us/step - loss: 0.0928 - val_loss: 0.0951\n",
      "Epoch 19/500\n",
      "741/741 [==============================] - 0s 138us/step - loss: 0.0916 - val_loss: 0.0938\n",
      "Epoch 20/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0904 - val_loss: 0.0921\n",
      "Epoch 21/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.0887 - val_loss: 0.0900\n",
      "Epoch 22/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0866 - val_loss: 0.0873\n",
      "Epoch 23/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0840 - val_loss: 0.0845\n",
      "Epoch 24/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0812 - val_loss: 0.0819\n",
      "Epoch 25/500\n",
      "741/741 [==============================] - 0s 138us/step - loss: 0.0787 - val_loss: 0.0796\n",
      "Epoch 26/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0769 - val_loss: 0.0779\n",
      "Epoch 27/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0756 - val_loss: 0.0765\n",
      "Epoch 28/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0744 - val_loss: 0.0751\n",
      "Epoch 29/500\n",
      "741/741 [==============================] - 0s 147us/step - loss: 0.0729 - val_loss: 0.0732\n",
      "Epoch 30/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0710 - val_loss: 0.0713\n",
      "Epoch 31/500\n",
      "741/741 [==============================] - 0s 138us/step - loss: 0.0686 - val_loss: 0.0679\n",
      "Epoch 32/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0653 - val_loss: 0.0642\n",
      "Epoch 33/500\n",
      "741/741 [==============================] - 0s 138us/step - loss: 0.0621 - val_loss: 0.0615\n",
      "Epoch 34/500\n",
      "741/741 [==============================] - 0s 142us/step - loss: 0.0595 - val_loss: 0.0593\n",
      "Epoch 35/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0574 - val_loss: 0.0576\n",
      "Epoch 36/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.0557 - val_loss: 0.0559\n",
      "Epoch 37/500\n",
      "741/741 [==============================] - 0s 138us/step - loss: 0.0541 - val_loss: 0.0548\n",
      "Epoch 38/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0527 - val_loss: 0.0533\n",
      "Epoch 39/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0514 - val_loss: 0.0526\n",
      "Epoch 40/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0504 - val_loss: 0.0517\n",
      "Epoch 41/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0494 - val_loss: 0.0509\n",
      "Epoch 42/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0486 - val_loss: 0.0499\n",
      "Epoch 43/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0477 - val_loss: 0.0491\n",
      "Epoch 44/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0469 - val_loss: 0.0485\n",
      "Epoch 45/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0461 - val_loss: 0.0478\n",
      "Epoch 46/500\n",
      "741/741 [==============================] - 0s 139us/step - loss: 0.0455 - val_loss: 0.0471\n",
      "Epoch 47/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0448 - val_loss: 0.0465\n",
      "Epoch 48/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0442 - val_loss: 0.0458\n",
      "Epoch 49/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0436 - val_loss: 0.0453\n",
      "Epoch 50/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0430 - val_loss: 0.0446\n",
      "Epoch 51/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0424 - val_loss: 0.0439\n",
      "Epoch 52/500\n",
      "741/741 [==============================] - 0s 138us/step - loss: 0.0418 - val_loss: 0.0433\n",
      "Epoch 53/500\n",
      "741/741 [==============================] - 0s 139us/step - loss: 0.0412 - val_loss: 0.0428\n",
      "Epoch 54/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0407 - val_loss: 0.0421\n",
      "Epoch 55/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0401 - val_loss: 0.0416\n",
      "Epoch 56/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0395 - val_loss: 0.0410\n",
      "Epoch 57/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0391 - val_loss: 0.0405\n",
      "Epoch 58/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0385 - val_loss: 0.0403\n",
      "Epoch 59/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0381 - val_loss: 0.0394\n",
      "Epoch 60/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0375 - val_loss: 0.0391\n",
      "Epoch 61/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0372 - val_loss: 0.0388\n",
      "Epoch 62/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0367 - val_loss: 0.0382\n",
      "Epoch 63/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0363 - val_loss: 0.0381\n",
      "Epoch 64/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0359 - val_loss: 0.0376\n",
      "Epoch 65/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0355 - val_loss: 0.0372\n",
      "Epoch 66/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0351 - val_loss: 0.0371\n",
      "Epoch 67/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0348 - val_loss: 0.0365\n",
      "Epoch 68/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.0344 - val_loss: 0.0363\n",
      "Epoch 69/500\n",
      "741/741 [==============================] - 0s 138us/step - loss: 0.0340 - val_loss: 0.0359\n",
      "Epoch 70/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0338 - val_loss: 0.0358\n",
      "Epoch 71/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0334 - val_loss: 0.0355\n",
      "Epoch 72/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0331 - val_loss: 0.0351\n",
      "Epoch 73/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0328 - val_loss: 0.0351\n",
      "Epoch 74/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0326 - val_loss: 0.0349\n",
      "Epoch 75/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0322 - val_loss: 0.0345\n",
      "Epoch 76/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0318 - val_loss: 0.0340\n",
      "Epoch 77/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0316 - val_loss: 0.0339\n",
      "Epoch 78/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0312 - val_loss: 0.0335\n",
      "Epoch 79/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 131us/step - loss: 0.0306 - val_loss: 0.0334\n",
      "Epoch 80/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0302 - val_loss: 0.0331\n",
      "Epoch 81/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0299 - val_loss: 0.0326\n",
      "Epoch 82/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0294 - val_loss: 0.0323\n",
      "Epoch 83/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0288 - val_loss: 0.0317\n",
      "Epoch 84/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0283 - val_loss: 0.0314\n",
      "Epoch 85/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0280 - val_loss: 0.0311\n",
      "Epoch 86/500\n",
      "741/741 [==============================] - 0s 140us/step - loss: 0.0275 - val_loss: 0.0309\n",
      "Epoch 87/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0271 - val_loss: 0.0305\n",
      "Epoch 88/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0269 - val_loss: 0.0298\n",
      "Epoch 89/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0265 - val_loss: 0.0298\n",
      "Epoch 90/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0262 - val_loss: 0.0296\n",
      "Epoch 91/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0258 - val_loss: 0.0294\n",
      "Epoch 92/500\n",
      "741/741 [==============================] - 0s 126us/step - loss: 0.0256 - val_loss: 0.0289\n",
      "Epoch 93/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0254 - val_loss: 0.0288\n",
      "Epoch 94/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0250 - val_loss: 0.0284\n",
      "Epoch 95/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0247 - val_loss: 0.0278\n",
      "Epoch 96/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0245 - val_loss: 0.0278\n",
      "Epoch 97/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0242 - val_loss: 0.0274\n",
      "Epoch 98/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0239 - val_loss: 0.0273\n",
      "Epoch 99/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0237 - val_loss: 0.0272\n",
      "Epoch 100/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0235 - val_loss: 0.0266\n",
      "Epoch 101/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0234 - val_loss: 0.0268\n",
      "Epoch 102/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0232 - val_loss: 0.0262\n",
      "Epoch 103/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0229 - val_loss: 0.0260\n",
      "Epoch 104/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0228 - val_loss: 0.0261\n",
      "Epoch 105/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0225 - val_loss: 0.0255\n",
      "Epoch 106/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0223 - val_loss: 0.0253\n",
      "Epoch 107/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0221 - val_loss: 0.0253\n",
      "Epoch 108/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0220 - val_loss: 0.0249\n",
      "Epoch 109/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0218 - val_loss: 0.0252\n",
      "Epoch 110/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0216 - val_loss: 0.0247\n",
      "Epoch 111/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0214 - val_loss: 0.0249\n",
      "Epoch 112/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0213 - val_loss: 0.0247\n",
      "Epoch 113/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0211 - val_loss: 0.0243\n",
      "Epoch 114/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0209 - val_loss: 0.0243\n",
      "Epoch 115/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0208 - val_loss: 0.0242\n",
      "Epoch 116/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0207 - val_loss: 0.0240\n",
      "Epoch 117/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0206 - val_loss: 0.0239\n",
      "Epoch 118/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0206 - val_loss: 0.0239\n",
      "Epoch 119/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0205 - val_loss: 0.0237\n",
      "Epoch 120/500\n",
      "741/741 [==============================] - 0s 138us/step - loss: 0.0204 - val_loss: 0.0239\n",
      "Epoch 121/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0202 - val_loss: 0.0238\n",
      "Epoch 122/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0202 - val_loss: 0.0235\n",
      "Epoch 123/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0200 - val_loss: 0.0234\n",
      "Epoch 124/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0199 - val_loss: 0.0234\n",
      "Epoch 125/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0198 - val_loss: 0.0235\n",
      "Epoch 126/500\n",
      "741/741 [==============================] - 0s 138us/step - loss: 0.0198 - val_loss: 0.0234\n",
      "Epoch 127/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0197 - val_loss: 0.0232\n",
      "Epoch 128/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0197 - val_loss: 0.0235\n",
      "Epoch 129/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0197 - val_loss: 0.0233\n",
      "Epoch 130/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0196 - val_loss: 0.0231\n",
      "Epoch 131/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0195 - val_loss: 0.0235\n",
      "Epoch 132/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0195 - val_loss: 0.0234\n",
      "Epoch 133/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0196 - val_loss: 0.0235\n",
      "Epoch 134/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0195 - val_loss: 0.0231\n",
      "Epoch 135/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0193 - val_loss: 0.0228\n",
      "Epoch 136/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0193 - val_loss: 0.0230\n",
      "Epoch 137/500\n",
      "741/741 [==============================] - 0s 147us/step - loss: 0.0192 - val_loss: 0.0231\n",
      "Epoch 138/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.0192 - val_loss: 0.0226\n",
      "Epoch 139/500\n",
      "741/741 [==============================] - 0s 138us/step - loss: 0.0191 - val_loss: 0.0234\n",
      "Epoch 140/500\n",
      "741/741 [==============================] - 0s 151us/step - loss: 0.0190 - val_loss: 0.0227\n",
      "Epoch 141/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0189 - val_loss: 0.0225\n",
      "Epoch 142/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0188 - val_loss: 0.0225\n",
      "Epoch 143/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.0186 - val_loss: 0.0226\n",
      "Epoch 144/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.0186 - val_loss: 0.0226\n",
      "Epoch 145/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0185 - val_loss: 0.0226\n",
      "Epoch 146/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0185 - val_loss: 0.0224\n",
      "Epoch 147/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0185 - val_loss: 0.0224\n",
      "Epoch 148/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0184 - val_loss: 0.0223\n",
      "Epoch 149/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0184 - val_loss: 0.0225\n",
      "Epoch 150/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0183 - val_loss: 0.0226\n",
      "Epoch 151/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0182 - val_loss: 0.0223\n",
      "Epoch 152/500\n",
      "741/741 [==============================] - 0s 140us/step - loss: 0.0182 - val_loss: 0.0220\n",
      "Epoch 153/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0181 - val_loss: 0.0220\n",
      "Epoch 154/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0181 - val_loss: 0.0217\n",
      "Epoch 155/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0180 - val_loss: 0.0218\n",
      "Epoch 156/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0179 - val_loss: 0.0218\n",
      "Epoch 157/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 131us/step - loss: 0.0178 - val_loss: 0.0217\n",
      "Epoch 158/500\n",
      "741/741 [==============================] - 0s 138us/step - loss: 0.0178 - val_loss: 0.0217\n",
      "Epoch 159/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0177 - val_loss: 0.0218\n",
      "Epoch 160/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0177 - val_loss: 0.0216\n",
      "Epoch 161/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0176 - val_loss: 0.0215\n",
      "Epoch 162/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0175 - val_loss: 0.0216\n",
      "Epoch 163/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0174 - val_loss: 0.0213\n",
      "Epoch 164/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0174 - val_loss: 0.0213\n",
      "Epoch 165/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0173 - val_loss: 0.0215\n",
      "Epoch 166/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.0173 - val_loss: 0.0210\n",
      "Epoch 167/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0172 - val_loss: 0.0212\n",
      "Epoch 168/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0172 - val_loss: 0.0211\n",
      "Epoch 169/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0171 - val_loss: 0.0209\n",
      "Epoch 170/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0171 - val_loss: 0.0210\n",
      "Epoch 171/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0170 - val_loss: 0.0217\n",
      "Epoch 172/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0171 - val_loss: 0.0213\n",
      "Epoch 173/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0170 - val_loss: 0.0208\n",
      "Epoch 174/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0167 - val_loss: 0.0211\n",
      "Epoch 175/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0166 - val_loss: 0.0211\n",
      "Epoch 176/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.0165 - val_loss: 0.0206\n",
      "Epoch 177/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0165 - val_loss: 0.0208\n",
      "Epoch 178/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0164 - val_loss: 0.0210\n",
      "Epoch 179/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0163 - val_loss: 0.0209\n",
      "Epoch 180/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0162 - val_loss: 0.0206\n",
      "Epoch 181/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0162 - val_loss: 0.0208\n",
      "Epoch 182/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0162 - val_loss: 0.0206\n",
      "Epoch 183/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0163 - val_loss: 0.0207\n",
      "Epoch 184/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0163 - val_loss: 0.0206\n",
      "Epoch 185/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0162 - val_loss: 0.0207\n",
      "Epoch 186/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0160 - val_loss: 0.0204\n",
      "Epoch 187/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0160 - val_loss: 0.0208\n",
      "Epoch 188/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0160 - val_loss: 0.0203\n",
      "Epoch 189/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0160 - val_loss: 0.0207\n",
      "Epoch 190/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0160 - val_loss: 0.0205\n",
      "Epoch 191/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0159 - val_loss: 0.0204\n",
      "Epoch 192/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0159 - val_loss: 0.0201\n",
      "Epoch 193/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0157 - val_loss: 0.0207\n",
      "Epoch 194/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0157 - val_loss: 0.0207\n",
      "Epoch 195/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0156 - val_loss: 0.0199\n",
      "Epoch 196/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0156 - val_loss: 0.0197\n",
      "Epoch 197/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0157 - val_loss: 0.0204\n",
      "Epoch 198/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0157 - val_loss: 0.0197\n",
      "Epoch 199/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0155 - val_loss: 0.0201\n",
      "Epoch 200/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0154 - val_loss: 0.0198\n",
      "Epoch 201/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0154 - val_loss: 0.0201\n",
      "Epoch 202/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0154 - val_loss: 0.0197\n",
      "Epoch 203/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0154 - val_loss: 0.0195\n",
      "Epoch 204/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0152 - val_loss: 0.0196\n",
      "Epoch 205/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0152 - val_loss: 0.0196\n",
      "Epoch 206/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0151 - val_loss: 0.0194\n",
      "Epoch 207/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0150 - val_loss: 0.0198\n",
      "Epoch 208/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0150 - val_loss: 0.0196\n",
      "Epoch 209/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0151 - val_loss: 0.0198\n",
      "Epoch 210/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0150 - val_loss: 0.0194\n",
      "Epoch 211/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0149 - val_loss: 0.0191\n",
      "Epoch 212/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0148 - val_loss: 0.0195\n",
      "Epoch 213/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0148 - val_loss: 0.0192\n",
      "Epoch 214/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0148 - val_loss: 0.0195\n",
      "Epoch 215/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0146 - val_loss: 0.0194\n",
      "Epoch 216/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0146 - val_loss: 0.0192\n",
      "Epoch 217/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0145 - val_loss: 0.0194\n",
      "Epoch 218/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0145 - val_loss: 0.0192\n",
      "Epoch 219/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0145 - val_loss: 0.0194\n",
      "Epoch 220/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0145 - val_loss: 0.0193\n",
      "Epoch 221/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0145 - val_loss: 0.0192\n",
      "Epoch 222/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0143 - val_loss: 0.0193\n",
      "Epoch 223/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0143 - val_loss: 0.0193\n",
      "Epoch 224/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0143 - val_loss: 0.0191\n",
      "Epoch 225/500\n",
      "741/741 [==============================] - 0s 138us/step - loss: 0.0142 - val_loss: 0.0193\n",
      "Epoch 226/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0142 - val_loss: 0.0191\n",
      "Epoch 227/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0141 - val_loss: 0.0191\n",
      "Epoch 228/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0141 - val_loss: 0.0192\n",
      "Epoch 229/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0140 - val_loss: 0.0192\n",
      "Epoch 230/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0140 - val_loss: 0.0189\n",
      "Epoch 231/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0139 - val_loss: 0.0188\n",
      "Epoch 232/500\n",
      "741/741 [==============================] - 0s 138us/step - loss: 0.0138 - val_loss: 0.0188\n",
      "Epoch 233/500\n",
      "741/741 [==============================] - 0s 138us/step - loss: 0.0138 - val_loss: 0.0189\n",
      "Epoch 234/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0137 - val_loss: 0.0187\n",
      "Epoch 235/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 127us/step - loss: 0.0137 - val_loss: 0.0189\n",
      "Epoch 236/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0137 - val_loss: 0.0185\n",
      "Epoch 237/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0136 - val_loss: 0.0186\n",
      "Epoch 238/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0135 - val_loss: 0.0191\n",
      "Epoch 239/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0136 - val_loss: 0.0188\n",
      "Epoch 240/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0136 - val_loss: 0.0186\n",
      "Epoch 241/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0135 - val_loss: 0.0187\n",
      "Epoch 242/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0135 - val_loss: 0.0187\n",
      "Epoch 243/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0135 - val_loss: 0.0187\n",
      "Epoch 244/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0135 - val_loss: 0.0188\n",
      "Epoch 245/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0135 - val_loss: 0.0191\n",
      "Epoch 246/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0135 - val_loss: 0.0187\n",
      "Epoch 247/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0135 - val_loss: 0.0190\n",
      "Epoch 248/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0134 - val_loss: 0.0186\n",
      "Epoch 249/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0134 - val_loss: 0.0186\n",
      "Epoch 250/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0133 - val_loss: 0.0188\n",
      "Epoch 251/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0133 - val_loss: 0.0187\n",
      "Epoch 252/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0133 - val_loss: 0.0188\n",
      "Epoch 253/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0133 - val_loss: 0.0188\n",
      "Epoch 254/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0133 - val_loss: 0.0188\n",
      "Epoch 255/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0133 - val_loss: 0.0188\n",
      "Epoch 256/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0132 - val_loss: 0.0184\n",
      "Epoch 257/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0132 - val_loss: 0.0187\n",
      "Epoch 258/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0132 - val_loss: 0.0186\n",
      "Epoch 259/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0132 - val_loss: 0.0188\n",
      "Epoch 260/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0133 - val_loss: 0.0187\n",
      "Epoch 261/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0134 - val_loss: 0.0186\n",
      "Epoch 262/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0133 - val_loss: 0.0190\n",
      "Epoch 263/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0133 - val_loss: 0.0186\n",
      "Epoch 264/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0132 - val_loss: 0.0183\n",
      "Epoch 265/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0131 - val_loss: 0.0186\n",
      "Epoch 266/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0132 - val_loss: 0.0188\n",
      "Epoch 267/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0132 - val_loss: 0.0187\n",
      "Epoch 268/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0131 - val_loss: 0.0187\n",
      "Epoch 269/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0130 - val_loss: 0.0186\n",
      "Epoch 270/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0130 - val_loss: 0.0185\n",
      "Epoch 271/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0131 - val_loss: 0.0185\n",
      "Epoch 272/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0131 - val_loss: 0.0188\n",
      "Epoch 273/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0131 - val_loss: 0.0184\n",
      "Epoch 274/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0130 - val_loss: 0.0188\n",
      "Epoch 275/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0131 - val_loss: 0.0185\n",
      "Epoch 276/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0131 - val_loss: 0.0186\n",
      "Epoch 277/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0131 - val_loss: 0.0185\n",
      "Epoch 278/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0130 - val_loss: 0.0183\n",
      "Epoch 279/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0129 - val_loss: 0.0184\n",
      "Epoch 280/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0129 - val_loss: 0.0186\n",
      "Epoch 281/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0129 - val_loss: 0.0187\n",
      "Epoch 282/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0128 - val_loss: 0.0188\n",
      "Epoch 283/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0128 - val_loss: 0.0183\n",
      "Epoch 284/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0128 - val_loss: 0.0186\n",
      "Epoch 285/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0128 - val_loss: 0.0185\n",
      "Epoch 286/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0129 - val_loss: 0.0185\n",
      "Epoch 287/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0132 - val_loss: 0.0192\n",
      "Epoch 288/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0133 - val_loss: 0.0187\n",
      "Epoch 289/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0129 - val_loss: 0.0183\n",
      "Epoch 290/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0129 - val_loss: 0.0183\n",
      "Epoch 291/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0128 - val_loss: 0.0181\n",
      "Epoch 292/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0127 - val_loss: 0.0183\n",
      "Epoch 293/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0128 - val_loss: 0.0187\n",
      "Epoch 294/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0129 - val_loss: 0.0184\n",
      "Epoch 295/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0128 - val_loss: 0.0187\n",
      "Epoch 296/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0127 - val_loss: 0.0184\n",
      "Epoch 297/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0127 - val_loss: 0.0185\n",
      "Epoch 298/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0127 - val_loss: 0.0182\n",
      "Epoch 299/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0126 - val_loss: 0.0186\n",
      "Epoch 300/500\n",
      "741/741 [==============================] - 0s 140us/step - loss: 0.0126 - val_loss: 0.0186\n",
      "Epoch 301/500\n",
      "741/741 [==============================] - 0s 142us/step - loss: 0.0126 - val_loss: 0.0185\n",
      "Epoch 302/500\n",
      "741/741 [==============================] - 0s 139us/step - loss: 0.0126 - val_loss: 0.0185\n",
      "Epoch 303/500\n",
      "741/741 [==============================] - 0s 144us/step - loss: 0.0126 - val_loss: 0.0190\n",
      "Epoch 304/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0126 - val_loss: 0.0185\n",
      "Epoch 305/500\n",
      "741/741 [==============================] - 0s 140us/step - loss: 0.0125 - val_loss: 0.0187\n",
      "Epoch 306/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0125 - val_loss: 0.0189\n",
      "Epoch 307/500\n",
      "741/741 [==============================] - 0s 140us/step - loss: 0.0125 - val_loss: 0.0185\n",
      "Epoch 308/500\n",
      "741/741 [==============================] - 0s 126us/step - loss: 0.0126 - val_loss: 0.0186\n",
      "Epoch 309/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.0125 - val_loss: 0.0186\n",
      "Epoch 310/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0125 - val_loss: 0.0185\n",
      "Epoch 311/500\n",
      "741/741 [==============================] - 0s 139us/step - loss: 0.0125 - val_loss: 0.0188\n",
      "Epoch 312/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0125 - val_loss: 0.0187\n",
      "Epoch 313/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 134us/step - loss: 0.0125 - val_loss: 0.0188\n",
      "Epoch 314/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0124 - val_loss: 0.0185\n",
      "Epoch 315/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0124 - val_loss: 0.0186\n",
      "Epoch 316/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0124 - val_loss: 0.0186\n",
      "Epoch 317/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0124 - val_loss: 0.0186\n",
      "Epoch 318/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0123 - val_loss: 0.0189\n",
      "Epoch 319/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0123 - val_loss: 0.0185\n",
      "Epoch 320/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0124 - val_loss: 0.0186\n",
      "Epoch 321/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0124 - val_loss: 0.0185\n",
      "Epoch 322/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0123 - val_loss: 0.0185\n",
      "Epoch 323/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0124 - val_loss: 0.0189\n",
      "Epoch 324/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0124 - val_loss: 0.0186\n",
      "Epoch 325/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0123 - val_loss: 0.0189\n",
      "Epoch 326/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0124 - val_loss: 0.0188\n",
      "Epoch 327/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0124 - val_loss: 0.0189\n",
      "Epoch 328/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0124 - val_loss: 0.0189\n",
      "Epoch 329/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0123 - val_loss: 0.0190\n",
      "Epoch 330/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0123 - val_loss: 0.0185\n",
      "Epoch 331/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0122 - val_loss: 0.0187\n",
      "Epoch 332/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0124 - val_loss: 0.0184\n",
      "Epoch 333/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0125 - val_loss: 0.0188\n",
      "Epoch 334/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0124 - val_loss: 0.0183\n",
      "Epoch 335/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0123 - val_loss: 0.0188\n",
      "Epoch 336/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0123 - val_loss: 0.0189\n",
      "Epoch 337/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0122 - val_loss: 0.0189\n",
      "Epoch 338/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0122 - val_loss: 0.0186\n",
      "Epoch 339/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0122 - val_loss: 0.0185\n",
      "Epoch 340/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0121 - val_loss: 0.0189\n",
      "Epoch 341/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0122 - val_loss: 0.0183\n",
      "Epoch 342/500\n",
      "741/741 [==============================] - 0s 138us/step - loss: 0.0122 - val_loss: 0.0186\n",
      "Epoch 343/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0123 - val_loss: 0.0190\n",
      "Epoch 344/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0123 - val_loss: 0.0186\n",
      "Epoch 345/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0122 - val_loss: 0.0185\n",
      "Epoch 346/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0122 - val_loss: 0.0190\n",
      "Epoch 347/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0121 - val_loss: 0.0185\n",
      "Epoch 348/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0122 - val_loss: 0.0187\n",
      "Epoch 349/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0121 - val_loss: 0.0188\n",
      "Epoch 350/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0121 - val_loss: 0.0188\n",
      "Epoch 351/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0121 - val_loss: 0.0188\n",
      "Epoch 352/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0122 - val_loss: 0.0190\n",
      "Epoch 353/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0121 - val_loss: 0.0189\n",
      "Epoch 354/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0122 - val_loss: 0.0189\n",
      "Epoch 355/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0124 - val_loss: 0.0193\n",
      "Epoch 356/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0124 - val_loss: 0.0186\n",
      "Epoch 357/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0122 - val_loss: 0.0185\n",
      "Epoch 358/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0121 - val_loss: 0.0186\n",
      "Epoch 359/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0121 - val_loss: 0.0188\n",
      "Epoch 360/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0121 - val_loss: 0.0185\n",
      "Epoch 361/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0120 - val_loss: 0.0190\n",
      "Epoch 362/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0120 - val_loss: 0.0185\n",
      "Epoch 363/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0120 - val_loss: 0.0191\n",
      "Epoch 364/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0120 - val_loss: 0.0187\n",
      "Epoch 365/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0119 - val_loss: 0.0186\n",
      "Epoch 366/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0119 - val_loss: 0.0187\n",
      "Epoch 367/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0119 - val_loss: 0.0185\n",
      "Epoch 368/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0119 - val_loss: 0.0190\n",
      "Epoch 369/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0120 - val_loss: 0.0186\n",
      "Epoch 370/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0119 - val_loss: 0.0193\n",
      "Epoch 371/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0120 - val_loss: 0.0185\n",
      "Epoch 372/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0120 - val_loss: 0.0188\n",
      "Epoch 373/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0120 - val_loss: 0.0191\n",
      "Epoch 374/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0120 - val_loss: 0.0191\n",
      "Epoch 375/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0123 - val_loss: 0.0191\n",
      "Epoch 376/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0122 - val_loss: 0.0194\n",
      "Epoch 377/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0122 - val_loss: 0.0194\n",
      "Epoch 378/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0122 - val_loss: 0.0190\n",
      "Epoch 379/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0122 - val_loss: 0.0188\n",
      "Epoch 380/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0125 - val_loss: 0.0193\n",
      "Epoch 381/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0123 - val_loss: 0.0187\n",
      "Epoch 382/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0122 - val_loss: 0.0189\n",
      "Epoch 383/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0120 - val_loss: 0.0186\n",
      "Epoch 384/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0119 - val_loss: 0.0188\n",
      "Epoch 385/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0119 - val_loss: 0.0187\n",
      "Epoch 386/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0118 - val_loss: 0.0186\n",
      "Epoch 387/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0118 - val_loss: 0.0189\n",
      "Epoch 388/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0119 - val_loss: 0.0188\n",
      "Epoch 389/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0119 - val_loss: 0.0186\n",
      "Epoch 390/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0118 - val_loss: 0.0187\n",
      "Epoch 391/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 130us/step - loss: 0.0118 - val_loss: 0.0188\n",
      "Epoch 392/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0118 - val_loss: 0.0188\n",
      "Epoch 393/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0118 - val_loss: 0.0190\n",
      "Epoch 394/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0118 - val_loss: 0.0189\n",
      "Epoch 395/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0117 - val_loss: 0.0188\n",
      "Epoch 396/500\n",
      "741/741 [==============================] - 0s 138us/step - loss: 0.0117 - val_loss: 0.0187\n",
      "Epoch 397/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0117 - val_loss: 0.0189\n",
      "Epoch 398/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0117 - val_loss: 0.0187\n",
      "Epoch 399/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.0117 - val_loss: 0.0189\n",
      "Epoch 400/500\n",
      "741/741 [==============================] - 0s 140us/step - loss: 0.0117 - val_loss: 0.0189\n",
      "Epoch 401/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0117 - val_loss: 0.0190\n",
      "Epoch 402/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0117 - val_loss: 0.0188\n",
      "Epoch 403/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0116 - val_loss: 0.0187\n",
      "Epoch 404/500\n",
      "741/741 [==============================] - 0s 143us/step - loss: 0.0116 - val_loss: 0.0187\n",
      "Epoch 405/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0116 - val_loss: 0.0187\n",
      "Epoch 406/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0116 - val_loss: 0.0188\n",
      "Epoch 407/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0116 - val_loss: 0.0187\n",
      "Epoch 408/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0116 - val_loss: 0.0189\n",
      "Epoch 409/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0118 - val_loss: 0.0190\n",
      "Epoch 410/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0117 - val_loss: 0.0188\n",
      "Epoch 411/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0117 - val_loss: 0.0188\n",
      "Epoch 412/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0116 - val_loss: 0.0188\n",
      "Epoch 413/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0116 - val_loss: 0.0189\n",
      "Epoch 414/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0116 - val_loss: 0.0189\n",
      "Epoch 415/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0116 - val_loss: 0.0187\n",
      "Epoch 416/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.0116 - val_loss: 0.0191\n",
      "Epoch 417/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0115 - val_loss: 0.0186\n",
      "Epoch 418/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0115 - val_loss: 0.0190\n",
      "Epoch 419/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0116 - val_loss: 0.0186\n",
      "Epoch 420/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0117 - val_loss: 0.0185\n",
      "Epoch 421/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0116 - val_loss: 0.0191\n",
      "Epoch 422/500\n",
      "741/741 [==============================] - 0s 147us/step - loss: 0.0115 - val_loss: 0.0185\n",
      "Epoch 423/500\n",
      "741/741 [==============================] - 0s 126us/step - loss: 0.0115 - val_loss: 0.0191\n",
      "Epoch 424/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0115 - val_loss: 0.0187\n",
      "Epoch 425/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0115 - val_loss: 0.0189\n",
      "Epoch 426/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0115 - val_loss: 0.0188\n",
      "Epoch 427/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0114 - val_loss: 0.0188\n",
      "Epoch 428/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0114 - val_loss: 0.0189\n",
      "Epoch 429/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0115 - val_loss: 0.0187\n",
      "Epoch 430/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0115 - val_loss: 0.0189\n",
      "Epoch 431/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0115 - val_loss: 0.0186\n",
      "Epoch 432/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0114 - val_loss: 0.0187\n",
      "Epoch 433/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0114 - val_loss: 0.0188\n",
      "Epoch 434/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0114 - val_loss: 0.0187\n",
      "Epoch 435/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0114 - val_loss: 0.0189\n",
      "Epoch 436/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0113 - val_loss: 0.0187\n",
      "Epoch 437/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0114 - val_loss: 0.0187\n",
      "Epoch 438/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0115 - val_loss: 0.0187\n",
      "Epoch 439/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0115 - val_loss: 0.0191\n",
      "Epoch 440/500\n",
      "741/741 [==============================] - 0s 126us/step - loss: 0.0114 - val_loss: 0.0186\n",
      "Epoch 441/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0114 - val_loss: 0.0190\n",
      "Epoch 442/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0114 - val_loss: 0.0188\n",
      "Epoch 443/500\n",
      "741/741 [==============================] - 0s 139us/step - loss: 0.0114 - val_loss: 0.0190\n",
      "Epoch 444/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0116 - val_loss: 0.0191\n",
      "Epoch 445/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0115 - val_loss: 0.0191\n",
      "Epoch 446/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.0115 - val_loss: 0.0187\n",
      "Epoch 447/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0114 - val_loss: 0.0187\n",
      "Epoch 448/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0112 - val_loss: 0.0187\n",
      "Epoch 449/500\n",
      "741/741 [==============================] - 0s 140us/step - loss: 0.0113 - val_loss: 0.0189\n",
      "Epoch 450/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0113 - val_loss: 0.0188\n",
      "Epoch 451/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0113 - val_loss: 0.0188\n",
      "Epoch 452/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0113 - val_loss: 0.0190\n",
      "Epoch 453/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0114 - val_loss: 0.0188\n",
      "Epoch 454/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0114 - val_loss: 0.0188\n",
      "Epoch 455/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0113 - val_loss: 0.0186\n",
      "Epoch 456/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0113 - val_loss: 0.0191\n",
      "Epoch 457/500\n",
      "741/741 [==============================] - 0s 140us/step - loss: 0.0113 - val_loss: 0.0187\n",
      "Epoch 458/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0114 - val_loss: 0.0184\n",
      "Epoch 459/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0113 - val_loss: 0.0185\n",
      "Epoch 460/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0113 - val_loss: 0.0187\n",
      "Epoch 461/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0113 - val_loss: 0.0185\n",
      "Epoch 462/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0114 - val_loss: 0.0190\n",
      "Epoch 463/500\n",
      "741/741 [==============================] - 0s 146us/step - loss: 0.0115 - val_loss: 0.0192\n",
      "Epoch 464/500\n",
      "741/741 [==============================] - 0s 142us/step - loss: 0.0114 - val_loss: 0.0189\n",
      "Epoch 465/500\n",
      "741/741 [==============================] - 0s 144us/step - loss: 0.0114 - val_loss: 0.0191\n",
      "Epoch 466/500\n",
      "741/741 [==============================] - 0s 146us/step - loss: 0.0114 - val_loss: 0.0199\n",
      "Epoch 467/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0120 - val_loss: 0.0197\n",
      "Epoch 468/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.0118 - val_loss: 0.0189\n",
      "Epoch 469/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 132us/step - loss: 0.0115 - val_loss: 0.0193\n",
      "Epoch 470/500\n",
      "741/741 [==============================] - 0s 138us/step - loss: 0.0114 - val_loss: 0.0184\n",
      "Epoch 471/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0113 - val_loss: 0.0195\n",
      "Epoch 472/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0115 - val_loss: 0.0185\n",
      "Epoch 473/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0114 - val_loss: 0.0188\n",
      "Epoch 474/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0114 - val_loss: 0.0188\n",
      "Epoch 475/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0112 - val_loss: 0.0185\n",
      "Epoch 476/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0112 - val_loss: 0.0189\n",
      "Epoch 477/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0114 - val_loss: 0.0189\n",
      "Epoch 478/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0112 - val_loss: 0.0186\n",
      "Epoch 479/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0112 - val_loss: 0.0186\n",
      "Epoch 480/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0111 - val_loss: 0.0188\n",
      "Epoch 481/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.0112 - val_loss: 0.0188\n",
      "Epoch 482/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0112 - val_loss: 0.0188\n",
      "Epoch 483/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0115 - val_loss: 0.0195\n",
      "Epoch 484/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0117 - val_loss: 0.0185\n",
      "Epoch 485/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0115 - val_loss: 0.0189\n",
      "Epoch 486/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0114 - val_loss: 0.0187\n",
      "Epoch 487/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0114 - val_loss: 0.0183\n",
      "Epoch 488/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0112 - val_loss: 0.0185\n",
      "Epoch 489/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0111 - val_loss: 0.0187\n",
      "Epoch 490/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0111 - val_loss: 0.0184\n",
      "Epoch 491/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0110 - val_loss: 0.0187\n",
      "Epoch 492/500\n",
      "741/741 [==============================] - 0s 127us/step - loss: 0.0110 - val_loss: 0.0184\n",
      "Epoch 493/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0110 - val_loss: 0.0189\n",
      "Epoch 494/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0110 - val_loss: 0.0184\n",
      "Epoch 495/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0110 - val_loss: 0.0187\n",
      "Epoch 496/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0109 - val_loss: 0.0186\n",
      "Epoch 497/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0110 - val_loss: 0.0189\n",
      "Epoch 498/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0109 - val_loss: 0.0185\n",
      "Epoch 499/500\n",
      "741/741 [==============================] - 0s 128us/step - loss: 0.0109 - val_loss: 0.0185\n",
      "Epoch 500/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0109 - val_loss: 0.0186\n"
     ]
    }
   ],
   "source": [
    "test_ae.Train_autoencoder(X_AE_Train, X_AE_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  98.88990466240043\n",
      "F1 Score =  0.9738488360168187\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True       4748.0          76.0\n",
      "1  Pred False        179.0       17968.0\n"
     ]
    }
   ],
   "source": [
    "test_ae.Test_Accuracy(X_AE_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  98.02150537634408\n",
      "F1 Score =  0.9530132788559754\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True        933.0          31.0\n",
      "1  Pred False         61.0        3625.0\n"
     ]
    }
   ],
   "source": [
    "test_ae.Test_Accuracy(X_AE_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  98.37166229356382\n",
      "F1 Score =  0.961404792390708\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True       2628.0          65.0\n",
      "1  Pred False        146.0       10119.0\n"
     ]
    }
   ],
   "source": [
    "test_ae.Test_Accuracy(X_AE_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above it seems we have a solid foundation from which to start.  \n",
    "\n",
    "The next step is to add a further step of robustness to the process and adding corruption to the input.  This should add a degree of robustness to outliers.\n",
    "\n",
    "The process for this is simple - \n",
    "1. Generate uniform random numbers in matrix of shape ($X_0, X_1$)\n",
    "2. Set elements of matrix as 1 if below a probability threshold, else set to 0.\n",
    "3. Mask original X by means of the absolute value of mask - X (as such any element with mask 1 is flipped, otherwise left as it is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit to Path of Exile for naming convention\n",
    "def Corrupt_data(Orig_X, Probability = 0.001) :\n",
    "    Rand_set = np.random.uniform(size=Orig_X.shape)\n",
    "    \n",
    "    Vaal_Mask = 1 - np.heaviside(Rand_set - Probability, 1)\n",
    "    \n",
    "    Vaal_X = np.absolute(Vaal_Mask - Orig_X)\n",
    "    \n",
    "    return Vaal_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to train a denoising autoencoder the process is simple - corrupt the input with the above function, and feed this corrupted input in against the ground truth as comparison and see how it does, comparing against the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vaal_X_AE_Train = Corrupt_data(X_AE_Train, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 741 samples, validate on 150 samples\n",
      "Epoch 1/500\n",
      "741/741 [==============================] - 0s 618us/step - loss: 0.2456 - val_loss: 0.2391\n",
      "Epoch 2/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.2327 - val_loss: 0.2246\n",
      "Epoch 3/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.2151 - val_loss: 0.2041\n",
      "Epoch 4/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.1911 - val_loss: 0.1780\n",
      "Epoch 5/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.1643 - val_loss: 0.1530\n",
      "Epoch 6/500\n",
      "741/741 [==============================] - 0s 138us/step - loss: 0.1419 - val_loss: 0.1362\n",
      "Epoch 7/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.1283 - val_loss: 0.1264\n",
      "Epoch 8/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.1199 - val_loss: 0.1198\n",
      "Epoch 9/500\n",
      "741/741 [==============================] - 0s 138us/step - loss: 0.1139 - val_loss: 0.1145\n",
      "Epoch 10/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.1093 - val_loss: 0.1104\n",
      "Epoch 11/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.1059 - val_loss: 0.1074\n",
      "Epoch 12/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.1034 - val_loss: 0.1053\n",
      "Epoch 13/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.1015 - val_loss: 0.1038\n",
      "Epoch 14/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.1001 - val_loss: 0.1025\n",
      "Epoch 15/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0989 - val_loss: 0.1015\n",
      "Epoch 16/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0980 - val_loss: 0.1006\n",
      "Epoch 17/500\n",
      "741/741 [==============================] - 0s 130us/step - loss: 0.0972 - val_loss: 0.0998\n",
      "Epoch 18/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.0965 - val_loss: 0.0989\n",
      "Epoch 19/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0958 - val_loss: 0.0981\n",
      "Epoch 20/500\n",
      "741/741 [==============================] - 0s 143us/step - loss: 0.0949 - val_loss: 0.0972\n",
      "Epoch 21/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.0939 - val_loss: 0.0958\n",
      "Epoch 22/500\n",
      "741/741 [==============================] - 0s 140us/step - loss: 0.0928 - val_loss: 0.0948\n",
      "Epoch 23/500\n",
      "741/741 [==============================] - 0s 144us/step - loss: 0.0917 - val_loss: 0.0934\n",
      "Epoch 24/500\n",
      "741/741 [==============================] - 0s 140us/step - loss: 0.0904 - val_loss: 0.0918\n",
      "Epoch 25/500\n",
      "741/741 [==============================] - 0s 139us/step - loss: 0.0891 - val_loss: 0.0901\n",
      "Epoch 26/500\n",
      "741/741 [==============================] - 0s 142us/step - loss: 0.0875 - val_loss: 0.0880\n",
      "Epoch 27/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.0860 - val_loss: 0.0864\n",
      "Epoch 28/500\n",
      "741/741 [==============================] - 0s 146us/step - loss: 0.0845 - val_loss: 0.0845\n",
      "Epoch 29/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0833 - val_loss: 0.0834\n",
      "Epoch 30/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0821 - val_loss: 0.0820\n",
      "Epoch 31/500\n",
      "741/741 [==============================] - 0s 132us/step - loss: 0.0810 - val_loss: 0.0808\n",
      "Epoch 32/500\n",
      "741/741 [==============================] - 0s 131us/step - loss: 0.0797 - val_loss: 0.0794\n",
      "Epoch 33/500\n",
      "741/741 [==============================] - 0s 142us/step - loss: 0.0785 - val_loss: 0.0783\n",
      "Epoch 34/500\n",
      "741/741 [==============================] - 0s 134us/step - loss: 0.0773 - val_loss: 0.0772\n",
      "Epoch 35/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0761 - val_loss: 0.0761\n",
      "Epoch 36/500\n",
      "741/741 [==============================] - 0s 139us/step - loss: 0.0750 - val_loss: 0.0744\n",
      "Epoch 37/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.0739 - val_loss: 0.0735\n",
      "Epoch 38/500\n",
      "741/741 [==============================] - 0s 139us/step - loss: 0.0726 - val_loss: 0.0716\n",
      "Epoch 39/500\n",
      "741/741 [==============================] - 0s 136us/step - loss: 0.0712 - val_loss: 0.0701\n",
      "Epoch 40/500\n",
      "741/741 [==============================] - 0s 138us/step - loss: 0.0698 - val_loss: 0.0681\n",
      "Epoch 41/500\n",
      "741/741 [==============================] - 0s 139us/step - loss: 0.0685 - val_loss: 0.0672\n",
      "Epoch 42/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0675 - val_loss: 0.0658\n",
      "Epoch 43/500\n",
      "741/741 [==============================] - 0s 144us/step - loss: 0.0664 - val_loss: 0.0638\n",
      "Epoch 44/500\n",
      "741/741 [==============================] - 0s 154us/step - loss: 0.0654 - val_loss: 0.0636\n",
      "Epoch 45/500\n",
      "741/741 [==============================] - 0s 144us/step - loss: 0.0648 - val_loss: 0.0626\n",
      "Epoch 46/500\n",
      "741/741 [==============================] - 0s 165us/step - loss: 0.0643 - val_loss: 0.0620\n",
      "Epoch 47/500\n",
      "741/741 [==============================] - 0s 155us/step - loss: 0.0640 - val_loss: 0.0613\n",
      "Epoch 48/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0635 - val_loss: 0.0609\n",
      "Epoch 49/500\n",
      "741/741 [==============================] - 0s 140us/step - loss: 0.0629 - val_loss: 0.0603\n",
      "Epoch 50/500\n",
      "741/741 [==============================] - 0s 150us/step - loss: 0.0626 - val_loss: 0.0593\n",
      "Epoch 51/500\n",
      "741/741 [==============================] - 0s 146us/step - loss: 0.0621 - val_loss: 0.0583\n",
      "Epoch 52/500\n",
      "741/741 [==============================] - 0s 144us/step - loss: 0.0615 - val_loss: 0.0583\n",
      "Epoch 53/500\n",
      "741/741 [==============================] - 0s 142us/step - loss: 0.0606 - val_loss: 0.0572\n",
      "Epoch 54/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0599 - val_loss: 0.0561\n",
      "Epoch 55/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0588 - val_loss: 0.0553\n",
      "Epoch 56/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0581 - val_loss: 0.0541\n",
      "Epoch 57/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0571 - val_loss: 0.0537\n",
      "Epoch 58/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0562 - val_loss: 0.0524\n",
      "Epoch 59/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0555 - val_loss: 0.0520\n",
      "Epoch 60/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0548 - val_loss: 0.0518\n",
      "Epoch 61/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0542 - val_loss: 0.0506\n",
      "Epoch 62/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0536 - val_loss: 0.0501\n",
      "Epoch 63/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0530 - val_loss: 0.0496\n",
      "Epoch 64/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0525 - val_loss: 0.0494\n",
      "Epoch 65/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0520 - val_loss: 0.0490\n",
      "Epoch 66/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0516 - val_loss: 0.0489\n",
      "Epoch 67/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0513 - val_loss: 0.0487\n",
      "Epoch 68/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0510 - val_loss: 0.0480\n",
      "Epoch 69/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0506 - val_loss: 0.0479\n",
      "Epoch 70/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0503 - val_loss: 0.0482\n",
      "Epoch 71/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0498 - val_loss: 0.0472\n",
      "Epoch 72/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0499 - val_loss: 0.0472\n",
      "Epoch 73/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0496 - val_loss: 0.0469\n",
      "Epoch 74/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0493 - val_loss: 0.0467\n",
      "Epoch 75/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0489 - val_loss: 0.0462\n",
      "Epoch 76/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0487 - val_loss: 0.0463\n",
      "Epoch 77/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0484 - val_loss: 0.0464\n",
      "Epoch 78/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0483 - val_loss: 0.0462\n",
      "Epoch 79/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 135us/step - loss: 0.0480 - val_loss: 0.0455\n",
      "Epoch 80/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0477 - val_loss: 0.0457\n",
      "Epoch 81/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0476 - val_loss: 0.0455\n",
      "Epoch 82/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0474 - val_loss: 0.0454\n",
      "Epoch 83/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0472 - val_loss: 0.0451\n",
      "Epoch 84/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0470 - val_loss: 0.0448\n",
      "Epoch 85/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0469 - val_loss: 0.0450\n",
      "Epoch 86/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0466 - val_loss: 0.0445\n",
      "Epoch 87/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0464 - val_loss: 0.0444\n",
      "Epoch 88/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0462 - val_loss: 0.0447\n",
      "Epoch 89/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0462 - val_loss: 0.0441\n",
      "Epoch 90/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0460 - val_loss: 0.0435\n",
      "Epoch 91/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0459 - val_loss: 0.0439\n",
      "Epoch 92/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0457 - val_loss: 0.0438\n",
      "Epoch 93/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0455 - val_loss: 0.0433\n",
      "Epoch 94/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0453 - val_loss: 0.0435\n",
      "Epoch 95/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0451 - val_loss: 0.0434\n",
      "Epoch 96/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0450 - val_loss: 0.0428\n",
      "Epoch 97/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0448 - val_loss: 0.0425\n",
      "Epoch 98/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0447 - val_loss: 0.0430\n",
      "Epoch 99/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0449 - val_loss: 0.0430\n",
      "Epoch 100/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0446 - val_loss: 0.0428\n",
      "Epoch 101/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0443 - val_loss: 0.0426\n",
      "Epoch 102/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0442 - val_loss: 0.0420\n",
      "Epoch 103/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0440 - val_loss: 0.0417\n",
      "Epoch 104/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0437 - val_loss: 0.0416\n",
      "Epoch 105/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0435 - val_loss: 0.0414\n",
      "Epoch 106/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0433 - val_loss: 0.0416\n",
      "Epoch 107/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0433 - val_loss: 0.0413\n",
      "Epoch 108/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0430 - val_loss: 0.0413\n",
      "Epoch 109/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0429 - val_loss: 0.0411\n",
      "Epoch 110/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0427 - val_loss: 0.0408\n",
      "Epoch 111/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0426 - val_loss: 0.0407\n",
      "Epoch 112/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0424 - val_loss: 0.0408\n",
      "Epoch 113/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0423 - val_loss: 0.0402\n",
      "Epoch 114/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0421 - val_loss: 0.0404\n",
      "Epoch 115/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0420 - val_loss: 0.0402\n",
      "Epoch 116/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0420 - val_loss: 0.0405\n",
      "Epoch 117/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0419 - val_loss: 0.0399\n",
      "Epoch 118/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0415 - val_loss: 0.0399\n",
      "Epoch 119/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0414 - val_loss: 0.0396\n",
      "Epoch 120/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0411 - val_loss: 0.0395\n",
      "Epoch 121/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0410 - val_loss: 0.0394\n",
      "Epoch 122/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0410 - val_loss: 0.0393\n",
      "Epoch 123/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0408 - val_loss: 0.0391\n",
      "Epoch 124/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0406 - val_loss: 0.0391\n",
      "Epoch 125/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0406 - val_loss: 0.0393\n",
      "Epoch 126/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0407 - val_loss: 0.0393\n",
      "Epoch 127/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0406 - val_loss: 0.0392\n",
      "Epoch 128/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0406 - val_loss: 0.0389\n",
      "Epoch 129/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0400 - val_loss: 0.0385\n",
      "Epoch 130/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0398 - val_loss: 0.0384\n",
      "Epoch 131/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0396 - val_loss: 0.0385\n",
      "Epoch 132/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0395 - val_loss: 0.0384\n",
      "Epoch 133/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0395 - val_loss: 0.0382\n",
      "Epoch 134/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0392 - val_loss: 0.0379\n",
      "Epoch 135/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0391 - val_loss: 0.0378\n",
      "Epoch 136/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0389 - val_loss: 0.0381\n",
      "Epoch 137/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0387 - val_loss: 0.0378\n",
      "Epoch 138/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0386 - val_loss: 0.0379\n",
      "Epoch 139/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0385 - val_loss: 0.0380\n",
      "Epoch 140/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0384 - val_loss: 0.0377\n",
      "Epoch 141/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0383 - val_loss: 0.0375\n",
      "Epoch 142/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0382 - val_loss: 0.0374\n",
      "Epoch 143/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0381 - val_loss: 0.0370\n",
      "Epoch 144/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0382 - val_loss: 0.0367\n",
      "Epoch 145/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0382 - val_loss: 0.0370\n",
      "Epoch 146/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0377 - val_loss: 0.0367\n",
      "Epoch 147/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0376 - val_loss: 0.0374\n",
      "Epoch 148/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0374 - val_loss: 0.0371\n",
      "Epoch 149/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0373 - val_loss: 0.0372\n",
      "Epoch 150/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0373 - val_loss: 0.0373\n",
      "Epoch 151/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0371 - val_loss: 0.0370\n",
      "Epoch 152/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0367 - val_loss: 0.0372\n",
      "Epoch 153/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0368 - val_loss: 0.0371\n",
      "Epoch 154/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0369 - val_loss: 0.0372\n",
      "Epoch 155/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0366 - val_loss: 0.0366\n",
      "Epoch 156/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0365 - val_loss: 0.0367\n",
      "Epoch 157/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 148us/step - loss: 0.0363 - val_loss: 0.0366\n",
      "Epoch 158/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0361 - val_loss: 0.0365\n",
      "Epoch 159/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0359 - val_loss: 0.0367\n",
      "Epoch 160/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0360 - val_loss: 0.0366\n",
      "Epoch 161/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0357 - val_loss: 0.0361\n",
      "Epoch 162/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0356 - val_loss: 0.0360\n",
      "Epoch 163/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0355 - val_loss: 0.0360\n",
      "Epoch 164/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0353 - val_loss: 0.0359\n",
      "Epoch 165/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0353 - val_loss: 0.0358\n",
      "Epoch 166/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0352 - val_loss: 0.0356\n",
      "Epoch 167/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0350 - val_loss: 0.0356\n",
      "Epoch 168/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0353 - val_loss: 0.0358\n",
      "Epoch 169/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0351 - val_loss: 0.0358\n",
      "Epoch 170/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0351 - val_loss: 0.0356\n",
      "Epoch 171/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0348 - val_loss: 0.0359\n",
      "Epoch 172/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0348 - val_loss: 0.0355\n",
      "Epoch 173/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0347 - val_loss: 0.0354\n",
      "Epoch 174/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0346 - val_loss: 0.0354\n",
      "Epoch 175/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0345 - val_loss: 0.0356\n",
      "Epoch 176/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0342 - val_loss: 0.0353\n",
      "Epoch 177/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0342 - val_loss: 0.0354\n",
      "Epoch 178/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0343 - val_loss: 0.0354\n",
      "Epoch 179/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0349 - val_loss: 0.0353\n",
      "Epoch 180/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0340 - val_loss: 0.0349\n",
      "Epoch 181/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0337 - val_loss: 0.0351\n",
      "Epoch 182/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0337 - val_loss: 0.0348\n",
      "Epoch 183/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0337 - val_loss: 0.0349\n",
      "Epoch 184/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0336 - val_loss: 0.0350\n",
      "Epoch 185/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0333 - val_loss: 0.0349\n",
      "Epoch 186/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0336 - val_loss: 0.0347\n",
      "Epoch 187/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0337 - val_loss: 0.0347\n",
      "Epoch 188/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0335 - val_loss: 0.0348\n",
      "Epoch 189/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0333 - val_loss: 0.0346\n",
      "Epoch 190/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0331 - val_loss: 0.0349\n",
      "Epoch 191/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0329 - val_loss: 0.0349\n",
      "Epoch 192/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0330 - val_loss: 0.0348\n",
      "Epoch 193/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0330 - val_loss: 0.0344\n",
      "Epoch 194/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0328 - val_loss: 0.0350\n",
      "Epoch 195/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0327 - val_loss: 0.0346\n",
      "Epoch 196/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0325 - val_loss: 0.0346\n",
      "Epoch 197/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0323 - val_loss: 0.0346\n",
      "Epoch 198/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0323 - val_loss: 0.0347\n",
      "Epoch 199/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0322 - val_loss: 0.0346\n",
      "Epoch 200/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0322 - val_loss: 0.0344\n",
      "Epoch 201/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0320 - val_loss: 0.0345\n",
      "Epoch 202/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0324 - val_loss: 0.0346\n",
      "Epoch 203/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0323 - val_loss: 0.0343\n",
      "Epoch 204/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0320 - val_loss: 0.0342\n",
      "Epoch 205/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0318 - val_loss: 0.0344\n",
      "Epoch 206/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0318 - val_loss: 0.0343\n",
      "Epoch 207/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0317 - val_loss: 0.0344\n",
      "Epoch 208/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0316 - val_loss: 0.0341\n",
      "Epoch 209/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0317 - val_loss: 0.0344\n",
      "Epoch 210/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0320 - val_loss: 0.0345\n",
      "Epoch 211/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0317 - val_loss: 0.0341\n",
      "Epoch 212/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0317 - val_loss: 0.0340\n",
      "Epoch 213/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0315 - val_loss: 0.0343\n",
      "Epoch 214/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0314 - val_loss: 0.0343\n",
      "Epoch 215/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0314 - val_loss: 0.0341\n",
      "Epoch 216/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0313 - val_loss: 0.0340\n",
      "Epoch 217/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0312 - val_loss: 0.0342\n",
      "Epoch 218/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0313 - val_loss: 0.0344\n",
      "Epoch 219/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0312 - val_loss: 0.0341\n",
      "Epoch 220/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0310 - val_loss: 0.0342\n",
      "Epoch 221/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0310 - val_loss: 0.0347\n",
      "Epoch 222/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0309 - val_loss: 0.0339\n",
      "Epoch 223/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0307 - val_loss: 0.0338\n",
      "Epoch 224/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0308 - val_loss: 0.0339\n",
      "Epoch 225/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0311 - val_loss: 0.0340\n",
      "Epoch 226/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0315 - val_loss: 0.0337\n",
      "Epoch 227/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0313 - val_loss: 0.0336\n",
      "Epoch 228/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0314 - val_loss: 0.0337\n",
      "Epoch 229/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0307 - val_loss: 0.0340\n",
      "Epoch 230/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0305 - val_loss: 0.0338\n",
      "Epoch 231/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0304 - val_loss: 0.0335\n",
      "Epoch 232/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0304 - val_loss: 0.0335\n",
      "Epoch 233/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0303 - val_loss: 0.0337\n",
      "Epoch 234/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0301 - val_loss: 0.0339\n",
      "Epoch 235/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 135us/step - loss: 0.0301 - val_loss: 0.0339\n",
      "Epoch 236/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0304 - val_loss: 0.0333\n",
      "Epoch 237/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0301 - val_loss: 0.0336\n",
      "Epoch 238/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0300 - val_loss: 0.0336\n",
      "Epoch 239/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0299 - val_loss: 0.0332\n",
      "Epoch 240/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0299 - val_loss: 0.0340\n",
      "Epoch 241/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0299 - val_loss: 0.0336\n",
      "Epoch 242/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0296 - val_loss: 0.0336\n",
      "Epoch 243/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0297 - val_loss: 0.0336\n",
      "Epoch 244/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0297 - val_loss: 0.0332\n",
      "Epoch 245/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0296 - val_loss: 0.0333\n",
      "Epoch 246/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0295 - val_loss: 0.0332\n",
      "Epoch 247/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0294 - val_loss: 0.0335\n",
      "Epoch 248/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0294 - val_loss: 0.0332\n",
      "Epoch 249/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0295 - val_loss: 0.0334\n",
      "Epoch 250/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0294 - val_loss: 0.0331\n",
      "Epoch 251/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0294 - val_loss: 0.0335\n",
      "Epoch 252/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0293 - val_loss: 0.0334\n",
      "Epoch 253/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0292 - val_loss: 0.0334\n",
      "Epoch 254/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0293 - val_loss: 0.0332\n",
      "Epoch 255/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0292 - val_loss: 0.0330\n",
      "Epoch 256/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0293 - val_loss: 0.0330\n",
      "Epoch 257/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0292 - val_loss: 0.0332\n",
      "Epoch 258/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0291 - val_loss: 0.0331\n",
      "Epoch 259/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0290 - val_loss: 0.0330\n",
      "Epoch 260/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0289 - val_loss: 0.0331\n",
      "Epoch 261/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0288 - val_loss: 0.0330\n",
      "Epoch 262/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0289 - val_loss: 0.0329\n",
      "Epoch 263/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0288 - val_loss: 0.0329\n",
      "Epoch 264/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0289 - val_loss: 0.0329\n",
      "Epoch 265/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0291 - val_loss: 0.0330\n",
      "Epoch 266/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0287 - val_loss: 0.0332\n",
      "Epoch 267/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0287 - val_loss: 0.0327\n",
      "Epoch 268/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0286 - val_loss: 0.0328\n",
      "Epoch 269/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0286 - val_loss: 0.0331\n",
      "Epoch 270/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0284 - val_loss: 0.0328\n",
      "Epoch 271/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0284 - val_loss: 0.0329\n",
      "Epoch 272/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0285 - val_loss: 0.0330\n",
      "Epoch 273/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0285 - val_loss: 0.0331\n",
      "Epoch 274/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0283 - val_loss: 0.0332\n",
      "Epoch 275/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0286 - val_loss: 0.0333\n",
      "Epoch 276/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0286 - val_loss: 0.0330\n",
      "Epoch 277/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0284 - val_loss: 0.0330\n",
      "Epoch 278/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0283 - val_loss: 0.0335\n",
      "Epoch 279/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0283 - val_loss: 0.0327\n",
      "Epoch 280/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0281 - val_loss: 0.0327\n",
      "Epoch 281/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0281 - val_loss: 0.0329\n",
      "Epoch 282/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0281 - val_loss: 0.0326\n",
      "Epoch 283/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0280 - val_loss: 0.0327\n",
      "Epoch 284/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0282 - val_loss: 0.0328\n",
      "Epoch 285/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0280 - val_loss: 0.0329\n",
      "Epoch 286/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0279 - val_loss: 0.0329\n",
      "Epoch 287/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0278 - val_loss: 0.0327\n",
      "Epoch 288/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0278 - val_loss: 0.0328\n",
      "Epoch 289/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0278 - val_loss: 0.0328\n",
      "Epoch 290/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0279 - val_loss: 0.0326\n",
      "Epoch 291/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0278 - val_loss: 0.0325\n",
      "Epoch 292/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0277 - val_loss: 0.0326\n",
      "Epoch 293/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0278 - val_loss: 0.0326\n",
      "Epoch 294/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0275 - val_loss: 0.0324\n",
      "Epoch 295/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0276 - val_loss: 0.0328\n",
      "Epoch 296/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0277 - val_loss: 0.0326\n",
      "Epoch 297/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0278 - val_loss: 0.0326\n",
      "Epoch 298/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0276 - val_loss: 0.0325\n",
      "Epoch 299/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0275 - val_loss: 0.0327\n",
      "Epoch 300/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0276 - val_loss: 0.0323\n",
      "Epoch 301/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0276 - val_loss: 0.0325\n",
      "Epoch 302/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0276 - val_loss: 0.0325\n",
      "Epoch 303/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0272 - val_loss: 0.0326\n",
      "Epoch 304/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0271 - val_loss: 0.0327\n",
      "Epoch 305/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0269 - val_loss: 0.0325\n",
      "Epoch 306/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0269 - val_loss: 0.0323\n",
      "Epoch 307/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0271 - val_loss: 0.0326\n",
      "Epoch 308/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0271 - val_loss: 0.0324\n",
      "Epoch 309/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0274 - val_loss: 0.0328\n",
      "Epoch 310/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0276 - val_loss: 0.0324\n",
      "Epoch 311/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0275 - val_loss: 0.0326\n",
      "Epoch 312/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0271 - val_loss: 0.0325\n",
      "Epoch 313/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 135us/step - loss: 0.0269 - val_loss: 0.0325\n",
      "Epoch 314/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0268 - val_loss: 0.0324\n",
      "Epoch 315/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0267 - val_loss: 0.0324\n",
      "Epoch 316/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0265 - val_loss: 0.0322\n",
      "Epoch 317/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0265 - val_loss: 0.0322\n",
      "Epoch 318/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0264 - val_loss: 0.0323\n",
      "Epoch 319/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0265 - val_loss: 0.0323\n",
      "Epoch 320/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0265 - val_loss: 0.0322\n",
      "Epoch 321/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0263 - val_loss: 0.0320\n",
      "Epoch 322/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0263 - val_loss: 0.0323\n",
      "Epoch 323/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0262 - val_loss: 0.0321\n",
      "Epoch 324/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0261 - val_loss: 0.0320\n",
      "Epoch 325/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0261 - val_loss: 0.0323\n",
      "Epoch 326/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0260 - val_loss: 0.0321\n",
      "Epoch 327/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0260 - val_loss: 0.0319\n",
      "Epoch 328/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0262 - val_loss: 0.0320\n",
      "Epoch 329/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0261 - val_loss: 0.0321\n",
      "Epoch 330/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0260 - val_loss: 0.0317\n",
      "Epoch 331/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0260 - val_loss: 0.0321\n",
      "Epoch 332/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0260 - val_loss: 0.0320\n",
      "Epoch 333/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0260 - val_loss: 0.0319\n",
      "Epoch 334/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0258 - val_loss: 0.0319\n",
      "Epoch 335/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0259 - val_loss: 0.0320\n",
      "Epoch 336/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0258 - val_loss: 0.0324\n",
      "Epoch 337/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0259 - val_loss: 0.0318\n",
      "Epoch 338/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0260 - val_loss: 0.0321\n",
      "Epoch 339/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0262 - val_loss: 0.0318\n",
      "Epoch 340/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0261 - val_loss: 0.0318\n",
      "Epoch 341/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0260 - val_loss: 0.0319\n",
      "Epoch 342/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0259 - val_loss: 0.0317\n",
      "Epoch 343/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0258 - val_loss: 0.0319\n",
      "Epoch 344/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0259 - val_loss: 0.0316\n",
      "Epoch 345/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0256 - val_loss: 0.0317\n",
      "Epoch 346/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0253 - val_loss: 0.0316\n",
      "Epoch 347/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0255 - val_loss: 0.0318\n",
      "Epoch 348/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0254 - val_loss: 0.0319\n",
      "Epoch 349/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0253 - val_loss: 0.0318\n",
      "Epoch 350/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0251 - val_loss: 0.0317\n",
      "Epoch 351/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0250 - val_loss: 0.0318\n",
      "Epoch 352/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0250 - val_loss: 0.0317\n",
      "Epoch 353/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0252 - val_loss: 0.0320\n",
      "Epoch 354/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0250 - val_loss: 0.0316\n",
      "Epoch 355/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0250 - val_loss: 0.0317\n",
      "Epoch 356/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0250 - val_loss: 0.0319\n",
      "Epoch 357/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0251 - val_loss: 0.0316\n",
      "Epoch 358/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0251 - val_loss: 0.0316\n",
      "Epoch 359/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0250 - val_loss: 0.0314\n",
      "Epoch 360/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0251 - val_loss: 0.0316\n",
      "Epoch 361/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0251 - val_loss: 0.0316\n",
      "Epoch 362/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0250 - val_loss: 0.0315\n",
      "Epoch 363/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0251 - val_loss: 0.0315\n",
      "Epoch 364/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0249 - val_loss: 0.0317\n",
      "Epoch 365/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0248 - val_loss: 0.0316\n",
      "Epoch 366/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0246 - val_loss: 0.0316\n",
      "Epoch 367/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0246 - val_loss: 0.0314\n",
      "Epoch 368/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0245 - val_loss: 0.0315\n",
      "Epoch 369/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0244 - val_loss: 0.0316\n",
      "Epoch 370/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0246 - val_loss: 0.0314\n",
      "Epoch 371/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0245 - val_loss: 0.0314\n",
      "Epoch 372/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0245 - val_loss: 0.0315\n",
      "Epoch 373/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0243 - val_loss: 0.0315\n",
      "Epoch 374/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0243 - val_loss: 0.0315\n",
      "Epoch 375/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0246 - val_loss: 0.0314\n",
      "Epoch 376/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0244 - val_loss: 0.0313\n",
      "Epoch 377/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0243 - val_loss: 0.0314\n",
      "Epoch 378/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0244 - val_loss: 0.0314\n",
      "Epoch 379/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0242 - val_loss: 0.0316\n",
      "Epoch 380/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0241 - val_loss: 0.0316\n",
      "Epoch 381/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0240 - val_loss: 0.0315\n",
      "Epoch 382/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0240 - val_loss: 0.0316\n",
      "Epoch 383/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0239 - val_loss: 0.0315\n",
      "Epoch 384/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0240 - val_loss: 0.0317\n",
      "Epoch 385/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0241 - val_loss: 0.0314\n",
      "Epoch 386/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0239 - val_loss: 0.0313\n",
      "Epoch 387/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0240 - val_loss: 0.0319\n",
      "Epoch 388/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0240 - val_loss: 0.0314\n",
      "Epoch 389/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0240 - val_loss: 0.0314\n",
      "Epoch 390/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0239 - val_loss: 0.0316\n",
      "Epoch 391/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 135us/step - loss: 0.0239 - val_loss: 0.0312\n",
      "Epoch 392/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0239 - val_loss: 0.0311\n",
      "Epoch 393/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0239 - val_loss: 0.0313\n",
      "Epoch 394/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0238 - val_loss: 0.0311\n",
      "Epoch 395/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0239 - val_loss: 0.0311\n",
      "Epoch 396/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0237 - val_loss: 0.0316\n",
      "Epoch 397/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0236 - val_loss: 0.0313\n",
      "Epoch 398/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0236 - val_loss: 0.0314\n",
      "Epoch 399/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0236 - val_loss: 0.0314\n",
      "Epoch 400/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0238 - val_loss: 0.0311\n",
      "Epoch 401/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0237 - val_loss: 0.0315\n",
      "Epoch 402/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0239 - val_loss: 0.0312\n",
      "Epoch 403/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0241 - val_loss: 0.0317\n",
      "Epoch 404/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0243 - val_loss: 0.0313\n",
      "Epoch 405/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0243 - val_loss: 0.0313\n",
      "Epoch 406/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0238 - val_loss: 0.0311\n",
      "Epoch 407/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0236 - val_loss: 0.0309\n",
      "Epoch 408/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0235 - val_loss: 0.0312\n",
      "Epoch 409/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0234 - val_loss: 0.0314\n",
      "Epoch 410/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0233 - val_loss: 0.0310\n",
      "Epoch 411/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0232 - val_loss: 0.0313\n",
      "Epoch 412/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0231 - val_loss: 0.0313\n",
      "Epoch 413/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0231 - val_loss: 0.0309\n",
      "Epoch 414/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0231 - val_loss: 0.0312\n",
      "Epoch 415/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0230 - val_loss: 0.0314\n",
      "Epoch 416/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0230 - val_loss: 0.0312\n",
      "Epoch 417/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0230 - val_loss: 0.0315\n",
      "Epoch 418/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0230 - val_loss: 0.0312\n",
      "Epoch 419/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0232 - val_loss: 0.0313\n",
      "Epoch 420/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0232 - val_loss: 0.0318\n",
      "Epoch 421/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0233 - val_loss: 0.0311\n",
      "Epoch 422/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0233 - val_loss: 0.0312\n",
      "Epoch 423/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0234 - val_loss: 0.0313\n",
      "Epoch 424/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0230 - val_loss: 0.0310\n",
      "Epoch 425/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0230 - val_loss: 0.0305\n",
      "Epoch 426/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0231 - val_loss: 0.0313\n",
      "Epoch 427/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0228 - val_loss: 0.0312\n",
      "Epoch 428/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0229 - val_loss: 0.0312\n",
      "Epoch 429/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0229 - val_loss: 0.0313\n",
      "Epoch 430/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0229 - val_loss: 0.0312\n",
      "Epoch 431/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0229 - val_loss: 0.0314\n",
      "Epoch 432/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0226 - val_loss: 0.0310\n",
      "Epoch 433/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0226 - val_loss: 0.0311\n",
      "Epoch 434/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0226 - val_loss: 0.0313\n",
      "Epoch 435/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0225 - val_loss: 0.0312\n",
      "Epoch 436/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0226 - val_loss: 0.0313\n",
      "Epoch 437/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0225 - val_loss: 0.0310\n",
      "Epoch 438/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0224 - val_loss: 0.0311\n",
      "Epoch 439/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0225 - val_loss: 0.0313\n",
      "Epoch 440/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0224 - val_loss: 0.0309\n",
      "Epoch 441/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0224 - val_loss: 0.0312\n",
      "Epoch 442/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0224 - val_loss: 0.0312\n",
      "Epoch 443/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0223 - val_loss: 0.0311\n",
      "Epoch 444/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0224 - val_loss: 0.0308\n",
      "Epoch 445/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0226 - val_loss: 0.0317\n",
      "Epoch 446/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0229 - val_loss: 0.0313\n",
      "Epoch 447/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0226 - val_loss: 0.0314\n",
      "Epoch 448/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0225 - val_loss: 0.0312\n",
      "Epoch 449/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0225 - val_loss: 0.0311\n",
      "Epoch 450/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0223 - val_loss: 0.0311\n",
      "Epoch 451/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0222 - val_loss: 0.0314\n",
      "Epoch 452/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0223 - val_loss: 0.0310\n",
      "Epoch 453/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0223 - val_loss: 0.0310\n",
      "Epoch 454/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0223 - val_loss: 0.0312\n",
      "Epoch 455/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0223 - val_loss: 0.0312\n",
      "Epoch 456/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0224 - val_loss: 0.0313\n",
      "Epoch 457/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0224 - val_loss: 0.0307\n",
      "Epoch 458/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0222 - val_loss: 0.0311\n",
      "Epoch 459/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0221 - val_loss: 0.0312\n",
      "Epoch 460/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0223 - val_loss: 0.0313\n",
      "Epoch 461/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0223 - val_loss: 0.0312\n",
      "Epoch 462/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0223 - val_loss: 0.0314\n",
      "Epoch 463/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0221 - val_loss: 0.0310\n",
      "Epoch 464/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0224 - val_loss: 0.0310\n",
      "Epoch 465/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0224 - val_loss: 0.0316\n",
      "Epoch 466/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0225 - val_loss: 0.0313\n",
      "Epoch 467/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0224 - val_loss: 0.0311\n",
      "Epoch 468/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0225 - val_loss: 0.0316\n",
      "Epoch 469/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 135us/step - loss: 0.0221 - val_loss: 0.0314\n",
      "Epoch 470/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0221 - val_loss: 0.0312\n",
      "Epoch 471/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0221 - val_loss: 0.0310\n",
      "Epoch 472/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0220 - val_loss: 0.0315\n",
      "Epoch 473/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0218 - val_loss: 0.0313\n",
      "Epoch 474/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0220 - val_loss: 0.0314\n",
      "Epoch 475/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0218 - val_loss: 0.0315\n",
      "Epoch 476/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0217 - val_loss: 0.0316\n",
      "Epoch 477/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0216 - val_loss: 0.0312\n",
      "Epoch 478/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0216 - val_loss: 0.0312\n",
      "Epoch 479/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0216 - val_loss: 0.0311\n",
      "Epoch 480/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0216 - val_loss: 0.0312\n",
      "Epoch 481/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0216 - val_loss: 0.0311\n",
      "Epoch 482/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0216 - val_loss: 0.0314\n",
      "Epoch 483/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0214 - val_loss: 0.0310\n",
      "Epoch 484/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0214 - val_loss: 0.0315\n",
      "Epoch 485/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0216 - val_loss: 0.0310\n",
      "Epoch 486/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0218 - val_loss: 0.0313\n",
      "Epoch 487/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0216 - val_loss: 0.0314\n",
      "Epoch 488/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0215 - val_loss: 0.0314\n",
      "Epoch 489/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0215 - val_loss: 0.0317\n",
      "Epoch 490/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0216 - val_loss: 0.0314\n",
      "Epoch 491/500\n",
      "741/741 [==============================] - 0s 162us/step - loss: 0.0220 - val_loss: 0.0317\n",
      "Epoch 492/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0218 - val_loss: 0.0316\n",
      "Epoch 493/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0217 - val_loss: 0.0313\n",
      "Epoch 494/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0216 - val_loss: 0.0314\n",
      "Epoch 495/500\n",
      "741/741 [==============================] - 0s 148us/step - loss: 0.0215 - val_loss: 0.0312\n",
      "Epoch 496/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0213 - val_loss: 0.0317\n",
      "Epoch 497/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0212 - val_loss: 0.0319\n",
      "Epoch 498/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0213 - val_loss: 0.0319\n",
      "Epoch 499/500\n",
      "741/741 [==============================] - 0s 135us/step - loss: 0.0216 - val_loss: 0.0313\n",
      "Epoch 500/500\n",
      "741/741 [==============================] - 0s 121us/step - loss: 0.0216 - val_loss: 0.0317\n"
     ]
    }
   ],
   "source": [
    "denoise_ae = Autoencoder_simple(enc_lay, dec_lay, num_params)\n",
    "denoise_ae.Train_Denoise_autoencoder(Vaal_X_AE_Train, X_AE_Train, X_AE_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  96.95703278046233\n",
      "F1 Score =  0.9281677114376735\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True       4516.0         288.0\n",
      "1  Pred False        411.0       17756.0\n"
     ]
    }
   ],
   "source": [
    "denoise_ae.Test_Accuracy(X_AE_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  96.19354838709677\n",
      "F1 Score =  0.9098318899643402\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True        893.0          76.0\n",
      "1  Pred False        101.0        3580.0\n"
     ]
    }
   ],
   "source": [
    "denoise_ae.Test_Accuracy(X_AE_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  96.67386942429387\n",
      "F1 Score =  0.921307285010042\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True       2523.0         180.0\n",
      "1  Pred False        251.0       10004.0\n"
     ]
    }
   ],
   "source": [
    "denoise_ae.Test_Accuracy(X_AE_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this works pretty well and seems to learn a reasonably robust representation of the data. \n",
    "\n",
    "Experimenting with different probabilities of corruption on the input has been surprising as even a 50% corrupted input still yields a greater than 50% accuracy, although this could be the network learning the patterns in the random number generation more than a representation of the data.\n",
    "\n",
    "Eventually it seems like a corruption of 10% still yields a solid representation with similar accuracy scores to 0 corruption so I will use this as a placeholder value for testing how well the encoded representation can be used by a classifier.\n",
    "\n",
    "To compare I will use a simple 3 way test - comparing the accuracy of a neural network with the same architecture with the earlier layers being the denoising autoencoder, the simple autoencoder and no pretraining respectively on the same binary encoded input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get simple autoencoder inputs\n",
    "S_AE_Train = test_ae.Get_Encoding(X_AE_Train)\n",
    "S_AE_CV = test_ae.Get_Encoding(X_AE_CV)\n",
    "\n",
    "#Get denoising autoencoder inputs\n",
    "D_AE_Train = denoise_ae.Get_Encoding(X_AE_Train)\n",
    "D_AE_CV = denoise_ae.Get_Encoding(X_AE_CV)\n",
    "\n",
    "#Generate autoencoder networks\n",
    "AE_layers = [10, 7, 5, 5, 3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "741/741 [==============================] - 1s 2ms/step - loss: 0.7804 - acc: 0.3522\n",
      "Epoch 2/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.7285 - acc: 0.5614\n",
      "Epoch 3/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.7058 - acc: 0.6977\n",
      "Epoch 4/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.6941 - acc: 0.7031\n",
      "Epoch 5/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.6849 - acc: 0.7328\n",
      "Epoch 6/500\n",
      "741/741 [==============================] - 0s 301us/step - loss: 0.6774 - acc: 0.7584\n",
      "Epoch 7/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.6698 - acc: 0.7625\n",
      "Epoch 8/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.6632 - acc: 0.7625\n",
      "Epoch 9/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.6559 - acc: 0.7679\n",
      "Epoch 10/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.6492 - acc: 0.7706\n",
      "Epoch 11/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.6417 - acc: 0.7800\n",
      "Epoch 12/500\n",
      "741/741 [==============================] - 0s 301us/step - loss: 0.6351 - acc: 0.7841\n",
      "Epoch 13/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.6285 - acc: 0.7841 0s - loss: 0.6256 - acc: 0.799\n",
      "Epoch 14/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.6213 - acc: 0.7854\n",
      "Epoch 15/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.6152 - acc: 0.7881\n",
      "Epoch 16/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.6097 - acc: 0.7976\n",
      "Epoch 17/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.6040 - acc: 0.7989\n",
      "Epoch 18/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.5987 - acc: 0.7989\n",
      "Epoch 19/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.5929 - acc: 0.8057\n",
      "Epoch 20/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.5877 - acc: 0.8070\n",
      "Epoch 21/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.5826 - acc: 0.8178\n",
      "Epoch 22/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.5778 - acc: 0.8192\n",
      "Epoch 23/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.5731 - acc: 0.8178\n",
      "Epoch 24/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.5688 - acc: 0.8192\n",
      "Epoch 25/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.5651 - acc: 0.8165\n",
      "Epoch 26/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.5618 - acc: 0.8165\n",
      "Epoch 27/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.5591 - acc: 0.8111\n",
      "Epoch 28/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.5530 - acc: 0.8205\n",
      "Epoch 29/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.5490 - acc: 0.8219\n",
      "Epoch 30/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.5456 - acc: 0.8219\n",
      "Epoch 31/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.5435 - acc: 0.8205\n",
      "Epoch 32/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.5399 - acc: 0.8232\n",
      "Epoch 33/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.5364 - acc: 0.8246\n",
      "Epoch 34/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.5344 - acc: 0.8205\n",
      "Epoch 35/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.5329 - acc: 0.8219 0s - loss: 0.5235 - acc: 0.83\n",
      "Epoch 36/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.5286 - acc: 0.8232\n",
      "Epoch 37/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.5265 - acc: 0.8259\n",
      "Epoch 38/500\n",
      "741/741 [==============================] - 0s 305us/step - loss: 0.5238 - acc: 0.8205\n",
      "Epoch 39/500\n",
      "741/741 [==============================] - 0s 305us/step - loss: 0.5215 - acc: 0.8192\n",
      "Epoch 40/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.5197 - acc: 0.8219\n",
      "Epoch 41/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.5196 - acc: 0.8232\n",
      "Epoch 42/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.5156 - acc: 0.8246\n",
      "Epoch 43/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.5155 - acc: 0.8205\n",
      "Epoch 44/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.5145 - acc: 0.8232\n",
      "Epoch 45/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.5131 - acc: 0.8192\n",
      "Epoch 46/500\n",
      "741/741 [==============================] - 0s 305us/step - loss: 0.5085 - acc: 0.8232\n",
      "Epoch 47/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.5081 - acc: 0.8205\n",
      "Epoch 48/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.5048 - acc: 0.8178\n",
      "Epoch 49/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.5012 - acc: 0.8165\n",
      "Epoch 50/500\n",
      "741/741 [==============================] - 0s 305us/step - loss: 0.4997 - acc: 0.8259\n",
      "Epoch 51/500\n",
      "741/741 [==============================] - 0s 301us/step - loss: 0.4971 - acc: 0.8178\n",
      "Epoch 52/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4970 - acc: 0.8300\n",
      "Epoch 53/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.4955 - acc: 0.8273\n",
      "Epoch 54/500\n",
      "741/741 [==============================] - 0s 301us/step - loss: 0.4967 - acc: 0.8246\n",
      "Epoch 55/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.4919 - acc: 0.8219\n",
      "Epoch 56/500\n",
      "741/741 [==============================] - 0s 301us/step - loss: 0.4905 - acc: 0.8219\n",
      "Epoch 57/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4889 - acc: 0.8219\n",
      "Epoch 58/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4865 - acc: 0.8219\n",
      "Epoch 59/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.4850 - acc: 0.8259\n",
      "Epoch 60/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4845 - acc: 0.8232\n",
      "Epoch 61/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4838 - acc: 0.8286\n",
      "Epoch 62/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.4815 - acc: 0.8313\n",
      "Epoch 63/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.4813 - acc: 0.8259\n",
      "Epoch 64/500\n",
      "741/741 [==============================] - 0s 333us/step - loss: 0.4794 - acc: 0.8340\n",
      "Epoch 65/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4781 - acc: 0.8327\n",
      "Epoch 66/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4770 - acc: 0.8354\n",
      "Epoch 67/500\n",
      "741/741 [==============================] - 0s 381us/step - loss: 0.4752 - acc: 0.8340\n",
      "Epoch 68/500\n",
      "741/741 [==============================] - 0s 363us/step - loss: 0.4770 - acc: 0.8286\n",
      "Epoch 69/500\n",
      "741/741 [==============================] - 0s 327us/step - loss: 0.4730 - acc: 0.8340\n",
      "Epoch 70/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4722 - acc: 0.8381\n",
      "Epoch 71/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.4708 - acc: 0.8340\n",
      "Epoch 72/500\n",
      "741/741 [==============================] - 0s 301us/step - loss: 0.4703 - acc: 0.8340\n",
      "Epoch 73/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4694 - acc: 0.8367\n",
      "Epoch 74/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.4686 - acc: 0.8354\n",
      "Epoch 75/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4675 - acc: 0.8354\n",
      "Epoch 76/500\n",
      "741/741 [==============================] - 0s 305us/step - loss: 0.4679 - acc: 0.8354\n",
      "Epoch 77/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.4660 - acc: 0.8354\n",
      "Epoch 78/500\n",
      "741/741 [==============================] - 0s 301us/step - loss: 0.4694 - acc: 0.8286\n",
      "Epoch 79/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4658 - acc: 0.8313\n",
      "Epoch 80/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4652 - acc: 0.8327\n",
      "Epoch 81/500\n",
      "741/741 [==============================] - 0s 305us/step - loss: 0.4623 - acc: 0.8381\n",
      "Epoch 82/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.4608 - acc: 0.8381\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 297us/step - loss: 0.4631 - acc: 0.8381\n",
      "Epoch 84/500\n",
      "741/741 [==============================] - 0s 282us/step - loss: 0.4587 - acc: 0.8367\n",
      "Epoch 85/500\n",
      "741/741 [==============================] - 0s 283us/step - loss: 0.4629 - acc: 0.8313\n",
      "Epoch 86/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4626 - acc: 0.8286\n",
      "Epoch 87/500\n",
      "741/741 [==============================] - 0s 285us/step - loss: 0.4571 - acc: 0.8340\n",
      "Epoch 88/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4572 - acc: 0.8394\n",
      "Epoch 89/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4567 - acc: 0.8340\n",
      "Epoch 90/500\n",
      "741/741 [==============================] - 0s 285us/step - loss: 0.4567 - acc: 0.8313\n",
      "Epoch 91/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4568 - acc: 0.8367\n",
      "Epoch 92/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4567 - acc: 0.8300\n",
      "Epoch 93/500\n",
      "741/741 [==============================] - 0s 285us/step - loss: 0.4537 - acc: 0.8327\n",
      "Epoch 94/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4558 - acc: 0.8327\n",
      "Epoch 95/500\n",
      "741/741 [==============================] - 0s 282us/step - loss: 0.4533 - acc: 0.8381\n",
      "Epoch 96/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4560 - acc: 0.8259\n",
      "Epoch 97/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4528 - acc: 0.8340\n",
      "Epoch 98/500\n",
      "741/741 [==============================] - 0s 283us/step - loss: 0.4525 - acc: 0.8367\n",
      "Epoch 99/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.4515 - acc: 0.8381\n",
      "Epoch 100/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4518 - acc: 0.8340\n",
      "Epoch 101/500\n",
      "741/741 [==============================] - 0s 285us/step - loss: 0.4510 - acc: 0.8381\n",
      "Epoch 102/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4494 - acc: 0.8408\n",
      "Epoch 103/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.4500 - acc: 0.8367\n",
      "Epoch 104/500\n",
      "741/741 [==============================] - 0s 305us/step - loss: 0.4482 - acc: 0.8394\n",
      "Epoch 105/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4532 - acc: 0.8327\n",
      "Epoch 106/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.4491 - acc: 0.8367\n",
      "Epoch 107/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4494 - acc: 0.8381\n",
      "Epoch 108/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4479 - acc: 0.8381\n",
      "Epoch 109/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4506 - acc: 0.8354\n",
      "Epoch 110/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4471 - acc: 0.8421\n",
      "Epoch 111/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4461 - acc: 0.8394\n",
      "Epoch 112/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4461 - acc: 0.8394\n",
      "Epoch 113/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4457 - acc: 0.8340\n",
      "Epoch 114/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4457 - acc: 0.8408\n",
      "Epoch 115/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4514 - acc: 0.8340\n",
      "Epoch 116/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4435 - acc: 0.8381\n",
      "Epoch 117/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4430 - acc: 0.8421\n",
      "Epoch 118/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4430 - acc: 0.8394\n",
      "Epoch 119/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4440 - acc: 0.8300\n",
      "Epoch 120/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4419 - acc: 0.8381\n",
      "Epoch 121/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4426 - acc: 0.8421\n",
      "Epoch 122/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4429 - acc: 0.8327\n",
      "Epoch 123/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4397 - acc: 0.8394\n",
      "Epoch 124/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4473 - acc: 0.8327\n",
      "Epoch 125/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4392 - acc: 0.8381\n",
      "Epoch 126/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.4399 - acc: 0.8421\n",
      "Epoch 127/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4403 - acc: 0.8421\n",
      "Epoch 128/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.4399 - acc: 0.8435\n",
      "Epoch 129/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4422 - acc: 0.8340\n",
      "Epoch 130/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4387 - acc: 0.8435\n",
      "Epoch 131/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4378 - acc: 0.8408\n",
      "Epoch 132/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4374 - acc: 0.8408\n",
      "Epoch 133/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4373 - acc: 0.8394\n",
      "Epoch 134/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4377 - acc: 0.8394\n",
      "Epoch 135/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4391 - acc: 0.8381\n",
      "Epoch 136/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4392 - acc: 0.8327\n",
      "Epoch 137/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4359 - acc: 0.8394\n",
      "Epoch 138/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4360 - acc: 0.8394\n",
      "Epoch 139/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4357 - acc: 0.8354\n",
      "Epoch 140/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4371 - acc: 0.8381\n",
      "Epoch 141/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4341 - acc: 0.8435\n",
      "Epoch 142/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4363 - acc: 0.8394\n",
      "Epoch 143/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4353 - acc: 0.8394\n",
      "Epoch 144/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4375 - acc: 0.8394\n",
      "Epoch 145/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4342 - acc: 0.8421\n",
      "Epoch 146/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4334 - acc: 0.8408\n",
      "Epoch 147/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4336 - acc: 0.8462\n",
      "Epoch 148/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4334 - acc: 0.8435\n",
      "Epoch 149/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4321 - acc: 0.8421\n",
      "Epoch 150/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4354 - acc: 0.8394\n",
      "Epoch 151/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4341 - acc: 0.8435\n",
      "Epoch 152/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4327 - acc: 0.8381\n",
      "Epoch 153/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4343 - acc: 0.8435\n",
      "Epoch 154/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4381 - acc: 0.8421\n",
      "Epoch 155/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4324 - acc: 0.8435\n",
      "Epoch 156/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4392 - acc: 0.8354\n",
      "Epoch 157/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4338 - acc: 0.8435\n",
      "Epoch 158/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4317 - acc: 0.8421\n",
      "Epoch 159/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4307 - acc: 0.8408\n",
      "Epoch 160/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4316 - acc: 0.8421\n",
      "Epoch 161/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4329 - acc: 0.8421\n",
      "Epoch 162/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4292 - acc: 0.8435\n",
      "Epoch 163/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4359 - acc: 0.8313\n",
      "Epoch 164/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4329 - acc: 0.8300\n",
      "Epoch 165/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 290us/step - loss: 0.4306 - acc: 0.8421\n",
      "Epoch 166/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4297 - acc: 0.8448\n",
      "Epoch 167/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4288 - acc: 0.8448\n",
      "Epoch 168/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4307 - acc: 0.8327\n",
      "Epoch 169/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4317 - acc: 0.8340\n",
      "Epoch 170/500\n",
      "741/741 [==============================] - 0s 285us/step - loss: 0.4275 - acc: 0.8448\n",
      "Epoch 171/500\n",
      "741/741 [==============================] - 0s 285us/step - loss: 0.4344 - acc: 0.8421\n",
      "Epoch 172/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4336 - acc: 0.8300\n",
      "Epoch 173/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4332 - acc: 0.8408\n",
      "Epoch 174/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4311 - acc: 0.8421\n",
      "Epoch 175/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.4307 - acc: 0.8381\n",
      "Epoch 176/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.4278 - acc: 0.8408\n",
      "Epoch 177/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.4281 - acc: 0.8408\n",
      "Epoch 178/500\n",
      "741/741 [==============================] - 0s 331us/step - loss: 0.4273 - acc: 0.8421\n",
      "Epoch 179/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4280 - acc: 0.8421\n",
      "Epoch 180/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4281 - acc: 0.8448 0s - loss: 0.4031 - acc: 0.8\n",
      "Epoch 181/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4267 - acc: 0.8448\n",
      "Epoch 182/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4245 - acc: 0.8462\n",
      "Epoch 183/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4260 - acc: 0.8448\n",
      "Epoch 184/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4267 - acc: 0.8435\n",
      "Epoch 185/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4263 - acc: 0.8462\n",
      "Epoch 186/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4259 - acc: 0.8435\n",
      "Epoch 187/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4242 - acc: 0.8421\n",
      "Epoch 188/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4251 - acc: 0.8435\n",
      "Epoch 189/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4247 - acc: 0.8421\n",
      "Epoch 190/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.4228 - acc: 0.8448\n",
      "Epoch 191/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4235 - acc: 0.8462\n",
      "Epoch 192/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4272 - acc: 0.8435\n",
      "Epoch 193/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4247 - acc: 0.8421\n",
      "Epoch 194/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4231 - acc: 0.8421\n",
      "Epoch 195/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4247 - acc: 0.8408\n",
      "Epoch 196/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4227 - acc: 0.8462\n",
      "Epoch 197/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4246 - acc: 0.8435\n",
      "Epoch 198/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4270 - acc: 0.8381\n",
      "Epoch 199/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4393 - acc: 0.8381\n",
      "Epoch 200/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4265 - acc: 0.8421\n",
      "Epoch 201/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4248 - acc: 0.8394\n",
      "Epoch 202/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4221 - acc: 0.8462\n",
      "Epoch 203/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4227 - acc: 0.8421\n",
      "Epoch 204/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4200 - acc: 0.8462\n",
      "Epoch 205/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4225 - acc: 0.8381\n",
      "Epoch 206/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4186 - acc: 0.8462\n",
      "Epoch 207/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4199 - acc: 0.8448\n",
      "Epoch 208/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4203 - acc: 0.8475\n",
      "Epoch 209/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.4223 - acc: 0.8408\n",
      "Epoch 210/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4212 - acc: 0.8354\n",
      "Epoch 211/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4181 - acc: 0.8475\n",
      "Epoch 212/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4224 - acc: 0.8367\n",
      "Epoch 213/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.4230 - acc: 0.8421\n",
      "Epoch 214/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4155 - acc: 0.8462\n",
      "Epoch 215/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.4195 - acc: 0.8462\n",
      "Epoch 216/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4173 - acc: 0.8421\n",
      "Epoch 217/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4187 - acc: 0.8489\n",
      "Epoch 218/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4192 - acc: 0.8408\n",
      "Epoch 219/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4188 - acc: 0.8435\n",
      "Epoch 220/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.4187 - acc: 0.8475\n",
      "Epoch 221/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4172 - acc: 0.8489\n",
      "Epoch 222/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4191 - acc: 0.8435\n",
      "Epoch 223/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4167 - acc: 0.8435\n",
      "Epoch 224/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4232 - acc: 0.8394\n",
      "Epoch 225/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.4153 - acc: 0.8489\n",
      "Epoch 226/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.4153 - acc: 0.8475\n",
      "Epoch 227/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.4160 - acc: 0.8448\n",
      "Epoch 228/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4158 - acc: 0.8475\n",
      "Epoch 229/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.4150 - acc: 0.8462\n",
      "Epoch 230/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4158 - acc: 0.8462\n",
      "Epoch 231/500\n",
      "741/741 [==============================] - 0s 305us/step - loss: 0.4167 - acc: 0.8421\n",
      "Epoch 232/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4162 - acc: 0.8489\n",
      "Epoch 233/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4162 - acc: 0.8421\n",
      "Epoch 234/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4146 - acc: 0.8435\n",
      "Epoch 235/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.4133 - acc: 0.8448\n",
      "Epoch 236/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4208 - acc: 0.8367\n",
      "Epoch 237/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4150 - acc: 0.8475\n",
      "Epoch 238/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4175 - acc: 0.8475\n",
      "Epoch 239/500\n",
      "741/741 [==============================] - 0s 305us/step - loss: 0.4166 - acc: 0.8421\n",
      "Epoch 240/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4152 - acc: 0.8435\n",
      "Epoch 241/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.4154 - acc: 0.8448\n",
      "Epoch 242/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.4145 - acc: 0.8462\n",
      "Epoch 243/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4199 - acc: 0.8475\n",
      "Epoch 244/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4171 - acc: 0.8394\n",
      "Epoch 245/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.4158 - acc: 0.8381\n",
      "Epoch 246/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 292us/step - loss: 0.4172 - acc: 0.8408\n",
      "Epoch 247/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.4183 - acc: 0.8367\n",
      "Epoch 248/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.4147 - acc: 0.8489\n",
      "Epoch 249/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4143 - acc: 0.8435\n",
      "Epoch 250/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.4144 - acc: 0.8435\n",
      "Epoch 251/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.4134 - acc: 0.8462\n",
      "Epoch 252/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4158 - acc: 0.8367\n",
      "Epoch 253/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4145 - acc: 0.8462\n",
      "Epoch 254/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4130 - acc: 0.8421\n",
      "Epoch 255/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.4222 - acc: 0.8475\n",
      "Epoch 256/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4137 - acc: 0.8408\n",
      "Epoch 257/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4137 - acc: 0.8394\n",
      "Epoch 258/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4130 - acc: 0.8448\n",
      "Epoch 259/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4214 - acc: 0.8367\n",
      "Epoch 260/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4122 - acc: 0.8408\n",
      "Epoch 261/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4144 - acc: 0.8435\n",
      "Epoch 262/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4165 - acc: 0.8394\n",
      "Epoch 263/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4211 - acc: 0.8367\n",
      "Epoch 264/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.4134 - acc: 0.8408\n",
      "Epoch 265/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4114 - acc: 0.8408\n",
      "Epoch 266/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4125 - acc: 0.8421\n",
      "Epoch 267/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4126 - acc: 0.8448\n",
      "Epoch 268/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4183 - acc: 0.8367\n",
      "Epoch 269/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4142 - acc: 0.8394\n",
      "Epoch 270/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4112 - acc: 0.8394\n",
      "Epoch 271/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4112 - acc: 0.8462\n",
      "Epoch 272/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4129 - acc: 0.8408\n",
      "Epoch 273/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4112 - acc: 0.8435\n",
      "Epoch 274/500\n",
      "741/741 [==============================] - 0s 409us/step - loss: 0.4114 - acc: 0.8421\n",
      "Epoch 275/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4157 - acc: 0.8421\n",
      "Epoch 276/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4093 - acc: 0.8435\n",
      "Epoch 277/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4155 - acc: 0.8448\n",
      "Epoch 278/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.4121 - acc: 0.8367\n",
      "Epoch 279/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4112 - acc: 0.8421\n",
      "Epoch 280/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.4119 - acc: 0.8394\n",
      "Epoch 281/500\n",
      "741/741 [==============================] - 0s 331us/step - loss: 0.4099 - acc: 0.8475\n",
      "Epoch 282/500\n",
      "741/741 [==============================] - 0s 329us/step - loss: 0.4119 - acc: 0.8448\n",
      "Epoch 283/500\n",
      "741/741 [==============================] - 0s 332us/step - loss: 0.4100 - acc: 0.8462\n",
      "Epoch 284/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4121 - acc: 0.8448\n",
      "Epoch 285/500\n",
      "741/741 [==============================] - 0s 329us/step - loss: 0.4084 - acc: 0.8475\n",
      "Epoch 286/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4086 - acc: 0.8475\n",
      "Epoch 287/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.4112 - acc: 0.8475\n",
      "Epoch 288/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.4178 - acc: 0.8340\n",
      "Epoch 289/500\n",
      "741/741 [==============================] - ETA: 0s - loss: 0.4083 - acc: 0.841 - 0s 327us/step - loss: 0.4107 - acc: 0.8394\n",
      "Epoch 290/500\n",
      "741/741 [==============================] - 0s 332us/step - loss: 0.4105 - acc: 0.8475\n",
      "Epoch 291/500\n",
      "741/741 [==============================] - 0s 332us/step - loss: 0.4094 - acc: 0.8462\n",
      "Epoch 292/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4111 - acc: 0.8408\n",
      "Epoch 293/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.4090 - acc: 0.8475\n",
      "Epoch 294/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4082 - acc: 0.8448\n",
      "Epoch 295/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.4086 - acc: 0.8435\n",
      "Epoch 296/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.4101 - acc: 0.8448\n",
      "Epoch 297/500\n",
      "741/741 [==============================] - 0s 328us/step - loss: 0.4105 - acc: 0.8435\n",
      "Epoch 298/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4088 - acc: 0.8448\n",
      "Epoch 299/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.4140 - acc: 0.8354\n",
      "Epoch 300/500\n",
      "741/741 [==============================] - 0s 332us/step - loss: 0.4102 - acc: 0.8394\n",
      "Epoch 301/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4105 - acc: 0.8381\n",
      "Epoch 302/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4107 - acc: 0.8448\n",
      "Epoch 303/500\n",
      "741/741 [==============================] - 0s 328us/step - loss: 0.4077 - acc: 0.8502\n",
      "Epoch 304/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4076 - acc: 0.8475\n",
      "Epoch 305/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.4089 - acc: 0.8448\n",
      "Epoch 306/500\n",
      "741/741 [==============================] - 0s 336us/step - loss: 0.4090 - acc: 0.8448\n",
      "Epoch 307/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4106 - acc: 0.8475\n",
      "Epoch 308/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4136 - acc: 0.8421\n",
      "Epoch 309/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4090 - acc: 0.8462\n",
      "Epoch 310/500\n",
      "741/741 [==============================] - 0s 329us/step - loss: 0.4073 - acc: 0.8462\n",
      "Epoch 311/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4077 - acc: 0.8475\n",
      "Epoch 312/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4107 - acc: 0.8448\n",
      "Epoch 313/500\n",
      "741/741 [==============================] - 0s 329us/step - loss: 0.4077 - acc: 0.8408\n",
      "Epoch 314/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4136 - acc: 0.8394\n",
      "Epoch 315/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4087 - acc: 0.8462\n",
      "Epoch 316/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4088 - acc: 0.8435\n",
      "Epoch 317/500\n",
      "741/741 [==============================] - 0s 333us/step - loss: 0.4098 - acc: 0.8462\n",
      "Epoch 318/500\n",
      "741/741 [==============================] - 0s 359us/step - loss: 0.4101 - acc: 0.8462\n",
      "Epoch 319/500\n",
      "741/741 [==============================] - 0s 368us/step - loss: 0.4076 - acc: 0.8394\n",
      "Epoch 320/500\n",
      "741/741 [==============================] - 0s 343us/step - loss: 0.4105 - acc: 0.8408\n",
      "Epoch 321/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.4075 - acc: 0.8462\n",
      "Epoch 322/500\n",
      "741/741 [==============================] - 0s 333us/step - loss: 0.4095 - acc: 0.8408\n",
      "Epoch 323/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.4138 - acc: 0.8462\n",
      "Epoch 324/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4079 - acc: 0.8448\n",
      "Epoch 325/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4105 - acc: 0.8448\n",
      "Epoch 326/500\n",
      "741/741 [==============================] - 0s 327us/step - loss: 0.4065 - acc: 0.8475\n",
      "Epoch 327/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 309us/step - loss: 0.4077 - acc: 0.8435\n",
      "Epoch 328/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.4086 - acc: 0.8381\n",
      "Epoch 329/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4063 - acc: 0.8475\n",
      "Epoch 330/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4059 - acc: 0.8475\n",
      "Epoch 331/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4105 - acc: 0.8381\n",
      "Epoch 332/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4112 - acc: 0.8435\n",
      "Epoch 333/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4096 - acc: 0.8448\n",
      "Epoch 334/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4089 - acc: 0.8435\n",
      "Epoch 335/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4066 - acc: 0.8408\n",
      "Epoch 336/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4062 - acc: 0.8489\n",
      "Epoch 337/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.4059 - acc: 0.8448\n",
      "Epoch 338/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.4070 - acc: 0.8462\n",
      "Epoch 339/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4080 - acc: 0.8394\n",
      "Epoch 340/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4086 - acc: 0.8408\n",
      "Epoch 341/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4158 - acc: 0.8381\n",
      "Epoch 342/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4062 - acc: 0.8462\n",
      "Epoch 343/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4057 - acc: 0.8462\n",
      "Epoch 344/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4066 - acc: 0.8475\n",
      "Epoch 345/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4056 - acc: 0.8448\n",
      "Epoch 346/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.4057 - acc: 0.8394\n",
      "Epoch 347/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4050 - acc: 0.8435\n",
      "Epoch 348/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4057 - acc: 0.8435\n",
      "Epoch 349/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4081 - acc: 0.8354\n",
      "Epoch 350/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4091 - acc: 0.8448\n",
      "Epoch 351/500\n",
      "741/741 [==============================] - 0s 301us/step - loss: 0.4084 - acc: 0.8435\n",
      "Epoch 352/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.4079 - acc: 0.8381\n",
      "Epoch 353/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.4136 - acc: 0.8421\n",
      "Epoch 354/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4050 - acc: 0.8462\n",
      "Epoch 355/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4064 - acc: 0.8475\n",
      "Epoch 356/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4087 - acc: 0.8435\n",
      "Epoch 357/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4079 - acc: 0.8489\n",
      "Epoch 358/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.4106 - acc: 0.8394\n",
      "Epoch 359/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.4062 - acc: 0.8421\n",
      "Epoch 360/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.4068 - acc: 0.8462\n",
      "Epoch 361/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.4131 - acc: 0.8421\n",
      "Epoch 362/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4048 - acc: 0.8448\n",
      "Epoch 363/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4098 - acc: 0.8448\n",
      "Epoch 364/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.4099 - acc: 0.8462\n",
      "Epoch 365/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4122 - acc: 0.8408\n",
      "Epoch 366/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.4091 - acc: 0.8435\n",
      "Epoch 367/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4133 - acc: 0.8286\n",
      "Epoch 368/500\n",
      "741/741 [==============================] - 0s 305us/step - loss: 0.4074 - acc: 0.8421\n",
      "Epoch 369/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4115 - acc: 0.8354\n",
      "Epoch 370/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4101 - acc: 0.8421\n",
      "Epoch 371/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.4132 - acc: 0.8448\n",
      "Epoch 372/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4077 - acc: 0.8421\n",
      "Epoch 373/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4158 - acc: 0.8340\n",
      "Epoch 374/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4056 - acc: 0.8489\n",
      "Epoch 375/500\n",
      "741/741 [==============================] - 0s 329us/step - loss: 0.4058 - acc: 0.8448\n",
      "Epoch 376/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4079 - acc: 0.8327\n",
      "Epoch 377/500\n",
      "741/741 [==============================] - 0s 332us/step - loss: 0.4267 - acc: 0.8327\n",
      "Epoch 378/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4074 - acc: 0.8421\n",
      "Epoch 379/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4078 - acc: 0.8448\n",
      "Epoch 380/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4045 - acc: 0.8435\n",
      "Epoch 381/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.4049 - acc: 0.8462\n",
      "Epoch 382/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4122 - acc: 0.8394\n",
      "Epoch 383/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.4128 - acc: 0.8394\n",
      "Epoch 384/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4116 - acc: 0.8408\n",
      "Epoch 385/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4095 - acc: 0.8381\n",
      "Epoch 386/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4047 - acc: 0.8475\n",
      "Epoch 387/500\n",
      "741/741 [==============================] - 0s 347us/step - loss: 0.4066 - acc: 0.8421\n",
      "Epoch 388/500\n",
      "741/741 [==============================] - 0s 333us/step - loss: 0.4015 - acc: 0.8475\n",
      "Epoch 389/500\n",
      "741/741 [==============================] - 0s 351us/step - loss: 0.4137 - acc: 0.8408\n",
      "Epoch 390/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4056 - acc: 0.8448\n",
      "Epoch 391/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4027 - acc: 0.8462\n",
      "Epoch 392/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4025 - acc: 0.8462\n",
      "Epoch 393/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4034 - acc: 0.8340\n",
      "Epoch 394/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4085 - acc: 0.8367\n",
      "Epoch 395/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4011 - acc: 0.8502\n",
      "Epoch 396/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.4052 - acc: 0.8462\n",
      "Epoch 397/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.4082 - acc: 0.8435\n",
      "Epoch 398/500\n",
      "741/741 [==============================] - 0s 327us/step - loss: 0.4071 - acc: 0.8448\n",
      "Epoch 399/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4054 - acc: 0.8408\n",
      "Epoch 400/500\n",
      "741/741 [==============================] - 0s 305us/step - loss: 0.4038 - acc: 0.8462\n",
      "Epoch 401/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4079 - acc: 0.8462\n",
      "Epoch 402/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4051 - acc: 0.8408\n",
      "Epoch 403/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4027 - acc: 0.8475\n",
      "Epoch 404/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4021 - acc: 0.8475\n",
      "Epoch 405/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4047 - acc: 0.8408\n",
      "Epoch 406/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.4071 - acc: 0.8475\n",
      "Epoch 407/500\n",
      "741/741 [==============================] - 0s 329us/step - loss: 0.4082 - acc: 0.8448\n",
      "Epoch 408/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4051 - acc: 0.8408\n",
      "Epoch 409/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 316us/step - loss: 0.4057 - acc: 0.8435\n",
      "Epoch 410/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4029 - acc: 0.8502\n",
      "Epoch 411/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.4025 - acc: 0.8448\n",
      "Epoch 412/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.4032 - acc: 0.8462\n",
      "Epoch 413/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.4086 - acc: 0.8462\n",
      "Epoch 414/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4045 - acc: 0.8462\n",
      "Epoch 415/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.4037 - acc: 0.8462\n",
      "Epoch 416/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.4013 - acc: 0.8489\n",
      "Epoch 417/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.4061 - acc: 0.8394\n",
      "Epoch 418/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4052 - acc: 0.8421\n",
      "Epoch 419/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4022 - acc: 0.8502\n",
      "Epoch 420/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4025 - acc: 0.8475\n",
      "Epoch 421/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4021 - acc: 0.8502\n",
      "Epoch 422/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.4024 - acc: 0.8489\n",
      "Epoch 423/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.4014 - acc: 0.8475\n",
      "Epoch 424/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4026 - acc: 0.8462\n",
      "Epoch 425/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4023 - acc: 0.8421\n",
      "Epoch 426/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4044 - acc: 0.8462\n",
      "Epoch 427/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4060 - acc: 0.8367\n",
      "Epoch 428/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4091 - acc: 0.8462\n",
      "Epoch 429/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4038 - acc: 0.8408\n",
      "Epoch 430/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4014 - acc: 0.8502\n",
      "Epoch 431/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.4024 - acc: 0.8475\n",
      "Epoch 432/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.3992 - acc: 0.8489\n",
      "Epoch 433/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4004 - acc: 0.8462\n",
      "Epoch 434/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4050 - acc: 0.8475\n",
      "Epoch 435/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.4073 - acc: 0.8475\n",
      "Epoch 436/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4038 - acc: 0.8435\n",
      "Epoch 437/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4038 - acc: 0.8475\n",
      "Epoch 438/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4005 - acc: 0.8489\n",
      "Epoch 439/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4018 - acc: 0.8462\n",
      "Epoch 440/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.4001 - acc: 0.8448\n",
      "Epoch 441/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.4018 - acc: 0.8502\n",
      "Epoch 442/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4022 - acc: 0.8516\n",
      "Epoch 443/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4032 - acc: 0.8421\n",
      "Epoch 444/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.4007 - acc: 0.8489\n",
      "Epoch 445/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.4053 - acc: 0.8408\n",
      "Epoch 446/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.4022 - acc: 0.8502\n",
      "Epoch 447/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.4084 - acc: 0.8394\n",
      "Epoch 448/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.3996 - acc: 0.8475\n",
      "Epoch 449/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4047 - acc: 0.8408\n",
      "Epoch 450/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.4028 - acc: 0.8462\n",
      "Epoch 451/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.4021 - acc: 0.8448\n",
      "Epoch 452/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.4005 - acc: 0.8475\n",
      "Epoch 453/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4075 - acc: 0.8475\n",
      "Epoch 454/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4095 - acc: 0.8435\n",
      "Epoch 455/500\n",
      "741/741 [==============================] - 0s 336us/step - loss: 0.4074 - acc: 0.8408\n",
      "Epoch 456/500\n",
      "741/741 [==============================] - 0s 345us/step - loss: 0.4061 - acc: 0.8408\n",
      "Epoch 457/500\n",
      "741/741 [==============================] - 0s 352us/step - loss: 0.4002 - acc: 0.8489\n",
      "Epoch 458/500\n",
      "741/741 [==============================] - 0s 344us/step - loss: 0.4019 - acc: 0.8408\n",
      "Epoch 459/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4024 - acc: 0.8502\n",
      "Epoch 460/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4016 - acc: 0.8475\n",
      "Epoch 461/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4045 - acc: 0.8502\n",
      "Epoch 462/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4081 - acc: 0.8421\n",
      "Epoch 463/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4027 - acc: 0.8462\n",
      "Epoch 464/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.4035 - acc: 0.8408\n",
      "Epoch 465/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4075 - acc: 0.8448\n",
      "Epoch 466/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.4138 - acc: 0.8421\n",
      "Epoch 467/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4159 - acc: 0.8300\n",
      "Epoch 468/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4114 - acc: 0.8408\n",
      "Epoch 469/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4145 - acc: 0.8448\n",
      "Epoch 470/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.4092 - acc: 0.8421\n",
      "Epoch 471/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4142 - acc: 0.8367\n",
      "Epoch 472/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4078 - acc: 0.8462\n",
      "Epoch 473/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4092 - acc: 0.8394\n",
      "Epoch 474/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.4060 - acc: 0.8435\n",
      "Epoch 475/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4083 - acc: 0.8381\n",
      "Epoch 476/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4095 - acc: 0.8394\n",
      "Epoch 477/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.4134 - acc: 0.8421\n",
      "Epoch 478/500\n",
      "741/741 [==============================] - 0s 327us/step - loss: 0.4072 - acc: 0.8502\n",
      "Epoch 479/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4138 - acc: 0.8435\n",
      "Epoch 480/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4130 - acc: 0.8421\n",
      "Epoch 481/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4102 - acc: 0.8435\n",
      "Epoch 482/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4113 - acc: 0.8381\n",
      "Epoch 483/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4087 - acc: 0.8421\n",
      "Epoch 484/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4087 - acc: 0.8408\n",
      "Epoch 485/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4085 - acc: 0.8421\n",
      "Epoch 486/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4112 - acc: 0.8408\n",
      "Epoch 487/500\n",
      "741/741 [==============================] - 0s 327us/step - loss: 0.4164 - acc: 0.8354\n",
      "Epoch 488/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4092 - acc: 0.8381\n",
      "Epoch 489/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4104 - acc: 0.8367\n",
      "Epoch 490/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4082 - acc: 0.8367\n",
      "Epoch 491/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 321us/step - loss: 0.4052 - acc: 0.8475\n",
      "Epoch 492/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.4100 - acc: 0.8381\n",
      "Epoch 493/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4069 - acc: 0.8462\n",
      "Epoch 494/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4103 - acc: 0.8448\n",
      "Epoch 495/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.4119 - acc: 0.8408\n",
      "Epoch 496/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4074 - acc: 0.8408\n",
      "Epoch 497/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4060 - acc: 0.8421\n",
      "Epoch 498/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4081 - acc: 0.8448\n",
      "Epoch 499/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.4064 - acc: 0.8435\n",
      "Epoch 500/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4081 - acc: 0.8448\n"
     ]
    }
   ],
   "source": [
    "#Build Simple Model network\n",
    "simple_model = NN_model((S_AE_Train.shape[1], ), AE_layers, regularizers.l2(0.001), None)\n",
    "simple_model.compile(optimizer = \"Adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "simple_model.fit(x = S_AE_Train, y = Y_AE_Train, epochs = 500, verbose = 1)\n",
    "\n",
    "\n",
    "s_train_pred = simple_model.predict(x = S_AE_Train)\n",
    "s_cv_pred = simple_model.predict(x = S_AE_CV)\n",
    "\n",
    "s_train_hat = normalize_predictions(s_train_pred)\n",
    "s_cv_hat = normalize_predictions(s_cv_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "741/741 [==============================] - 1s 2ms/step - loss: 0.7137 - acc: 0.6181\n",
      "Epoch 2/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.6943 - acc: 0.6181\n",
      "Epoch 3/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.6834 - acc: 0.6181\n",
      "Epoch 4/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.6743 - acc: 0.6181\n",
      "Epoch 5/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.6632 - acc: 0.6181\n",
      "Epoch 6/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.6516 - acc: 0.6181\n",
      "Epoch 7/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.6398 - acc: 0.6181\n",
      "Epoch 8/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.6271 - acc: 0.6181\n",
      "Epoch 9/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.6173 - acc: 0.6181\n",
      "Epoch 10/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.6091 - acc: 0.6181\n",
      "Epoch 11/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.6029 - acc: 0.6181\n",
      "Epoch 12/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.5987 - acc: 0.6181\n",
      "Epoch 13/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.5904 - acc: 0.6181\n",
      "Epoch 14/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.5857 - acc: 0.6181\n",
      "Epoch 15/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.5817 - acc: 0.6181\n",
      "Epoch 16/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.5778 - acc: 0.6181\n",
      "Epoch 17/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.5738 - acc: 0.6181\n",
      "Epoch 18/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.5725 - acc: 0.6302\n",
      "Epoch 19/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.5683 - acc: 0.7881\n",
      "Epoch 20/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.5657 - acc: 0.7949\n",
      "Epoch 21/500\n",
      "741/741 [==============================] - 0s 301us/step - loss: 0.5643 - acc: 0.7895\n",
      "Epoch 22/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.5631 - acc: 0.7814\n",
      "Epoch 23/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.5595 - acc: 0.8016\n",
      "Epoch 24/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.5575 - acc: 0.8097\n",
      "Epoch 25/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.5552 - acc: 0.8097\n",
      "Epoch 26/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.5541 - acc: 0.8084\n",
      "Epoch 27/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.5527 - acc: 0.8097\n",
      "Epoch 28/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.5521 - acc: 0.8030\n",
      "Epoch 29/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.5501 - acc: 0.8124\n",
      "Epoch 30/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.5477 - acc: 0.8097\n",
      "Epoch 31/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.5448 - acc: 0.8016\n",
      "Epoch 32/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.5430 - acc: 0.8111\n",
      "Epoch 33/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.5409 - acc: 0.8138\n",
      "Epoch 34/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.5401 - acc: 0.8016\n",
      "Epoch 35/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.5364 - acc: 0.8070\n",
      "Epoch 36/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.5367 - acc: 0.8043\n",
      "Epoch 37/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.5353 - acc: 0.8138\n",
      "Epoch 38/500\n",
      "741/741 [==============================] - 0s 301us/step - loss: 0.5328 - acc: 0.8151\n",
      "Epoch 39/500\n",
      "741/741 [==============================] - 0s 305us/step - loss: 0.5294 - acc: 0.8205\n",
      "Epoch 40/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.5320 - acc: 0.8097\n",
      "Epoch 41/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.5292 - acc: 0.8070\n",
      "Epoch 42/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.5253 - acc: 0.8192\n",
      "Epoch 43/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.5249 - acc: 0.8205\n",
      "Epoch 44/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.5222 - acc: 0.8205\n",
      "Epoch 45/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.5207 - acc: 0.8219\n",
      "Epoch 46/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.5220 - acc: 0.8205\n",
      "Epoch 47/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.5167 - acc: 0.8273\n",
      "Epoch 48/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.5154 - acc: 0.8286\n",
      "Epoch 49/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.5152 - acc: 0.8300\n",
      "Epoch 50/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.5129 - acc: 0.8300\n",
      "Epoch 51/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.5127 - acc: 0.8327\n",
      "Epoch 52/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.5118 - acc: 0.8246\n",
      "Epoch 53/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.5107 - acc: 0.8340\n",
      "Epoch 54/500\n",
      "741/741 [==============================] - 0s 301us/step - loss: 0.5141 - acc: 0.8286\n",
      "Epoch 55/500\n",
      "741/741 [==============================] - 0s 301us/step - loss: 0.5070 - acc: 0.8354\n",
      "Epoch 56/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.5061 - acc: 0.8313\n",
      "Epoch 57/500\n",
      "741/741 [==============================] - 0s 335us/step - loss: 0.5038 - acc: 0.8340\n",
      "Epoch 58/500\n",
      "741/741 [==============================] - 0s 339us/step - loss: 0.5014 - acc: 0.8340\n",
      "Epoch 59/500\n",
      "741/741 [==============================] - 0s 328us/step - loss: 0.5014 - acc: 0.8340\n",
      "Epoch 60/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.5037 - acc: 0.8354\n",
      "Epoch 61/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.5026 - acc: 0.8300\n",
      "Epoch 62/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.4974 - acc: 0.8367\n",
      "Epoch 63/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.4969 - acc: 0.8327\n",
      "Epoch 64/500\n",
      "741/741 [==============================] - 0s 448us/step - loss: 0.5014 - acc: 0.8313 0s - loss: 0.4975 - acc: 0\n",
      "Epoch 65/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4992 - acc: 0.8111\n",
      "Epoch 66/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4943 - acc: 0.8367\n",
      "Epoch 67/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4954 - acc: 0.8286\n",
      "Epoch 68/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4950 - acc: 0.8340\n",
      "Epoch 69/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.4924 - acc: 0.8300\n",
      "Epoch 70/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.4935 - acc: 0.8259\n",
      "Epoch 71/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.4899 - acc: 0.8340\n",
      "Epoch 72/500\n",
      "741/741 [==============================] - 0s 305us/step - loss: 0.4892 - acc: 0.8313\n",
      "Epoch 73/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4862 - acc: 0.8354\n",
      "Epoch 74/500\n",
      "741/741 [==============================] - 0s 333us/step - loss: 0.4865 - acc: 0.8327\n",
      "Epoch 75/500\n",
      "741/741 [==============================] - 0s 375us/step - loss: 0.4842 - acc: 0.8340\n",
      "Epoch 76/500\n",
      "741/741 [==============================] - 0s 337us/step - loss: 0.4851 - acc: 0.8354\n",
      "Epoch 77/500\n",
      "741/741 [==============================] - 0s 333us/step - loss: 0.4829 - acc: 0.8340\n",
      "Epoch 78/500\n",
      "741/741 [==============================] - 0s 337us/step - loss: 0.4824 - acc: 0.8327\n",
      "Epoch 79/500\n",
      "741/741 [==============================] - 0s 344us/step - loss: 0.4835 - acc: 0.8381\n",
      "Epoch 80/500\n",
      "741/741 [==============================] - 0s 343us/step - loss: 0.4811 - acc: 0.8394\n",
      "Epoch 81/500\n",
      "741/741 [==============================] - 0s 344us/step - loss: 0.4823 - acc: 0.8367\n",
      "Epoch 82/500\n",
      "741/741 [==============================] - 0s 339us/step - loss: 0.4818 - acc: 0.8300\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 336us/step - loss: 0.4796 - acc: 0.8273\n",
      "Epoch 84/500\n",
      "741/741 [==============================] - 0s 328us/step - loss: 0.4782 - acc: 0.8340\n",
      "Epoch 85/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4771 - acc: 0.8367\n",
      "Epoch 86/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4777 - acc: 0.8354\n",
      "Epoch 87/500\n",
      "741/741 [==============================] - 0s 336us/step - loss: 0.4748 - acc: 0.8367\n",
      "Epoch 88/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.4725 - acc: 0.8394\n",
      "Epoch 89/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4710 - acc: 0.8408\n",
      "Epoch 90/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.4730 - acc: 0.8367\n",
      "Epoch 91/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4724 - acc: 0.8367\n",
      "Epoch 92/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.4755 - acc: 0.8354\n",
      "Epoch 93/500\n",
      "741/741 [==============================] - 0s 328us/step - loss: 0.4673 - acc: 0.8381\n",
      "Epoch 94/500\n",
      "741/741 [==============================] - 0s 327us/step - loss: 0.4693 - acc: 0.8354\n",
      "Epoch 95/500\n",
      "741/741 [==============================] - 0s 328us/step - loss: 0.4674 - acc: 0.8327\n",
      "Epoch 96/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4706 - acc: 0.8300\n",
      "Epoch 97/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.4724 - acc: 0.8286\n",
      "Epoch 98/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.4656 - acc: 0.8340\n",
      "Epoch 99/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4646 - acc: 0.8381\n",
      "Epoch 100/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4664 - acc: 0.8367\n",
      "Epoch 101/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.4684 - acc: 0.8327\n",
      "Epoch 102/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.4659 - acc: 0.8367\n",
      "Epoch 103/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4618 - acc: 0.8354\n",
      "Epoch 104/500\n",
      "741/741 [==============================] - 0s 327us/step - loss: 0.4613 - acc: 0.8408\n",
      "Epoch 105/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.4662 - acc: 0.8367\n",
      "Epoch 106/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4615 - acc: 0.8408\n",
      "Epoch 107/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4592 - acc: 0.8421\n",
      "Epoch 108/500\n",
      "741/741 [==============================] - 0s 332us/step - loss: 0.4579 - acc: 0.8381\n",
      "Epoch 109/500\n",
      "741/741 [==============================] - 0s 332us/step - loss: 0.4561 - acc: 0.8408\n",
      "Epoch 110/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4638 - acc: 0.8381\n",
      "Epoch 111/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4553 - acc: 0.8394\n",
      "Epoch 112/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.4559 - acc: 0.8367\n",
      "Epoch 113/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.4583 - acc: 0.8381\n",
      "Epoch 114/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.4592 - acc: 0.8448 0s - loss: 0.4549 - acc: 0.84\n",
      "Epoch 115/500\n",
      "741/741 [==============================] - 0s 335us/step - loss: 0.4554 - acc: 0.8354\n",
      "Epoch 116/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4565 - acc: 0.8435\n",
      "Epoch 117/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4524 - acc: 0.8408\n",
      "Epoch 118/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4526 - acc: 0.8408\n",
      "Epoch 119/500\n",
      "741/741 [==============================] - 0s 332us/step - loss: 0.4530 - acc: 0.8394\n",
      "Epoch 120/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4506 - acc: 0.8462\n",
      "Epoch 121/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.4560 - acc: 0.8381\n",
      "Epoch 122/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.4531 - acc: 0.8435\n",
      "Epoch 123/500\n",
      "741/741 [==============================] - 0s 345us/step - loss: 0.4510 - acc: 0.8448\n",
      "Epoch 124/500\n",
      "741/741 [==============================] - 0s 347us/step - loss: 0.4509 - acc: 0.8421\n",
      "Epoch 125/500\n",
      "741/741 [==============================] - 0s 358us/step - loss: 0.4489 - acc: 0.8489\n",
      "Epoch 126/500\n",
      "741/741 [==============================] - 0s 339us/step - loss: 0.4614 - acc: 0.8421\n",
      "Epoch 127/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4604 - acc: 0.8232\n",
      "Epoch 128/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4526 - acc: 0.8394\n",
      "Epoch 129/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4506 - acc: 0.8502\n",
      "Epoch 130/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.4471 - acc: 0.8435\n",
      "Epoch 131/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.4520 - acc: 0.8381\n",
      "Epoch 132/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4531 - acc: 0.8421\n",
      "Epoch 133/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4535 - acc: 0.8367\n",
      "Epoch 134/500\n",
      "741/741 [==============================] - 0s 331us/step - loss: 0.4496 - acc: 0.8435\n",
      "Epoch 135/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.4462 - acc: 0.8435\n",
      "Epoch 136/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4478 - acc: 0.8327\n",
      "Epoch 137/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4496 - acc: 0.8408\n",
      "Epoch 138/500\n",
      "741/741 [==============================] - 0s 331us/step - loss: 0.4471 - acc: 0.8421\n",
      "Epoch 139/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4462 - acc: 0.8448\n",
      "Epoch 140/500\n",
      "741/741 [==============================] - 0s 329us/step - loss: 0.4433 - acc: 0.8448\n",
      "Epoch 141/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4443 - acc: 0.8367\n",
      "Epoch 142/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4427 - acc: 0.8489\n",
      "Epoch 143/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.4447 - acc: 0.8489\n",
      "Epoch 144/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4547 - acc: 0.8259\n",
      "Epoch 145/500\n",
      "741/741 [==============================] - 0s 331us/step - loss: 0.4429 - acc: 0.8408\n",
      "Epoch 146/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4460 - acc: 0.8462\n",
      "Epoch 147/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4442 - acc: 0.8462\n",
      "Epoch 148/500\n",
      "741/741 [==============================] - 0s 331us/step - loss: 0.4401 - acc: 0.8448\n",
      "Epoch 149/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.4411 - acc: 0.8435\n",
      "Epoch 150/500\n",
      "741/741 [==============================] - 0s 333us/step - loss: 0.4381 - acc: 0.8462\n",
      "Epoch 151/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4400 - acc: 0.8394\n",
      "Epoch 152/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4526 - acc: 0.8340\n",
      "Epoch 153/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4406 - acc: 0.8435\n",
      "Epoch 154/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.4377 - acc: 0.8462\n",
      "Epoch 155/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4423 - acc: 0.8435\n",
      "Epoch 156/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4397 - acc: 0.8475\n",
      "Epoch 157/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.4445 - acc: 0.8475\n",
      "Epoch 158/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4438 - acc: 0.8462\n",
      "Epoch 159/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4358 - acc: 0.8394\n",
      "Epoch 160/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4408 - acc: 0.8435\n",
      "Epoch 161/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4366 - acc: 0.8516\n",
      "Epoch 162/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4363 - acc: 0.8489\n",
      "Epoch 163/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4368 - acc: 0.8475\n",
      "Epoch 164/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 308us/step - loss: 0.4374 - acc: 0.8462\n",
      "Epoch 165/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4325 - acc: 0.8489\n",
      "Epoch 166/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4348 - acc: 0.8462\n",
      "Epoch 167/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4351 - acc: 0.8462\n",
      "Epoch 168/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.4317 - acc: 0.8475\n",
      "Epoch 169/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4340 - acc: 0.8489\n",
      "Epoch 170/500\n",
      "741/741 [==============================] - 0s 331us/step - loss: 0.4336 - acc: 0.8435\n",
      "Epoch 171/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.4326 - acc: 0.8489\n",
      "Epoch 172/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4335 - acc: 0.8529\n",
      "Epoch 173/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4304 - acc: 0.8475\n",
      "Epoch 174/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.4328 - acc: 0.8462\n",
      "Epoch 175/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4313 - acc: 0.8381\n",
      "Epoch 176/500\n",
      "741/741 [==============================] - 0s 328us/step - loss: 0.4320 - acc: 0.8543\n",
      "Epoch 177/500\n",
      "741/741 [==============================] - 0s 328us/step - loss: 0.4436 - acc: 0.8340\n",
      "Epoch 178/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4302 - acc: 0.8556\n",
      "Epoch 179/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.4304 - acc: 0.8489\n",
      "Epoch 180/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4313 - acc: 0.8475\n",
      "Epoch 181/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4266 - acc: 0.8516\n",
      "Epoch 182/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4300 - acc: 0.8516\n",
      "Epoch 183/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4301 - acc: 0.8475\n",
      "Epoch 184/500\n",
      "741/741 [==============================] - 0s 283us/step - loss: 0.4286 - acc: 0.8529\n",
      "Epoch 185/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4259 - acc: 0.8475\n",
      "Epoch 186/500\n",
      "741/741 [==============================] - 0s 356us/step - loss: 0.4272 - acc: 0.8435\n",
      "Epoch 187/500\n",
      "741/741 [==============================] - 0s 335us/step - loss: 0.4257 - acc: 0.8556\n",
      "Epoch 188/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4290 - acc: 0.8516\n",
      "Epoch 189/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4265 - acc: 0.8435\n",
      "Epoch 190/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.4323 - acc: 0.8408\n",
      "Epoch 191/500\n",
      "741/741 [==============================] - 0s 350us/step - loss: 0.4295 - acc: 0.8448\n",
      "Epoch 192/500\n",
      "741/741 [==============================] - 0s 352us/step - loss: 0.4245 - acc: 0.8543\n",
      "Epoch 193/500\n",
      "741/741 [==============================] - 0s 368us/step - loss: 0.4255 - acc: 0.8462\n",
      "Epoch 194/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4218 - acc: 0.8502\n",
      "Epoch 195/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4285 - acc: 0.8489\n",
      "Epoch 196/500\n",
      "741/741 [==============================] - 0s 327us/step - loss: 0.4275 - acc: 0.8448\n",
      "Epoch 197/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4267 - acc: 0.8462\n",
      "Epoch 198/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4227 - acc: 0.8502\n",
      "Epoch 199/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4209 - acc: 0.8543\n",
      "Epoch 200/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4273 - acc: 0.8489\n",
      "Epoch 201/500\n",
      "741/741 [==============================] - 0s 329us/step - loss: 0.4227 - acc: 0.8462\n",
      "Epoch 202/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4325 - acc: 0.8448\n",
      "Epoch 203/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4266 - acc: 0.8448\n",
      "Epoch 204/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.4241 - acc: 0.8489\n",
      "Epoch 205/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4197 - acc: 0.8489\n",
      "Epoch 206/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4224 - acc: 0.8462\n",
      "Epoch 207/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.4192 - acc: 0.8516\n",
      "Epoch 208/500\n",
      "741/741 [==============================] - 0s 333us/step - loss: 0.4195 - acc: 0.8448\n",
      "Epoch 209/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.4204 - acc: 0.8529\n",
      "Epoch 210/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.4183 - acc: 0.8529\n",
      "Epoch 211/500\n",
      "741/741 [==============================] - 0s 331us/step - loss: 0.4208 - acc: 0.8516\n",
      "Epoch 212/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4248 - acc: 0.8462\n",
      "Epoch 213/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4308 - acc: 0.8502\n",
      "Epoch 214/500\n",
      "741/741 [==============================] - 0s 332us/step - loss: 0.4225 - acc: 0.8502\n",
      "Epoch 215/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4255 - acc: 0.8435\n",
      "Epoch 216/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4211 - acc: 0.8516 0s - loss: 0.4158 - acc: 0.85\n",
      "Epoch 217/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4218 - acc: 0.8475\n",
      "Epoch 218/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4200 - acc: 0.8475\n",
      "Epoch 219/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4202 - acc: 0.8475\n",
      "Epoch 220/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4221 - acc: 0.8448\n",
      "Epoch 221/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.4165 - acc: 0.8516\n",
      "Epoch 222/500\n",
      "741/741 [==============================] - 0s 327us/step - loss: 0.4151 - acc: 0.8421\n",
      "Epoch 223/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4182 - acc: 0.8502\n",
      "Epoch 224/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4191 - acc: 0.8448\n",
      "Epoch 225/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4199 - acc: 0.8489\n",
      "Epoch 226/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.4174 - acc: 0.8529\n",
      "Epoch 227/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.4194 - acc: 0.8448\n",
      "Epoch 228/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4226 - acc: 0.8475\n",
      "Epoch 229/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4182 - acc: 0.8462\n",
      "Epoch 230/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4188 - acc: 0.8489\n",
      "Epoch 231/500\n",
      "741/741 [==============================] - 0s 328us/step - loss: 0.4187 - acc: 0.8475\n",
      "Epoch 232/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4132 - acc: 0.8489\n",
      "Epoch 233/500\n",
      "741/741 [==============================] - 0s 331us/step - loss: 0.4169 - acc: 0.8502\n",
      "Epoch 234/500\n",
      "741/741 [==============================] - 0s 336us/step - loss: 0.4205 - acc: 0.8435\n",
      "Epoch 235/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4183 - acc: 0.8381\n",
      "Epoch 236/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4153 - acc: 0.8502\n",
      "Epoch 237/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4152 - acc: 0.8543\n",
      "Epoch 238/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.4189 - acc: 0.8462\n",
      "Epoch 239/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.4180 - acc: 0.8489\n",
      "Epoch 240/500\n",
      "741/741 [==============================] - 0s 335us/step - loss: 0.4189 - acc: 0.8556\n",
      "Epoch 241/500\n",
      "741/741 [==============================] - 0s 328us/step - loss: 0.4185 - acc: 0.8475\n",
      "Epoch 242/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4165 - acc: 0.8489\n",
      "Epoch 243/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4169 - acc: 0.8502\n",
      "Epoch 244/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4169 - acc: 0.8462\n",
      "Epoch 245/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 312us/step - loss: 0.4196 - acc: 0.8489\n",
      "Epoch 246/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.4113 - acc: 0.8516\n",
      "Epoch 247/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4103 - acc: 0.8529\n",
      "Epoch 248/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4141 - acc: 0.8475\n",
      "Epoch 249/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4157 - acc: 0.8516\n",
      "Epoch 250/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.4122 - acc: 0.8489 0s - loss: 0.4001 - acc: 0.85\n",
      "Epoch 251/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.4150 - acc: 0.8543\n",
      "Epoch 252/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.4129 - acc: 0.8489\n",
      "Epoch 253/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4189 - acc: 0.8502\n",
      "Epoch 254/500\n",
      "741/741 [==============================] - 0s 333us/step - loss: 0.4132 - acc: 0.8570\n",
      "Epoch 255/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4132 - acc: 0.8529\n",
      "Epoch 256/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4100 - acc: 0.8570\n",
      "Epoch 257/500\n",
      "741/741 [==============================] - 0s 337us/step - loss: 0.4347 - acc: 0.8381\n",
      "Epoch 258/500\n",
      "741/741 [==============================] - 0s 354us/step - loss: 0.4366 - acc: 0.8327\n",
      "Epoch 259/500\n",
      "741/741 [==============================] - 0s 352us/step - loss: 0.4350 - acc: 0.8354\n",
      "Epoch 260/500\n",
      "741/741 [==============================] - 0s 350us/step - loss: 0.4128 - acc: 0.8502\n",
      "Epoch 261/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.4127 - acc: 0.8475\n",
      "Epoch 262/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4129 - acc: 0.8462\n",
      "Epoch 263/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4121 - acc: 0.8529\n",
      "Epoch 264/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4109 - acc: 0.8570\n",
      "Epoch 265/500\n",
      "741/741 [==============================] - 0s 328us/step - loss: 0.4148 - acc: 0.8394\n",
      "Epoch 266/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4123 - acc: 0.8543\n",
      "Epoch 267/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.4148 - acc: 0.8502\n",
      "Epoch 268/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4147 - acc: 0.8475\n",
      "Epoch 269/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4139 - acc: 0.8435\n",
      "Epoch 270/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4190 - acc: 0.8435\n",
      "Epoch 271/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4142 - acc: 0.8543\n",
      "Epoch 272/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4130 - acc: 0.8435\n",
      "Epoch 273/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4138 - acc: 0.8489\n",
      "Epoch 274/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4119 - acc: 0.8462\n",
      "Epoch 275/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4081 - acc: 0.8543\n",
      "Epoch 276/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4116 - acc: 0.8367\n",
      "Epoch 277/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4079 - acc: 0.8462\n",
      "Epoch 278/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4104 - acc: 0.8475\n",
      "Epoch 279/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4176 - acc: 0.8394\n",
      "Epoch 280/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4079 - acc: 0.8556\n",
      "Epoch 281/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4054 - acc: 0.8502\n",
      "Epoch 282/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4113 - acc: 0.8489\n",
      "Epoch 283/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4099 - acc: 0.8502\n",
      "Epoch 284/500\n",
      "741/741 [==============================] - 0s 345us/step - loss: 0.4036 - acc: 0.8543\n",
      "Epoch 285/500\n",
      "741/741 [==============================] - 0s 328us/step - loss: 0.4112 - acc: 0.8489\n",
      "Epoch 286/500\n",
      "741/741 [==============================] - 0s 331us/step - loss: 0.4121 - acc: 0.8462\n",
      "Epoch 287/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4051 - acc: 0.8556\n",
      "Epoch 288/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4078 - acc: 0.8462\n",
      "Epoch 289/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.4059 - acc: 0.8516\n",
      "Epoch 290/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4074 - acc: 0.8475\n",
      "Epoch 291/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4082 - acc: 0.8421\n",
      "Epoch 292/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4122 - acc: 0.8529\n",
      "Epoch 293/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4053 - acc: 0.8529\n",
      "Epoch 294/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4055 - acc: 0.8502\n",
      "Epoch 295/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4035 - acc: 0.8516\n",
      "Epoch 296/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.4053 - acc: 0.8516\n",
      "Epoch 297/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.4023 - acc: 0.8489\n",
      "Epoch 298/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4041 - acc: 0.8489\n",
      "Epoch 299/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4037 - acc: 0.8462\n",
      "Epoch 300/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4050 - acc: 0.8516\n",
      "Epoch 301/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.4089 - acc: 0.8516\n",
      "Epoch 302/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4082 - acc: 0.8475\n",
      "Epoch 303/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4070 - acc: 0.8570\n",
      "Epoch 304/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4066 - acc: 0.8516\n",
      "Epoch 305/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4083 - acc: 0.8489\n",
      "Epoch 306/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.4109 - acc: 0.8516\n",
      "Epoch 307/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4007 - acc: 0.8583\n",
      "Epoch 308/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4082 - acc: 0.8448\n",
      "Epoch 309/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4075 - acc: 0.8543\n",
      "Epoch 310/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4021 - acc: 0.8516\n",
      "Epoch 311/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4134 - acc: 0.8394\n",
      "Epoch 312/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4079 - acc: 0.8475\n",
      "Epoch 313/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.4022 - acc: 0.8543\n",
      "Epoch 314/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.4050 - acc: 0.8529\n",
      "Epoch 315/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.3999 - acc: 0.8529\n",
      "Epoch 316/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.4060 - acc: 0.8502\n",
      "Epoch 317/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4083 - acc: 0.8462\n",
      "Epoch 318/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4049 - acc: 0.8475\n",
      "Epoch 319/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.4036 - acc: 0.8516\n",
      "Epoch 320/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.4071 - acc: 0.8462\n",
      "Epoch 321/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4025 - acc: 0.8475\n",
      "Epoch 322/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.4055 - acc: 0.8502\n",
      "Epoch 323/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4071 - acc: 0.8502\n",
      "Epoch 324/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4067 - acc: 0.8516\n",
      "Epoch 325/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4127 - acc: 0.8367\n",
      "Epoch 326/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 337us/step - loss: 0.4023 - acc: 0.8502\n",
      "Epoch 327/500\n",
      "741/741 [==============================] - 0s 355us/step - loss: 0.3988 - acc: 0.8516\n",
      "Epoch 328/500\n",
      "741/741 [==============================] - 0s 356us/step - loss: 0.3993 - acc: 0.8516\n",
      "Epoch 329/500\n",
      "741/741 [==============================] - 0s 331us/step - loss: 0.4176 - acc: 0.8340\n",
      "Epoch 330/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4033 - acc: 0.8516\n",
      "Epoch 331/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4106 - acc: 0.8408\n",
      "Epoch 332/500\n",
      "741/741 [==============================] - 0s 329us/step - loss: 0.4027 - acc: 0.8583\n",
      "Epoch 333/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.4003 - acc: 0.8556\n",
      "Epoch 334/500\n",
      "741/741 [==============================] - 0s 329us/step - loss: 0.3989 - acc: 0.8475\n",
      "Epoch 335/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4015 - acc: 0.8435\n",
      "Epoch 336/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4111 - acc: 0.8435\n",
      "Epoch 337/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.3970 - acc: 0.8610\n",
      "Epoch 338/500\n",
      "741/741 [==============================] - 0s 327us/step - loss: 0.3963 - acc: 0.8583\n",
      "Epoch 339/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.4061 - acc: 0.8435\n",
      "Epoch 340/500\n",
      "741/741 [==============================] - 0s 327us/step - loss: 0.4022 - acc: 0.8502\n",
      "Epoch 341/500\n",
      "741/741 [==============================] - 0s 339us/step - loss: 0.4007 - acc: 0.8556\n",
      "Epoch 342/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4084 - acc: 0.8381\n",
      "Epoch 343/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4028 - acc: 0.8462\n",
      "Epoch 344/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4096 - acc: 0.8381\n",
      "Epoch 345/500\n",
      "741/741 [==============================] - 0s 328us/step - loss: 0.4018 - acc: 0.8570\n",
      "Epoch 346/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.3975 - acc: 0.8529\n",
      "Epoch 347/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.3994 - acc: 0.8570\n",
      "Epoch 348/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.4023 - acc: 0.8556\n",
      "Epoch 349/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3961 - acc: 0.8570\n",
      "Epoch 350/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3981 - acc: 0.8529\n",
      "Epoch 351/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4000 - acc: 0.8475\n",
      "Epoch 352/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.3948 - acc: 0.8543\n",
      "Epoch 353/500\n",
      "741/741 [==============================] - 0s 345us/step - loss: 0.3983 - acc: 0.8543\n",
      "Epoch 354/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.3981 - acc: 0.8475\n",
      "Epoch 355/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4065 - acc: 0.8421\n",
      "Epoch 356/500\n",
      "741/741 [==============================] - 0s 331us/step - loss: 0.3954 - acc: 0.8529\n",
      "Epoch 357/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4067 - acc: 0.8435\n",
      "Epoch 358/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.3985 - acc: 0.8529\n",
      "Epoch 359/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.3984 - acc: 0.8489\n",
      "Epoch 360/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.3912 - acc: 0.8610\n",
      "Epoch 361/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.3995 - acc: 0.8556\n",
      "Epoch 362/500\n",
      "741/741 [==============================] - 0s 328us/step - loss: 0.3982 - acc: 0.8475\n",
      "Epoch 363/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4015 - acc: 0.8475\n",
      "Epoch 364/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.3955 - acc: 0.8570\n",
      "Epoch 365/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.3967 - acc: 0.8543\n",
      "Epoch 366/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.3924 - acc: 0.8583\n",
      "Epoch 367/500\n",
      "741/741 [==============================] - 0s 327us/step - loss: 0.3907 - acc: 0.8583\n",
      "Epoch 368/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.3950 - acc: 0.8610\n",
      "Epoch 369/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.3939 - acc: 0.8529\n",
      "Epoch 370/500\n",
      "741/741 [==============================] - 0s 329us/step - loss: 0.3976 - acc: 0.8489\n",
      "Epoch 371/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.3974 - acc: 0.8502\n",
      "Epoch 372/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.3954 - acc: 0.8529\n",
      "Epoch 373/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.3949 - acc: 0.8529\n",
      "Epoch 374/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.3991 - acc: 0.8556\n",
      "Epoch 375/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.3934 - acc: 0.8543\n",
      "Epoch 376/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.3956 - acc: 0.8475\n",
      "Epoch 377/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.3953 - acc: 0.8435\n",
      "Epoch 378/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.4004 - acc: 0.8556\n",
      "Epoch 379/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.3968 - acc: 0.8489\n",
      "Epoch 380/500\n",
      "741/741 [==============================] - 0s 327us/step - loss: 0.3953 - acc: 0.8583\n",
      "Epoch 381/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.4086 - acc: 0.8394\n",
      "Epoch 382/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4014 - acc: 0.8435\n",
      "Epoch 383/500\n",
      "741/741 [==============================] - 0s 328us/step - loss: 0.3931 - acc: 0.8570\n",
      "Epoch 384/500\n",
      "741/741 [==============================] - 0s 328us/step - loss: 0.4021 - acc: 0.8462\n",
      "Epoch 385/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4082 - acc: 0.8462\n",
      "Epoch 386/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.3996 - acc: 0.8475\n",
      "Epoch 387/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.4030 - acc: 0.8475\n",
      "Epoch 388/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.3961 - acc: 0.8610\n",
      "Epoch 389/500\n",
      "741/741 [==============================] - 0s 331us/step - loss: 0.3922 - acc: 0.8583\n",
      "Epoch 390/500\n",
      "741/741 [==============================] - 0s 329us/step - loss: 0.3926 - acc: 0.8583\n",
      "Epoch 391/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.3925 - acc: 0.8543\n",
      "Epoch 392/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.3919 - acc: 0.8502\n",
      "Epoch 393/500\n",
      "741/741 [==============================] - 0s 347us/step - loss: 0.3933 - acc: 0.8596\n",
      "Epoch 394/500\n",
      "741/741 [==============================] - 0s 350us/step - loss: 0.3930 - acc: 0.8543\n",
      "Epoch 395/500\n",
      "741/741 [==============================] - 0s 352us/step - loss: 0.3939 - acc: 0.8556\n",
      "Epoch 396/500\n",
      "741/741 [==============================] - 0s 336us/step - loss: 0.3946 - acc: 0.8556\n",
      "Epoch 397/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.3934 - acc: 0.8489\n",
      "Epoch 398/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.3895 - acc: 0.8596\n",
      "Epoch 399/500\n",
      "741/741 [==============================] - 0s 327us/step - loss: 0.3938 - acc: 0.8556\n",
      "Epoch 400/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.3938 - acc: 0.8623 0s - loss: 0.3968 - acc: 0.865\n",
      "Epoch 401/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.3880 - acc: 0.8610\n",
      "Epoch 402/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.3940 - acc: 0.8529\n",
      "Epoch 403/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.3927 - acc: 0.8543\n",
      "Epoch 404/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.3914 - acc: 0.8489\n",
      "Epoch 405/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.3959 - acc: 0.8502\n",
      "Epoch 406/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.3969 - acc: 0.8529\n",
      "Epoch 407/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 316us/step - loss: 0.3905 - acc: 0.8583\n",
      "Epoch 408/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.3912 - acc: 0.8516\n",
      "Epoch 409/500\n",
      "741/741 [==============================] - 0s 328us/step - loss: 0.3871 - acc: 0.8502\n",
      "Epoch 410/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.3961 - acc: 0.8462\n",
      "Epoch 411/500\n",
      "741/741 [==============================] - 0s 333us/step - loss: 0.3924 - acc: 0.8489\n",
      "Epoch 412/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.3956 - acc: 0.8448\n",
      "Epoch 413/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.4003 - acc: 0.8448\n",
      "Epoch 414/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.3885 - acc: 0.8583\n",
      "Epoch 415/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.3905 - acc: 0.8556\n",
      "Epoch 416/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.3932 - acc: 0.8556\n",
      "Epoch 417/500\n",
      "741/741 [==============================] - 0s 333us/step - loss: 0.3911 - acc: 0.8583\n",
      "Epoch 418/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.3892 - acc: 0.8489\n",
      "Epoch 419/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.3880 - acc: 0.8623\n",
      "Epoch 420/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.3867 - acc: 0.8596\n",
      "Epoch 421/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.3954 - acc: 0.8516\n",
      "Epoch 422/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.3938 - acc: 0.8570\n",
      "Epoch 423/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.3955 - acc: 0.8502\n",
      "Epoch 424/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.3925 - acc: 0.8583\n",
      "Epoch 425/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.3919 - acc: 0.8583\n",
      "Epoch 426/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.3900 - acc: 0.8570\n",
      "Epoch 427/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.3920 - acc: 0.8529\n",
      "Epoch 428/500\n",
      "741/741 [==============================] - 0s 327us/step - loss: 0.3880 - acc: 0.8489\n",
      "Epoch 429/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.3865 - acc: 0.8610\n",
      "Epoch 430/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.3896 - acc: 0.8516\n",
      "Epoch 431/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.3894 - acc: 0.8516\n",
      "Epoch 432/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.3952 - acc: 0.8489\n",
      "Epoch 433/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.3904 - acc: 0.8570\n",
      "Epoch 434/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.3912 - acc: 0.8570\n",
      "Epoch 435/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.3901 - acc: 0.8516\n",
      "Epoch 436/500\n",
      "741/741 [==============================] - 0s 328us/step - loss: 0.3973 - acc: 0.8435\n",
      "Epoch 437/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.3943 - acc: 0.8489\n",
      "Epoch 438/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.3900 - acc: 0.8421\n",
      "Epoch 439/500\n",
      "741/741 [==============================] - 0s 331us/step - loss: 0.3916 - acc: 0.8570\n",
      "Epoch 440/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.3964 - acc: 0.8475\n",
      "Epoch 441/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.3939 - acc: 0.8502\n",
      "Epoch 442/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.3913 - acc: 0.8583\n",
      "Epoch 443/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.3941 - acc: 0.8596\n",
      "Epoch 444/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.3906 - acc: 0.8596\n",
      "Epoch 445/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.3900 - acc: 0.8543\n",
      "Epoch 446/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4031 - acc: 0.8435\n",
      "Epoch 447/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.3927 - acc: 0.8462\n",
      "Epoch 448/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.3889 - acc: 0.8556\n",
      "Epoch 449/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.3904 - acc: 0.8516\n",
      "Epoch 450/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.3915 - acc: 0.8570\n",
      "Epoch 451/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.3859 - acc: 0.8556\n",
      "Epoch 452/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.3870 - acc: 0.8543\n",
      "Epoch 453/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.3874 - acc: 0.8596\n",
      "Epoch 454/500\n",
      "741/741 [==============================] - 0s 335us/step - loss: 0.3886 - acc: 0.8570\n",
      "Epoch 455/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.3857 - acc: 0.8583\n",
      "Epoch 456/500\n",
      "741/741 [==============================] - 0s 332us/step - loss: 0.3867 - acc: 0.8529\n",
      "Epoch 457/500\n",
      "741/741 [==============================] - 0s 331us/step - loss: 0.3986 - acc: 0.8421\n",
      "Epoch 458/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.3930 - acc: 0.8529\n",
      "Epoch 459/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.3853 - acc: 0.8596\n",
      "Epoch 460/500\n",
      "741/741 [==============================] - 0s 336us/step - loss: 0.3866 - acc: 0.8543\n",
      "Epoch 461/500\n",
      "741/741 [==============================] - 0s 350us/step - loss: 0.3935 - acc: 0.8529\n",
      "Epoch 462/500\n",
      "741/741 [==============================] - 0s 343us/step - loss: 0.3910 - acc: 0.8516\n",
      "Epoch 463/500\n",
      "741/741 [==============================] - 0s 335us/step - loss: 0.3901 - acc: 0.8502\n",
      "Epoch 464/500\n",
      "741/741 [==============================] - 0s 305us/step - loss: 0.3963 - acc: 0.8475\n",
      "Epoch 465/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.3938 - acc: 0.8502\n",
      "Epoch 466/500\n",
      "741/741 [==============================] - 0s 327us/step - loss: 0.4184 - acc: 0.8367\n",
      "Epoch 467/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.3927 - acc: 0.8516\n",
      "Epoch 468/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.3878 - acc: 0.8489\n",
      "Epoch 469/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.3872 - acc: 0.8543\n",
      "Epoch 470/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.3929 - acc: 0.8543\n",
      "Epoch 471/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.3902 - acc: 0.8529\n",
      "Epoch 472/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.3915 - acc: 0.8556\n",
      "Epoch 473/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.3936 - acc: 0.8529\n",
      "Epoch 474/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.3952 - acc: 0.8529\n",
      "Epoch 475/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.3877 - acc: 0.8529\n",
      "Epoch 476/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.3858 - acc: 0.8516\n",
      "Epoch 477/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.3915 - acc: 0.8556\n",
      "Epoch 478/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.3882 - acc: 0.8570\n",
      "Epoch 479/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.3904 - acc: 0.8529\n",
      "Epoch 480/500\n",
      "741/741 [==============================] - 0s 319us/step - loss: 0.3907 - acc: 0.8489\n",
      "Epoch 481/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.3921 - acc: 0.8570\n",
      "Epoch 482/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.3894 - acc: 0.8623\n",
      "Epoch 483/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.3910 - acc: 0.8448\n",
      "Epoch 484/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.3986 - acc: 0.8462\n",
      "Epoch 485/500\n",
      "741/741 [==============================] - 0s 329us/step - loss: 0.3934 - acc: 0.8502\n",
      "Epoch 486/500\n",
      "741/741 [==============================] - 0s 331us/step - loss: 0.3875 - acc: 0.8583\n",
      "Epoch 487/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.3875 - acc: 0.8543\n",
      "Epoch 488/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.3900 - acc: 0.8489\n",
      "Epoch 489/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 313us/step - loss: 0.3940 - acc: 0.8475\n",
      "Epoch 490/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.3875 - acc: 0.8529\n",
      "Epoch 491/500\n",
      "741/741 [==============================] - 0s 321us/step - loss: 0.3876 - acc: 0.8583\n",
      "Epoch 492/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.3890 - acc: 0.8556\n",
      "Epoch 493/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.3888 - acc: 0.8570\n",
      "Epoch 494/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.3864 - acc: 0.8570\n",
      "Epoch 495/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.3951 - acc: 0.8489\n",
      "Epoch 496/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.4013 - acc: 0.8462\n",
      "Epoch 497/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.3918 - acc: 0.8543\n",
      "Epoch 498/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.3882 - acc: 0.8583\n",
      "Epoch 499/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.4003 - acc: 0.8421\n",
      "Epoch 500/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.3917 - acc: 0.8556\n"
     ]
    }
   ],
   "source": [
    "#Build Denoising Model network\n",
    "denoise_model = NN_model((D_AE_Train.shape[1], ), AE_layers, regularizers.l2(0.001), None)\n",
    "denoise_model.compile(optimizer = \"Adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "denoise_model.fit(x = D_AE_Train, y = Y_AE_Train, epochs = 500, verbose = 1)\n",
    "\n",
    "\n",
    "d_train_pred = denoise_model.predict(x = D_AE_Train)\n",
    "d_cv_pred = denoise_model.predict(x = D_AE_CV)\n",
    "\n",
    "d_train_hat = normalize_predictions(d_train_pred)\n",
    "d_cv_hat = normalize_predictions(d_cv_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "741/741 [==============================] - 2s 2ms/step - loss: 0.7115 - acc: 0.6181\n",
      "Epoch 2/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.6959 - acc: 0.6181\n",
      "Epoch 3/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.6866 - acc: 0.6181\n",
      "Epoch 4/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.6803 - acc: 0.6181\n",
      "Epoch 5/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.6756 - acc: 0.6181\n",
      "Epoch 6/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.6711 - acc: 0.6181\n",
      "Epoch 7/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.6666 - acc: 0.6181\n",
      "Epoch 8/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.6622 - acc: 0.6181\n",
      "Epoch 9/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.6578 - acc: 0.6181\n",
      "Epoch 10/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.6537 - acc: 0.6181\n",
      "Epoch 11/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.6488 - acc: 0.6181\n",
      "Epoch 12/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.6446 - acc: 0.6181\n",
      "Epoch 13/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.6396 - acc: 0.6181\n",
      "Epoch 14/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.6353 - acc: 0.6181\n",
      "Epoch 15/500\n",
      "741/741 [==============================] - 0s 328us/step - loss: 0.6306 - acc: 0.6181\n",
      "Epoch 16/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.6259 - acc: 0.6181\n",
      "Epoch 17/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.6211 - acc: 0.6181\n",
      "Epoch 18/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.6151 - acc: 0.6181\n",
      "Epoch 19/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.6094 - acc: 0.6181\n",
      "Epoch 20/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.6033 - acc: 0.6181\n",
      "Epoch 21/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.5983 - acc: 0.6181\n",
      "Epoch 22/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.5914 - acc: 0.6181\n",
      "Epoch 23/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.5857 - acc: 0.6181\n",
      "Epoch 24/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.5799 - acc: 0.6181\n",
      "Epoch 25/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.5750 - acc: 0.6181\n",
      "Epoch 26/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.5703 - acc: 0.6181\n",
      "Epoch 27/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.5655 - acc: 0.6181\n",
      "Epoch 28/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.5611 - acc: 0.6181\n",
      "Epoch 29/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.5577 - acc: 0.6181\n",
      "Epoch 30/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.5561 - acc: 0.6181\n",
      "Epoch 31/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.5525 - acc: 0.6181\n",
      "Epoch 32/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.5497 - acc: 0.6491\n",
      "Epoch 33/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.5474 - acc: 0.8286\n",
      "Epoch 34/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.5430 - acc: 0.8340\n",
      "Epoch 35/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.5394 - acc: 0.8367\n",
      "Epoch 36/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.5373 - acc: 0.8381\n",
      "Epoch 37/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.5343 - acc: 0.8394\n",
      "Epoch 38/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.5294 - acc: 0.8300\n",
      "Epoch 39/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.5261 - acc: 0.8340\n",
      "Epoch 40/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.5228 - acc: 0.8435\n",
      "Epoch 41/500\n",
      "741/741 [==============================] - 0s 301us/step - loss: 0.5197 - acc: 0.8354\n",
      "Epoch 42/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.5156 - acc: 0.8516\n",
      "Epoch 43/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.5132 - acc: 0.8556\n",
      "Epoch 44/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.5104 - acc: 0.8543\n",
      "Epoch 45/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.5078 - acc: 0.8543\n",
      "Epoch 46/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.5075 - acc: 0.8502\n",
      "Epoch 47/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.5033 - acc: 0.8529\n",
      "Epoch 48/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.5021 - acc: 0.8570\n",
      "Epoch 49/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.5045 - acc: 0.8529\n",
      "Epoch 50/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.4983 - acc: 0.8556\n",
      "Epoch 51/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4980 - acc: 0.8435\n",
      "Epoch 52/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4944 - acc: 0.8570\n",
      "Epoch 53/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.4903 - acc: 0.8583\n",
      "Epoch 54/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.4884 - acc: 0.8556\n",
      "Epoch 55/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4872 - acc: 0.8583\n",
      "Epoch 56/500\n",
      "741/741 [==============================] - 0s 301us/step - loss: 0.4852 - acc: 0.8543\n",
      "Epoch 57/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4839 - acc: 0.8570\n",
      "Epoch 58/500\n",
      "741/741 [==============================] - 0s 305us/step - loss: 0.4858 - acc: 0.8583\n",
      "Epoch 59/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4817 - acc: 0.8556\n",
      "Epoch 60/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.4817 - acc: 0.8502\n",
      "Epoch 61/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.4785 - acc: 0.8610\n",
      "Epoch 62/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.4774 - acc: 0.8610\n",
      "Epoch 63/500\n",
      "741/741 [==============================] - 0s 301us/step - loss: 0.4738 - acc: 0.8610\n",
      "Epoch 64/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.4732 - acc: 0.8610\n",
      "Epoch 65/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.4733 - acc: 0.8610\n",
      "Epoch 66/500\n",
      "741/741 [==============================] - 0s 305us/step - loss: 0.4706 - acc: 0.8623\n",
      "Epoch 67/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4699 - acc: 0.8556\n",
      "Epoch 68/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.4687 - acc: 0.8610\n",
      "Epoch 69/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.4673 - acc: 0.8637\n",
      "Epoch 70/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.4650 - acc: 0.8610\n",
      "Epoch 71/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.4632 - acc: 0.8623\n",
      "Epoch 72/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.4620 - acc: 0.8637\n",
      "Epoch 73/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.4643 - acc: 0.8610\n",
      "Epoch 74/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4630 - acc: 0.8596\n",
      "Epoch 75/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.4576 - acc: 0.8623\n",
      "Epoch 76/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.4620 - acc: 0.8489\n",
      "Epoch 77/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.4578 - acc: 0.8637\n",
      "Epoch 78/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.4574 - acc: 0.8543\n",
      "Epoch 79/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.4554 - acc: 0.8596\n",
      "Epoch 80/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.4574 - acc: 0.8489\n",
      "Epoch 81/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.4550 - acc: 0.8650\n",
      "Epoch 82/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4557 - acc: 0.8637\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 297us/step - loss: 0.4522 - acc: 0.8623\n",
      "Epoch 84/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4501 - acc: 0.8623\n",
      "Epoch 85/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4514 - acc: 0.8637\n",
      "Epoch 86/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.4491 - acc: 0.8637\n",
      "Epoch 87/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4519 - acc: 0.8596\n",
      "Epoch 88/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.4458 - acc: 0.8650\n",
      "Epoch 89/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4461 - acc: 0.8583\n",
      "Epoch 90/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4476 - acc: 0.8637\n",
      "Epoch 91/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4462 - acc: 0.8583\n",
      "Epoch 92/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4449 - acc: 0.8623\n",
      "Epoch 93/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4437 - acc: 0.8637\n",
      "Epoch 94/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4459 - acc: 0.8570\n",
      "Epoch 95/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4420 - acc: 0.8637\n",
      "Epoch 96/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4415 - acc: 0.8623\n",
      "Epoch 97/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4415 - acc: 0.8570\n",
      "Epoch 98/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4433 - acc: 0.8637\n",
      "Epoch 99/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4418 - acc: 0.8664\n",
      "Epoch 100/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4370 - acc: 0.8637\n",
      "Epoch 101/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4366 - acc: 0.8570\n",
      "Epoch 102/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4358 - acc: 0.8650\n",
      "Epoch 103/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4353 - acc: 0.8637\n",
      "Epoch 104/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4358 - acc: 0.8516\n",
      "Epoch 105/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4343 - acc: 0.8664\n",
      "Epoch 106/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4330 - acc: 0.8570\n",
      "Epoch 107/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4342 - acc: 0.8570\n",
      "Epoch 108/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.4341 - acc: 0.8516\n",
      "Epoch 109/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.4310 - acc: 0.8637\n",
      "Epoch 110/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4324 - acc: 0.8623\n",
      "Epoch 111/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4356 - acc: 0.8596\n",
      "Epoch 112/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4334 - acc: 0.8570\n",
      "Epoch 113/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4306 - acc: 0.8637\n",
      "Epoch 114/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4345 - acc: 0.8556\n",
      "Epoch 115/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4294 - acc: 0.8596\n",
      "Epoch 116/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4300 - acc: 0.8583\n",
      "Epoch 117/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4282 - acc: 0.8583\n",
      "Epoch 118/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4260 - acc: 0.8623\n",
      "Epoch 119/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4250 - acc: 0.8650\n",
      "Epoch 120/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4252 - acc: 0.8637\n",
      "Epoch 121/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4248 - acc: 0.8623\n",
      "Epoch 122/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4253 - acc: 0.8637\n",
      "Epoch 123/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4264 - acc: 0.8610\n",
      "Epoch 124/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4244 - acc: 0.8650\n",
      "Epoch 125/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4231 - acc: 0.8623\n",
      "Epoch 126/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4231 - acc: 0.8623\n",
      "Epoch 127/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4227 - acc: 0.8637\n",
      "Epoch 128/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4246 - acc: 0.8664\n",
      "Epoch 129/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4244 - acc: 0.8570\n",
      "Epoch 130/500\n",
      "741/741 [==============================] - 0s 285us/step - loss: 0.4238 - acc: 0.8596\n",
      "Epoch 131/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4235 - acc: 0.8529 0s - loss: 0.4102 - acc: 0.863\n",
      "Epoch 132/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4238 - acc: 0.8650\n",
      "Epoch 133/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4208 - acc: 0.8610\n",
      "Epoch 134/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4211 - acc: 0.8570\n",
      "Epoch 135/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4178 - acc: 0.8623\n",
      "Epoch 136/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4195 - acc: 0.8583\n",
      "Epoch 137/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4176 - acc: 0.8623\n",
      "Epoch 138/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4185 - acc: 0.8596\n",
      "Epoch 139/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4193 - acc: 0.8637\n",
      "Epoch 140/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4217 - acc: 0.8583\n",
      "Epoch 141/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4182 - acc: 0.8596\n",
      "Epoch 142/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4157 - acc: 0.8664\n",
      "Epoch 143/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4159 - acc: 0.8664\n",
      "Epoch 144/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4180 - acc: 0.8570\n",
      "Epoch 145/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4136 - acc: 0.8610\n",
      "Epoch 146/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4143 - acc: 0.8596\n",
      "Epoch 147/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4131 - acc: 0.8650\n",
      "Epoch 148/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4145 - acc: 0.8570\n",
      "Epoch 149/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4134 - acc: 0.8623\n",
      "Epoch 150/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4125 - acc: 0.8623\n",
      "Epoch 151/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4119 - acc: 0.8650\n",
      "Epoch 152/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4125 - acc: 0.8637\n",
      "Epoch 153/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4125 - acc: 0.8677\n",
      "Epoch 154/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4124 - acc: 0.8650\n",
      "Epoch 155/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4103 - acc: 0.8664\n",
      "Epoch 156/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4122 - acc: 0.8623\n",
      "Epoch 157/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4114 - acc: 0.8677\n",
      "Epoch 158/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.4095 - acc: 0.8637\n",
      "Epoch 159/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.4095 - acc: 0.8637\n",
      "Epoch 160/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.4097 - acc: 0.8650\n",
      "Epoch 161/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.4089 - acc: 0.8677\n",
      "Epoch 162/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.4068 - acc: 0.8637\n",
      "Epoch 163/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4093 - acc: 0.8610\n",
      "Epoch 164/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 294us/step - loss: 0.4089 - acc: 0.8583\n",
      "Epoch 165/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4098 - acc: 0.8610\n",
      "Epoch 166/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4073 - acc: 0.8664\n",
      "Epoch 167/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4082 - acc: 0.8610\n",
      "Epoch 168/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4093 - acc: 0.8543\n",
      "Epoch 169/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4098 - acc: 0.8596\n",
      "Epoch 170/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4069 - acc: 0.8650\n",
      "Epoch 171/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4072 - acc: 0.8664\n",
      "Epoch 172/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4067 - acc: 0.8637\n",
      "Epoch 173/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4049 - acc: 0.8637\n",
      "Epoch 174/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4087 - acc: 0.8650\n",
      "Epoch 175/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4077 - acc: 0.8583\n",
      "Epoch 176/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4074 - acc: 0.8650\n",
      "Epoch 177/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4089 - acc: 0.8583\n",
      "Epoch 178/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4040 - acc: 0.8623\n",
      "Epoch 179/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4039 - acc: 0.8664\n",
      "Epoch 180/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4082 - acc: 0.8650\n",
      "Epoch 181/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4031 - acc: 0.8664\n",
      "Epoch 182/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4026 - acc: 0.8650\n",
      "Epoch 183/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.4017 - acc: 0.8650\n",
      "Epoch 184/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4050 - acc: 0.8583\n",
      "Epoch 185/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4016 - acc: 0.8664\n",
      "Epoch 186/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4034 - acc: 0.8623\n",
      "Epoch 187/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4026 - acc: 0.8637\n",
      "Epoch 188/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4005 - acc: 0.8650\n",
      "Epoch 189/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4022 - acc: 0.8610\n",
      "Epoch 190/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.4013 - acc: 0.8623\n",
      "Epoch 191/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.4024 - acc: 0.8637\n",
      "Epoch 192/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4005 - acc: 0.8664\n",
      "Epoch 193/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4028 - acc: 0.8596\n",
      "Epoch 194/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4029 - acc: 0.8637\n",
      "Epoch 195/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4034 - acc: 0.8570\n",
      "Epoch 196/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4028 - acc: 0.8583\n",
      "Epoch 197/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4029 - acc: 0.8664\n",
      "Epoch 198/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.4016 - acc: 0.8650\n",
      "Epoch 199/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4005 - acc: 0.8596\n",
      "Epoch 200/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4000 - acc: 0.8596\n",
      "Epoch 201/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.4000 - acc: 0.8637\n",
      "Epoch 202/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3983 - acc: 0.8664\n",
      "Epoch 203/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.4006 - acc: 0.8650\n",
      "Epoch 204/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.4008 - acc: 0.8596\n",
      "Epoch 205/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3986 - acc: 0.8637\n",
      "Epoch 206/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3968 - acc: 0.8664\n",
      "Epoch 207/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3978 - acc: 0.8556\n",
      "Epoch 208/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3970 - acc: 0.8637\n",
      "Epoch 209/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3963 - acc: 0.8623\n",
      "Epoch 210/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3975 - acc: 0.8610\n",
      "Epoch 211/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3973 - acc: 0.8570\n",
      "Epoch 212/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3967 - acc: 0.8637\n",
      "Epoch 213/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3939 - acc: 0.8637\n",
      "Epoch 214/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3971 - acc: 0.8650\n",
      "Epoch 215/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3960 - acc: 0.8637\n",
      "Epoch 216/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3958 - acc: 0.8650\n",
      "Epoch 217/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3960 - acc: 0.8623\n",
      "Epoch 218/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3938 - acc: 0.8610\n",
      "Epoch 219/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3954 - acc: 0.8610\n",
      "Epoch 220/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3946 - acc: 0.8650\n",
      "Epoch 221/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3971 - acc: 0.8570\n",
      "Epoch 222/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3947 - acc: 0.8637\n",
      "Epoch 223/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3976 - acc: 0.8623\n",
      "Epoch 224/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3942 - acc: 0.8623\n",
      "Epoch 225/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3948 - acc: 0.8570\n",
      "Epoch 226/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3927 - acc: 0.8623\n",
      "Epoch 227/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.3932 - acc: 0.8623\n",
      "Epoch 228/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3937 - acc: 0.8610\n",
      "Epoch 229/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3934 - acc: 0.8650\n",
      "Epoch 230/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3984 - acc: 0.8570\n",
      "Epoch 231/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3916 - acc: 0.8637\n",
      "Epoch 232/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3931 - acc: 0.8637\n",
      "Epoch 233/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3930 - acc: 0.8610\n",
      "Epoch 234/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.3934 - acc: 0.8610\n",
      "Epoch 235/500\n",
      "741/741 [==============================] - 0s 305us/step - loss: 0.3919 - acc: 0.8650\n",
      "Epoch 236/500\n",
      "741/741 [==============================] - 0s 317us/step - loss: 0.3941 - acc: 0.8650\n",
      "Epoch 237/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.3913 - acc: 0.8623\n",
      "Epoch 238/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3931 - acc: 0.8650\n",
      "Epoch 239/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3934 - acc: 0.8596\n",
      "Epoch 240/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3941 - acc: 0.8677\n",
      "Epoch 241/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.3901 - acc: 0.8623\n",
      "Epoch 242/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3893 - acc: 0.8610\n",
      "Epoch 243/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3914 - acc: 0.8650\n",
      "Epoch 244/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3887 - acc: 0.8623\n",
      "Epoch 245/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3906 - acc: 0.8637\n",
      "Epoch 246/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 290us/step - loss: 0.3898 - acc: 0.8637\n",
      "Epoch 247/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3881 - acc: 0.8610\n",
      "Epoch 248/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3883 - acc: 0.8623\n",
      "Epoch 249/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3893 - acc: 0.8664\n",
      "Epoch 250/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3870 - acc: 0.8610\n",
      "Epoch 251/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3891 - acc: 0.8583\n",
      "Epoch 252/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3920 - acc: 0.8637\n",
      "Epoch 253/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3901 - acc: 0.8650\n",
      "Epoch 254/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3923 - acc: 0.8610\n",
      "Epoch 255/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3932 - acc: 0.8516\n",
      "Epoch 256/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3899 - acc: 0.8650\n",
      "Epoch 257/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3938 - acc: 0.8556\n",
      "Epoch 258/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3920 - acc: 0.8637\n",
      "Epoch 259/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3898 - acc: 0.8570\n",
      "Epoch 260/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3884 - acc: 0.8650\n",
      "Epoch 261/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3884 - acc: 0.8596\n",
      "Epoch 262/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3874 - acc: 0.8556\n",
      "Epoch 263/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3885 - acc: 0.8637\n",
      "Epoch 264/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3884 - acc: 0.8610\n",
      "Epoch 265/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3885 - acc: 0.8637\n",
      "Epoch 266/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3869 - acc: 0.8583\n",
      "Epoch 267/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3898 - acc: 0.8637\n",
      "Epoch 268/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3894 - acc: 0.8543\n",
      "Epoch 269/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3911 - acc: 0.8583\n",
      "Epoch 270/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3894 - acc: 0.8650\n",
      "Epoch 271/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3874 - acc: 0.8610\n",
      "Epoch 272/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3929 - acc: 0.8529\n",
      "Epoch 273/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3879 - acc: 0.8664\n",
      "Epoch 274/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3892 - acc: 0.8596\n",
      "Epoch 275/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3873 - acc: 0.8610\n",
      "Epoch 276/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3848 - acc: 0.8637\n",
      "Epoch 277/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3870 - acc: 0.8664\n",
      "Epoch 278/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3853 - acc: 0.8623\n",
      "Epoch 279/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3871 - acc: 0.8650\n",
      "Epoch 280/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3847 - acc: 0.8623\n",
      "Epoch 281/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3848 - acc: 0.8637\n",
      "Epoch 282/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3879 - acc: 0.8664\n",
      "Epoch 283/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3855 - acc: 0.8650\n",
      "Epoch 284/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.3850 - acc: 0.8637\n",
      "Epoch 285/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3930 - acc: 0.8516\n",
      "Epoch 286/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3874 - acc: 0.8610\n",
      "Epoch 287/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3845 - acc: 0.8650\n",
      "Epoch 288/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.3852 - acc: 0.8650\n",
      "Epoch 289/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3834 - acc: 0.8664\n",
      "Epoch 290/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3829 - acc: 0.8623\n",
      "Epoch 291/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3835 - acc: 0.8623\n",
      "Epoch 292/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3873 - acc: 0.8664\n",
      "Epoch 293/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3884 - acc: 0.8610\n",
      "Epoch 294/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3866 - acc: 0.8610\n",
      "Epoch 295/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3842 - acc: 0.8650\n",
      "Epoch 296/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3876 - acc: 0.8637\n",
      "Epoch 297/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3873 - acc: 0.8596\n",
      "Epoch 298/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3859 - acc: 0.8637\n",
      "Epoch 299/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3887 - acc: 0.8583\n",
      "Epoch 300/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3839 - acc: 0.8583\n",
      "Epoch 301/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3873 - acc: 0.8637\n",
      "Epoch 302/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3863 - acc: 0.8623\n",
      "Epoch 303/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3830 - acc: 0.8623\n",
      "Epoch 304/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3859 - acc: 0.8610\n",
      "Epoch 305/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3861 - acc: 0.8556\n",
      "Epoch 306/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3843 - acc: 0.8677\n",
      "Epoch 307/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.3836 - acc: 0.8650\n",
      "Epoch 308/500\n",
      "741/741 [==============================] - 0s 325us/step - loss: 0.3840 - acc: 0.8650\n",
      "Epoch 309/500\n",
      "741/741 [==============================] - 0s 324us/step - loss: 0.3834 - acc: 0.8623\n",
      "Epoch 310/500\n",
      "741/741 [==============================] - 0s 312us/step - loss: 0.3845 - acc: 0.8623\n",
      "Epoch 311/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3833 - acc: 0.8664\n",
      "Epoch 312/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3832 - acc: 0.8637\n",
      "Epoch 313/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3857 - acc: 0.8650\n",
      "Epoch 314/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3846 - acc: 0.8610\n",
      "Epoch 315/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3844 - acc: 0.8623\n",
      "Epoch 316/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3837 - acc: 0.8664\n",
      "Epoch 317/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3815 - acc: 0.8650\n",
      "Epoch 318/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3820 - acc: 0.8610\n",
      "Epoch 319/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.3825 - acc: 0.8637\n",
      "Epoch 320/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3833 - acc: 0.8596\n",
      "Epoch 321/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3830 - acc: 0.8637\n",
      "Epoch 322/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.3823 - acc: 0.8637 0s - loss: 0.3945 - acc: 0.860\n",
      "Epoch 323/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3800 - acc: 0.8556\n",
      "Epoch 324/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3812 - acc: 0.8623\n",
      "Epoch 325/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.3813 - acc: 0.8650\n",
      "Epoch 326/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3803 - acc: 0.8623\n",
      "Epoch 327/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 298us/step - loss: 0.3821 - acc: 0.8637\n",
      "Epoch 328/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3807 - acc: 0.8596\n",
      "Epoch 329/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3813 - acc: 0.8650\n",
      "Epoch 330/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3823 - acc: 0.8610\n",
      "Epoch 331/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3842 - acc: 0.8596\n",
      "Epoch 332/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3831 - acc: 0.8596\n",
      "Epoch 333/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3814 - acc: 0.8637\n",
      "Epoch 334/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3823 - acc: 0.8543\n",
      "Epoch 335/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3835 - acc: 0.8650\n",
      "Epoch 336/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3802 - acc: 0.8637\n",
      "Epoch 337/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3801 - acc: 0.8664\n",
      "Epoch 338/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3809 - acc: 0.8637\n",
      "Epoch 339/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3821 - acc: 0.8570\n",
      "Epoch 340/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3866 - acc: 0.8610\n",
      "Epoch 341/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3812 - acc: 0.8596\n",
      "Epoch 342/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3811 - acc: 0.8623\n",
      "Epoch 343/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3812 - acc: 0.8650\n",
      "Epoch 344/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3789 - acc: 0.8637\n",
      "Epoch 345/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3812 - acc: 0.8610\n",
      "Epoch 346/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3788 - acc: 0.8623\n",
      "Epoch 347/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.3780 - acc: 0.8623\n",
      "Epoch 348/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3782 - acc: 0.8664\n",
      "Epoch 349/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.3776 - acc: 0.8623\n",
      "Epoch 350/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.3821 - acc: 0.8596\n",
      "Epoch 351/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3804 - acc: 0.8623\n",
      "Epoch 352/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3794 - acc: 0.8623\n",
      "Epoch 353/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3788 - acc: 0.8623\n",
      "Epoch 354/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3831 - acc: 0.8623\n",
      "Epoch 355/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3811 - acc: 0.8610\n",
      "Epoch 356/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3797 - acc: 0.8570\n",
      "Epoch 357/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3803 - acc: 0.8650\n",
      "Epoch 358/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3777 - acc: 0.8596\n",
      "Epoch 359/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.3808 - acc: 0.8596\n",
      "Epoch 360/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3770 - acc: 0.8664\n",
      "Epoch 361/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.3781 - acc: 0.8650\n",
      "Epoch 362/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3785 - acc: 0.8596\n",
      "Epoch 363/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3781 - acc: 0.8677\n",
      "Epoch 364/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.3782 - acc: 0.8650\n",
      "Epoch 365/500\n",
      "741/741 [==============================] - 0s 314us/step - loss: 0.3768 - acc: 0.8623\n",
      "Epoch 366/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.3776 - acc: 0.8596\n",
      "Epoch 367/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3795 - acc: 0.8596\n",
      "Epoch 368/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3815 - acc: 0.8623\n",
      "Epoch 369/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3797 - acc: 0.8704\n",
      "Epoch 370/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3872 - acc: 0.8664\n",
      "Epoch 371/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3792 - acc: 0.8637\n",
      "Epoch 372/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3794 - acc: 0.8623\n",
      "Epoch 373/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3754 - acc: 0.8637\n",
      "Epoch 374/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3775 - acc: 0.8637\n",
      "Epoch 375/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3756 - acc: 0.8637\n",
      "Epoch 376/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3753 - acc: 0.8637\n",
      "Epoch 377/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3773 - acc: 0.8664\n",
      "Epoch 378/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3768 - acc: 0.8583\n",
      "Epoch 379/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3810 - acc: 0.8583\n",
      "Epoch 380/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3801 - acc: 0.8623\n",
      "Epoch 381/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.3753 - acc: 0.8623\n",
      "Epoch 382/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.3786 - acc: 0.8596\n",
      "Epoch 383/500\n",
      "741/741 [==============================] - 0s 320us/step - loss: 0.3777 - acc: 0.8650\n",
      "Epoch 384/500\n",
      "741/741 [==============================] - 0s 301us/step - loss: 0.3790 - acc: 0.8583\n",
      "Epoch 385/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3758 - acc: 0.8664\n",
      "Epoch 386/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3762 - acc: 0.8610\n",
      "Epoch 387/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3757 - acc: 0.8637\n",
      "Epoch 388/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3754 - acc: 0.8596\n",
      "Epoch 389/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.3751 - acc: 0.8623\n",
      "Epoch 390/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3750 - acc: 0.8583\n",
      "Epoch 391/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3748 - acc: 0.8610\n",
      "Epoch 392/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3757 - acc: 0.8650\n",
      "Epoch 393/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3754 - acc: 0.8650\n",
      "Epoch 394/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3753 - acc: 0.8637\n",
      "Epoch 395/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3750 - acc: 0.8664\n",
      "Epoch 396/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3755 - acc: 0.8637\n",
      "Epoch 397/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3757 - acc: 0.8637\n",
      "Epoch 398/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3748 - acc: 0.8664\n",
      "Epoch 399/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3788 - acc: 0.8570\n",
      "Epoch 400/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3771 - acc: 0.8583\n",
      "Epoch 401/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3781 - acc: 0.8583\n",
      "Epoch 402/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3760 - acc: 0.8623\n",
      "Epoch 403/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3765 - acc: 0.8650\n",
      "Epoch 404/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3760 - acc: 0.8677\n",
      "Epoch 405/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3813 - acc: 0.8516\n",
      "Epoch 406/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3771 - acc: 0.8664\n",
      "Epoch 407/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3766 - acc: 0.8623\n",
      "Epoch 408/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3748 - acc: 0.8650\n",
      "Epoch 409/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 297us/step - loss: 0.3740 - acc: 0.8623\n",
      "Epoch 410/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.3780 - acc: 0.8623\n",
      "Epoch 411/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.3782 - acc: 0.8623\n",
      "Epoch 412/500\n",
      "741/741 [==============================] - 0s 301us/step - loss: 0.3747 - acc: 0.8637\n",
      "Epoch 413/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.3746 - acc: 0.8637\n",
      "Epoch 414/500\n",
      "741/741 [==============================] - 0s 304us/step - loss: 0.3755 - acc: 0.8637\n",
      "Epoch 415/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.3736 - acc: 0.8664\n",
      "Epoch 416/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.3727 - acc: 0.8637\n",
      "Epoch 417/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.3752 - acc: 0.8596\n",
      "Epoch 418/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.3749 - acc: 0.8637\n",
      "Epoch 419/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.3760 - acc: 0.8637\n",
      "Epoch 420/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.3770 - acc: 0.8610\n",
      "Epoch 421/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.3757 - acc: 0.8623\n",
      "Epoch 422/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3758 - acc: 0.8650\n",
      "Epoch 423/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.3768 - acc: 0.8596\n",
      "Epoch 424/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.3751 - acc: 0.8623\n",
      "Epoch 425/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3741 - acc: 0.8623\n",
      "Epoch 426/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3744 - acc: 0.8637\n",
      "Epoch 427/500\n",
      "741/741 [==============================] - 0s 287us/step - loss: 0.3734 - acc: 0.8637\n",
      "Epoch 428/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3764 - acc: 0.8596\n",
      "Epoch 429/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3725 - acc: 0.8610\n",
      "Epoch 430/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.3789 - acc: 0.8570\n",
      "Epoch 431/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3728 - acc: 0.8637\n",
      "Epoch 432/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3737 - acc: 0.8650\n",
      "Epoch 433/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.3731 - acc: 0.8637\n",
      "Epoch 434/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3758 - acc: 0.8610\n",
      "Epoch 435/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3744 - acc: 0.8570\n",
      "Epoch 436/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3719 - acc: 0.8610\n",
      "Epoch 437/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3751 - acc: 0.8650\n",
      "Epoch 438/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3765 - acc: 0.8529\n",
      "Epoch 439/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3811 - acc: 0.8583\n",
      "Epoch 440/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3734 - acc: 0.8623\n",
      "Epoch 441/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3740 - acc: 0.8623\n",
      "Epoch 442/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3738 - acc: 0.8556\n",
      "Epoch 443/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3754 - acc: 0.8637\n",
      "Epoch 444/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3820 - acc: 0.8435\n",
      "Epoch 445/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3743 - acc: 0.8556\n",
      "Epoch 446/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3722 - acc: 0.8664\n",
      "Epoch 447/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3734 - acc: 0.8623\n",
      "Epoch 448/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3719 - acc: 0.8677\n",
      "Epoch 449/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3780 - acc: 0.8637\n",
      "Epoch 450/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3735 - acc: 0.8610\n",
      "Epoch 451/500\n",
      "741/741 [==============================] - 0s 286us/step - loss: 0.3759 - acc: 0.8637\n",
      "Epoch 452/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3741 - acc: 0.8610\n",
      "Epoch 453/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3724 - acc: 0.8637\n",
      "Epoch 454/500\n",
      "741/741 [==============================] - 0s 306us/step - loss: 0.3732 - acc: 0.8623\n",
      "Epoch 455/500\n",
      "741/741 [==============================] - 0s 308us/step - loss: 0.3742 - acc: 0.8637\n",
      "Epoch 456/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.3738 - acc: 0.8623\n",
      "Epoch 457/500\n",
      "741/741 [==============================] - 0s 310us/step - loss: 0.3744 - acc: 0.8583\n",
      "Epoch 458/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3732 - acc: 0.8637\n",
      "Epoch 459/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3728 - acc: 0.8623\n",
      "Epoch 460/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3718 - acc: 0.8623\n",
      "Epoch 461/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3713 - acc: 0.8610\n",
      "Epoch 462/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3733 - acc: 0.8623\n",
      "Epoch 463/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3722 - acc: 0.8664\n",
      "Epoch 464/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3712 - acc: 0.8637\n",
      "Epoch 465/500\n",
      "741/741 [==============================] - 0s 289us/step - loss: 0.3732 - acc: 0.8623\n",
      "Epoch 466/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3737 - acc: 0.8596\n",
      "Epoch 467/500\n",
      "741/741 [==============================] - 0s 309us/step - loss: 0.3743 - acc: 0.8610\n",
      "Epoch 468/500\n",
      "741/741 [==============================] - 0s 316us/step - loss: 0.3747 - acc: 0.8570\n",
      "Epoch 469/500\n",
      "741/741 [==============================] - 0s 313us/step - loss: 0.3765 - acc: 0.8637\n",
      "Epoch 470/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.3790 - acc: 0.8650\n",
      "Epoch 471/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.3773 - acc: 0.8556\n",
      "Epoch 472/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.3752 - acc: 0.8596\n",
      "Epoch 473/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3759 - acc: 0.8583\n",
      "Epoch 474/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3738 - acc: 0.8596\n",
      "Epoch 475/500\n",
      "741/741 [==============================] - 0s 297us/step - loss: 0.3724 - acc: 0.8596\n",
      "Epoch 476/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.3757 - acc: 0.8610\n",
      "Epoch 477/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.3736 - acc: 0.8623\n",
      "Epoch 478/500\n",
      "741/741 [==============================] - 0s 290us/step - loss: 0.3757 - acc: 0.8596\n",
      "Epoch 479/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.3729 - acc: 0.8650\n",
      "Epoch 480/500\n",
      "741/741 [==============================] - 0s 296us/step - loss: 0.3741 - acc: 0.8570\n",
      "Epoch 481/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.3753 - acc: 0.8637\n",
      "Epoch 482/500\n",
      "741/741 [==============================] - 0s 294us/step - loss: 0.3786 - acc: 0.8650\n",
      "Epoch 483/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.3729 - acc: 0.8623\n",
      "Epoch 484/500\n",
      "741/741 [==============================] - 0s 395us/step - loss: 0.3704 - acc: 0.8610\n",
      "Epoch 485/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3719 - acc: 0.8637\n",
      "Epoch 486/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.3736 - acc: 0.8650\n",
      "Epoch 487/500\n",
      "741/741 [==============================] - 0s 292us/step - loss: 0.3733 - acc: 0.8650\n",
      "Epoch 488/500\n",
      "741/741 [==============================] - 0s 293us/step - loss: 0.3710 - acc: 0.8637\n",
      "Epoch 489/500\n",
      "741/741 [==============================] - 0s 323us/step - loss: 0.3826 - acc: 0.8516\n",
      "Epoch 490/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.3791 - acc: 0.8556\n",
      "Epoch 491/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 0s 305us/step - loss: 0.3730 - acc: 0.8610\n",
      "Epoch 492/500\n",
      "741/741 [==============================] - 0s 305us/step - loss: 0.3717 - acc: 0.8664\n",
      "Epoch 493/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.3744 - acc: 0.8610\n",
      "Epoch 494/500\n",
      "741/741 [==============================] - 0s 298us/step - loss: 0.3737 - acc: 0.8623\n",
      "Epoch 495/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.3761 - acc: 0.8583\n",
      "Epoch 496/500\n",
      "741/741 [==============================] - 0s 301us/step - loss: 0.3781 - acc: 0.8516\n",
      "Epoch 497/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.3700 - acc: 0.8650\n",
      "Epoch 498/500\n",
      "741/741 [==============================] - 0s 301us/step - loss: 0.3715 - acc: 0.8650\n",
      "Epoch 499/500\n",
      "741/741 [==============================] - 0s 300us/step - loss: 0.3766 - acc: 0.8637\n",
      "Epoch 500/500\n",
      "741/741 [==============================] - 0s 302us/step - loss: 0.3710 - acc: 0.8664\n"
     ]
    }
   ],
   "source": [
    "#Build equivalent full network with no autoencoder layers\n",
    "equiv_layers = [25, 20, 15, 10, 10, 7, 5, 5, 3, 3]\n",
    "\n",
    "equiv_model = NN_model((X_AE_Train.shape[1], ), AE_layers, regularizers.l2(0.001), None)\n",
    "equiv_model.compile(optimizer = \"Adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "equiv_model.fit(x = X_AE_Train, y = Y_AE_Train, epochs = 500, verbose = 1)\n",
    "\n",
    "\n",
    "e_train_pred = equiv_model.predict(x = X_AE_Train)\n",
    "e_cv_pred = equiv_model.predict(x = X_AE_CV)\n",
    "\n",
    "e_train_hat = normalize_predictions(e_train_pred)\n",
    "e_cv_hat = normalize_predictions(e_cv_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "Ye Olde Neuralnet\n",
      "Accuracy =  86.63967611336032\n",
      "F1 Score =  0.8099808061420345\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True        211.0          27.0\n",
      "1  Pred False         72.0         431.0\n",
      "Accuracy =  76.0\n",
      "F1 Score =  0.6727272727272727\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True         37.0          14.0\n",
      "1  Pred False         22.0          77.0\n",
      "\n",
      "\n",
      "--------------------------\n",
      "Simple Autoencoder\n",
      "Accuracy =  84.61538461538461\n",
      "F1 Score =  0.7738095238095238\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True        195.0          26.0\n",
      "1  Pred False         88.0         432.0\n",
      "Accuracy =  82.0\n",
      "F1 Score =  0.7567567567567568\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True         42.0          10.0\n",
      "1  Pred False         17.0          81.0\n",
      "\n",
      "\n",
      "--------------------------\n",
      "Denoising Autoencoder\n",
      "Accuracy =  85.82995951417004\n",
      "F1 Score =  0.7937131630648329\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True        202.0          24.0\n",
      "1  Pred False         81.0         434.0\n",
      "Accuracy =  82.0\n",
      "F1 Score =  0.7522935779816514\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True         41.0           9.0\n",
      "1  Pred False         18.0          82.0\n"
     ]
    }
   ],
   "source": [
    "print('--------------------------')\n",
    "print('Ye Olde Neuralnet')\n",
    "show_acc(Y_AE_Train, e_train_hat)\n",
    "show_acc(Y_AE_CV, e_cv_hat)\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "print('--------------------------')\n",
    "print('Simple Autoencoder')\n",
    "\n",
    "show_acc(Y_AE_Train, s_train_hat)\n",
    "show_acc(Y_AE_CV, s_cv_hat)\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "print('--------------------------')\n",
    "print('Denoising Autoencoder')\n",
    "\n",
    "show_acc(Y_AE_Train, d_train_hat)\n",
    "show_acc(Y_AE_CV, d_cv_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So interestingly both autoencoders outperformed the standard neural net by a reasonable margin with the same architecture.  \n",
    "\n",
    "Granted a more optimal architecture can probably be found for that training set, but by that metric the same can be said for the autoencoders, so this is a somewhat fair test.  Although the autoencoders had the edge due to gradient vanishing at deeper layers, which by splitting the training this effect is somewhat mitigated.\n",
    "\n",
    "Both autoencoders had very similar performance despite 10% of the denoising autoencoders training input being corrupted, which is quite remarkable.  I expect this has a regularizing effect somewhat similar to dropout regularization in larger scale projects.\n",
    "\n",
    "Overall I would say learning feature representation with denoising autoencoders was a success.\n",
    "\n",
    "One final test is a comparison to how a traditional method would perform on the same training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_svm= svm.SVC(C = 1, kernel= 'rbf')\n",
    "comp_svm.fit(X_AE_Train, Y_AE_Train)\n",
    "\n",
    "train_comp = comp_svm.predict(X_AE_Train)\n",
    "cv_comp = comp_svm.predict(X_AE_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  79.75708502024291\n",
      "F1 Score =  0.7395833333333334\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True        213.0          80.0\n",
      "1  Pred False         70.0         378.0\n",
      "Accuracy =  78.0\n",
      "F1 Score =  0.7226890756302521\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True         43.0          17.0\n",
      "1  Pred False         16.0          74.0\n"
     ]
    }
   ],
   "source": [
    "show_acc(Y_AE_Train, train_comp)\n",
    "show_acc(Y_AE_CV, cv_comp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the autoencoder dramatically outperformed the SVM at this task with this input schema, in fact of all the previous systems I have evaluated for this problem, I would say adding in a denoising autoencoder with a classifier neural network trained after is a worthy addition to a future ensemble."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
