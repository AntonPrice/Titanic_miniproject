{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Classification - Genetic Programming\n",
    "\n",
    "So for this notebook I will be developing a simple genetic algorithm from scratch in order to understand the underlying principles of how it works for use later on.\n",
    "\n",
    "The inspiration for this approach is to make something approaching - \n",
    "\n",
    "https://www.kaggle.com/scirpus/genetic-programming-lb-0-88\n",
    "\n",
    "While there are multiple libraries that achieve everything in this notebook and more in a more efficient way, the purpose of this is to get a grasp on how genetic programming works, so that when I come to use libraries like DEAP in the future I will have a greater understanding.\n",
    "\n",
    "Sources - \n",
    "\n",
    "https://blog.sicara.com/getting-started-genetic-algorithms-python-tutorial-81ffa1dd72f9\n",
    "\n",
    "First up do the usual imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##### First importing some relevant packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Stop pandas from truncating output view\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "#Import Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "#Import Keras\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Dropout, Reshape, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "#Import mathematical functions\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Import and seed random number generator\n",
    "import random\n",
    "from random import *\n",
    "\n",
    "from datetime import datetime\n",
    "seed(datetime.now())\n",
    "\n",
    "#Get regular expression package\n",
    "import re\n",
    "\n",
    "#Import  Scikit learn framework\n",
    "import sklearn as sk\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the functions built in previous parts\n",
    "from Titanic_Import import *\n",
    "\n",
    "full_set = pd.read_csv('D:/Datasets/Titanic/train.csv')\n",
    "sub_set = pd.read_csv('D:/Datasets/Titanic/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_set = full_set\n",
    "append_set = append_set.append([sub_set], ignore_index =True )\n",
    "clean_set = Cleanse_Data_v3(append_set)\n",
    "X_Train, Y_Train, X_CV, Y_CV, X_Test = dataset_splitter(clean_set, cv_size = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly lets define a loss function to fit.  For this I'll use a typical logistic loss function.\n",
    "\n",
    "$J = - \\frac{1}{m} \\sum_{i=1}^{m}{(Y_i\\log{\\hat{Y_i}} + (1 - Y_i)\\log{(1 - \\hat{Y_i})}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Log_Loss(Y_in, Y_hat_in) :\n",
    "    epsilon = 1e-9\n",
    "    m = Y_in.shape[0]\n",
    "    \n",
    "    Y_log = Y_in\n",
    "    Y_hat_log = Y_hat_in\n",
    "    \n",
    "    #Y_log[np.where(Y_log == 0)] = -1\n",
    "    #Y_hat_log[np.where(Y_hat_log == 0)] = -1\n",
    "    \n",
    "    loss = -(1.0/m )*np.sum((1 - Y_log)*np.log(1 - Y_hat_log + epsilon) + Y_log*np.log(Y_hat_log + epsilon))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.641192160147313"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.ones(200)\n",
    "testlog = Log_Loss(Y_CV, test)\n",
    "testlog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to keep this somewhat striaghtforward I will only be using some basic mathematical functions - \n",
    "\n",
    "* Plus\n",
    "* Minus\n",
    "* Multiply\n",
    "* Divide\n",
    "* Min\n",
    "* Max\n",
    "* Log\n",
    "* Sin\n",
    "* Cos\n",
    "* Tanh\n",
    "\n",
    "And using a sigmoid activation output.\n",
    "\n",
    "The general idea is to randomly select a feature and use a random one of the above functions on that output (or select a random number if it requires multiple inputs).\n",
    "\n",
    "For random numbers I will be using a the absolute value gaussian distribution of $\\mu = 1$ and $\\sigma = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of function strings\n",
    "test_func = []\n",
    "\n",
    "test_func.append('np.add(')\n",
    "test_func.append('np.subtract(')\n",
    "test_func.append('np.multiply(')\n",
    "test_func.append('np.divide(')\n",
    "test_func.append('np.minimum(')\n",
    "test_func.append('np.maximum(')\n",
    "test_func.append('np.sin(')\n",
    "test_func.append('np.cos(')\n",
    "test_func.append('np.tanh(')\n",
    "\n",
    "#Create function string mask to determine number of elements\n",
    "#0 = no extra elements\n",
    "#1 = 2 elements\n",
    "#2 = 2 elements and +1e-9 to second element\n",
    "func_mult = (1, 1, 1, 2, 1, 1, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now the functions are defined, I have also defined a mapping for functions that require 1 and 2 inputs, and a special function for divide that will add 1e-9 to avoid divide by zero errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_First_Gen(X_len, func_list, func_map):\n",
    "    select_func = randint(0, len(func_map) - 1)\n",
    "    mask= func_map[select_func]\n",
    "    funct = func_list[select_func]\n",
    "    \n",
    "    column_selector_init = randint(0, X_len)\n",
    "    if column_selector_init == 0 :\n",
    "        rand_num1 = np.abs(gauss(1, 1))\n",
    "        rand_num2 = np.abs(gauss(1, 1))\n",
    "        \n",
    "        if mask == 0:\n",
    "            funcstr = funct + str(rand_num1) + ')'\n",
    "        elif mask == 2:\n",
    "            rand_num = np.abs(gauss(1, 1))\n",
    "            funcstr = funct + str(rand_num2) + ','+ str(rand_num1) + '+1e-9)'\n",
    "        elif mask == 1:  \n",
    "            rand_num = np.abs(gauss(1, 1))\n",
    "            funcstr = funct + str(rand_num2) + ','+ str(rand_num1) + ')'\n",
    "        else :\n",
    "            funcstr = 'ERROR'\n",
    "        \n",
    "    else :\n",
    "        column_selector = column_selector_init - 1\n",
    "  \n",
    "        if mask == 0:\n",
    "            funcstr = funct + 'X[:,' + str(column_selector) + '])'\n",
    "        elif mask == 2:\n",
    "            rand_num = np.abs(gauss(1, 1))\n",
    "            funcstr = funct + 'X[:,' + str(column_selector) + '],'+ str(rand_num) + '+1e-9)'\n",
    "        elif mask == 1:  \n",
    "            rand_num = np.abs(gauss(1, 1))\n",
    "            funcstr = funct + 'X[:,' + str(column_selector) + '],'+ str(rand_num) + ')'\n",
    "        else :\n",
    "            funcstr = 'ERROR'\n",
    "    \n",
    "    return funcstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parent1 = Generate_First_Gen(5, test_func, func_mult)\n",
    "test_parent2 = Generate_First_Gen(5, test_func, func_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'np.tanh(X[:,0])'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_parent1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the first generation produced there needs to be a subsequent generation, this will take the output of 2 previous generations and put them together and apply some form of mutation.\n",
    "\n",
    "So as there are two distinct scenarios for function selected - \n",
    "* Dual Input function\n",
    "* Single Input function\n",
    "\n",
    "##### Dual Input\n",
    "The case of a dual input function being selected is no problem, simply apply the function to both parents.\n",
    "\n",
    "##### Single Input\n",
    "In the case of a single input function being selected then we can select a random parent and apply the function to that parent.\n",
    "\n",
    "##### Mutation Schema\n",
    "For a mutation schema, a simple idea is to have a random chance to call a first generation and recursively apply the algorithm to itself (after the first 2 parents have been processed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_Next_Generation(X_len, parent1, parent2, mut_prob, func_list, func_mask):\n",
    "    select_func = randint(0, len(func_mask) - 1)\n",
    "    mask= func_mask[select_func]\n",
    "    funct = func_list[select_func]\n",
    "    \n",
    "    mutate_rand = uniform(0.0, 1.0)\n",
    "    \n",
    "    if mutate_rand <= mut_prob :\n",
    "        #Mutated outcome\n",
    "        Mutation = Generate_First_Gen(X_len, func_list, func_mask)\n",
    "        #Generate original non-mutated outcome\n",
    "        Orig_output = Generate_Next_Generation(X_len, parent1, parent2, mut_prob, func_list, func_mask)\n",
    "        #Generate final output\n",
    "        funcstr = Generate_Next_Generation(X_len, Orig_output, Mutation, mut_prob, func_list, func_mask)\n",
    "    \n",
    "    else :\n",
    "        #No Mutate outcome\n",
    "        if mask == 0:\n",
    "            #Single Input - choose random parent\n",
    "            parent_rand = uniform(0.0, 1.0)\n",
    "            if parent_rand <= 0.5 :\n",
    "                funcstr = funct + parent1 + ')'\n",
    "            else :\n",
    "                funcstr = funct + parent2 + ')'\n",
    "                \n",
    "        elif mask == 2:\n",
    "            #Divide\n",
    "            rand_num = np.abs(gauss(1, 1))\n",
    "            funcstr = funct +  parent1 + ' , ' + parent2 + '+1e-9)'\n",
    "        elif mask == 1:  \n",
    "            #Dual Input\n",
    "            rand_num = np.abs(gauss(1, 1))\n",
    "            funcstr =funct + parent1 + ' , ' + parent2  + ')'\n",
    "        else :\n",
    "            funcstr = 'ERROR'\n",
    "    \n",
    "    return funcstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_test = Generate_Next_Generation(5, test_parent1, test_parent2, 0.3, test_func, func_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'np.sin(np.multiply(X[:,2],0.046544401782742995))'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have the core functions built the next step will be to evaluate the performance of the candidate by executing the generated string, applying an activation function and then applying the loss function.\n",
    "\n",
    "For simplicity I will use a simple sigmoid activation, and the cross-entropy loss as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid Activation Function - with overflow exception\n",
    "def sigmoid_overf(z):\n",
    "    if hasattr(z, \"__len__\") == True :\n",
    "        z[np.where(z < -7e2)] = -4e-2\n",
    "        \n",
    "        \n",
    "    else :\n",
    "        if z <= -7e2:\n",
    "            z = -4e-2\n",
    "\n",
    "    s = 1/(1+np.exp(-z))  \n",
    "    return s\n",
    "\n",
    "#Redefined predictions due to tuple out of range errors\n",
    "def normalize_predictions_new(y_hat):\n",
    "    #y_out = y_hat.reshape((y_hat.shape[0],))\n",
    "    y_out = np.around(y_hat)\n",
    "    \n",
    "    return y_out\n",
    "\n",
    "def Generate_Predictions(childstr, X):\n",
    "    return sigmoid_overf((eval(childstr)))\n",
    "\n",
    "def Evaluate_Child(childstr, X, Y):\n",
    "    y_hat = Generate_Predictions(childstr, X)\n",
    "    #y_true = normalize_predictions_new(y_hat)\n",
    "    return Log_Loss(Y, y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = Evaluate_Child(child_test, X_CV, Y_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6962387467819267"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to put it all together and iterate over generations, picking the best parents from each generation and pairing them off and then picking the best function of any generation as the final output.\n",
    "\n",
    "\n",
    "### Parameters Needed (due to dependancies)\n",
    "* X\n",
    "* Y\n",
    "* Function list + mapping\n",
    "* Probability of mutation\n",
    "\n",
    "### New Parameters Needed\n",
    "* Number of generations\n",
    "* Number of children per generation\n",
    "* Number of parents per generation\n",
    "* Verbose (0,1)\n",
    "\n",
    "I will also likely need 2 outputs - best children of generation and best children of all time.\n",
    "\n",
    "For verbosity probably outputting the loss of the best child per generation should suffice.\n",
    "\n",
    "Also note that while I have been generating this in parts with individual functions, due to the overlapping parameters I will be putting this into a class at the end.  This will have the benefits of parameter sharing as well as being able to include the option to initialize with a pre-defined population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parent_selector(X, Y, gen_in, num_parents):\n",
    "    #Initialize parameters\n",
    "    num_in = len(gen_in)\n",
    "    gen_out = []\n",
    "    loss_out = []\n",
    "    \n",
    "    \n",
    "    for i in range(num_in):\n",
    "        #Get child and loss of child\n",
    "        child_str = gen_in[i]\n",
    "        child_loss = Evaluate_Child(child_str, X, Y)\n",
    "        \n",
    "        if len(gen_out) <= num_parents:\n",
    "            #If not full generation out then append child\n",
    "            gen_out.append(child_str)\n",
    "            loss_out.append(child_loss)\n",
    "        else :\n",
    "            #Otherwise replace worst child if this child is better\n",
    "            max_loss_out = max(loss_out)\n",
    "            max_loss_out_ind = loss_out.index(max(loss_out))\n",
    "            \n",
    "            if child_loss < max_loss_out :\n",
    "                del loss_out[max_loss_out_ind]\n",
    "                del gen_out[max_loss_out_ind]\n",
    "                \n",
    "                gen_out.append(child_str)\n",
    "                loss_out.append(child_loss)\n",
    "    \n",
    "    return gen_out, loss_out\n",
    "\n",
    "def Best_Children_Selector(new_gen, new_losses, prev_gen, prev_losses) :\n",
    "    #Set best as previous best\n",
    "    best_childs = prev_gen\n",
    "    best_losses = prev_losses\n",
    "    \n",
    "    #Initialize variables\n",
    "    len_new = len(new_gen)\n",
    "    \n",
    "    #Iterate through new generation and replace worst of best if better\n",
    "    for i in range(len_new):\n",
    "        max_best_loss = max(best_losses)\n",
    "        max_best_loss_ind = best_losses.index(max(best_losses))\n",
    "        \n",
    "        loss_this = new_losses[i]\n",
    "\n",
    "        if loss_this < max_best_loss :\n",
    "            del best_childs[max_best_loss_ind]\n",
    "            del best_losses[max_best_loss_ind]\n",
    "            \n",
    "            best_childs.append(new_gen[i])\n",
    "            best_losses.append(loss_this)\n",
    "        \n",
    "    return best_childs, best_losses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Train_Genetic_Alg(X, Y, func_list, func_mask, mut_prob = 0.1, \n",
    "                      num_gens = 10, num_childs = 30, num_parents = 10, Verbose = 0, initial_gen = None) :\n",
    "    \n",
    "    #Define population containers\n",
    "    generation_par = []\n",
    "    hof_pars = []\n",
    "    hof_losses = []\n",
    "    hall_of_fame = {}\n",
    "    \n",
    "    #Initialize parameters\n",
    "    X_len = X.shape[1]\n",
    "    \n",
    "    #Generate first generation\n",
    "    if initial_gen == None :\n",
    "        for i in range(num_parents):\n",
    "            generation_par.append(Generate_First_Gen(X_len, func_list, func_mask))       \n",
    "    else :\n",
    "        generation_par = initial_gen\n",
    "    \n",
    "    \n",
    "    #Iterate through generations\n",
    "    for j in range(num_gens) :\n",
    "        #Kill previous generation's children\n",
    "        generation_out = []\n",
    "        \n",
    "        #Produce k children in generation j\n",
    "        for k in range(num_childs):\n",
    "            num_in = len(generation_par)\n",
    "            \n",
    "            #Pick random parents\n",
    "            par1 = generation_par[randint(0, num_in - 1)]\n",
    "            par2 = generation_par[randint(0, num_in - 1)]\n",
    "            \n",
    "            #Generate child\n",
    "            childk = Generate_Next_Generation(X_len, par1, par2, mut_prob, func_list, func_mask)\n",
    "            \n",
    "            #Append to childs generation\n",
    "            generation_out.append(childk)\n",
    "        \n",
    "        #Clear previous generations parents\n",
    "        generation_par = []\n",
    "        \n",
    "        #Generate new parents\n",
    "        generation_par, par_losses = Parent_selector(X, Y, generation_out, num_parents) \n",
    "        \n",
    "        #Generate Hall of Fame\n",
    "        if len(hof_pars) == 0:\n",
    "            hof_pars, hof_losses = Parent_selector(X, Y, generation_out, 5)\n",
    "        else :\n",
    "            hof_pars, hof_losses = Best_Children_Selector(generation_par, par_losses, hof_pars, hof_losses) \n",
    "            \n",
    "        #Show training progress\n",
    "        if Verbose == 1:\n",
    "            best_in_gen = min(par_losses)\n",
    "            gen_avg = np.mean(par_losses)\n",
    "            best_ever = min(hof_losses)\n",
    "            print(\"Generation %s, BestGen=%s AverageGen=%s, BestEver=%s\" % \n",
    "                  (j, best_in_gen, gen_avg, best_ever))\n",
    "        \n",
    "    \n",
    "    return hof_pars, hof_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0, BestGen=0.6489265661253781 AverageGen=0.6769359453076063, BestEver=0.6489265661253781\n",
      "Generation 1, BestGen=0.6493065087469346 AverageGen=0.654582499825159, BestEver=0.6489265661253781\n",
      "Generation 2, BestGen=0.6102642621376481 AverageGen=0.6227725146328259, BestEver=0.6102642621376481\n",
      "Generation 3, BestGen=0.5846908032913075 AverageGen=0.5939118062581066, BestEver=0.5846908032913075\n",
      "Generation 4, BestGen=0.5820009578444209 AverageGen=0.586713263771836, BestEver=0.5820009578444209\n",
      "Generation 5, BestGen=0.5820009578444209 AverageGen=0.5871509388209893, BestEver=0.5820009578444209\n",
      "Generation 6, BestGen=0.5838954161318731 AverageGen=0.5859328675138422, BestEver=0.5820009578444209\n",
      "Generation 7, BestGen=0.5820009578444209 AverageGen=0.5854977102031033, BestEver=0.5820009578444209\n",
      "Generation 8, BestGen=0.5820009578444209 AverageGen=0.5847178176435102, BestEver=0.5820009578444209\n",
      "Generation 9, BestGen=0.5820009578444209 AverageGen=0.5837875377343494, BestEver=0.5820009578444209\n",
      "Generation 10, BestGen=0.5840481143569503 AverageGen=0.5840481143581897, BestEver=0.5820009578444209\n",
      "Generation 11, BestGen=0.5840481143569503 AverageGen=0.5840481143569501, BestEver=0.5820009578444209\n"
     ]
    }
   ],
   "source": [
    "best_mods, mod_loss = Train_Genetic_Alg(X_Train, Y_Train, test_func, func_mult, mut_prob = 0.2, \n",
    "                      num_gens =12, num_childs = 100, num_parents = 10, Verbose = 1, initial_gen = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1203"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(best_mods[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  70.47756874095514\n",
      "F1 Score =  0.5188679245283019\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True        110.0          50.0\n",
      "1  Pred False        154.0         377.0\n",
      "Accuracy =  73.0\n",
      "F1 Score =  0.578125\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True         37.0          13.0\n",
      "1  Pred False         41.0         109.0\n"
     ]
    }
   ],
   "source": [
    "train_pred = Generate_Predictions(best_mods[0], X_Train)\n",
    "cv_pred = Generate_Predictions(best_mods[0], X_CV)\n",
    "\n",
    "train_hat = normalize_predictions(train_pred)\n",
    "cv_hat = normalize_predictions(cv_pred)\n",
    "\n",
    "show_acc(Y_Train, train_hat)\n",
    "show_acc(Y_CV, cv_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a genetic algorithm constructor, there was a lot of hiccups and a lot of bugs in old code were brought to light, however this algorithm definitely generates solutions.\n",
    "\n",
    "From testing a few areas of the code, the loss function has a major impact, especially the epsilon value to remove ln(0) errors and whether to normalize the predictions before applying the loss function or not.\n",
    "\n",
    "However I would consider this a pretty interesting and successful experiment.\n",
    "\n",
    "## Generating a Final Prediction\n",
    "\n",
    "So as the final model is effectively the n best models, it seems logical to stack the models together into a voting schema as a simple method of ensembling.\n",
    "\n",
    "And in generating this function it will be very useful in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stack all models in output from best genetic algorithms\n",
    "def Stack_models(hof_models, X):\n",
    "    num_mods = len(hof_models)\n",
    "    \n",
    "    for i in range(num_mods):\n",
    "        if i == 0 :\n",
    "            pred = Generate_Predictions(hof_models[i], X)        \n",
    "            stack_hat = normalize_predictions(pred)\n",
    "        else :\n",
    "            pred = Generate_Predictions(hof_models[i], X) \n",
    "            hats = normalize_predictions(pred)\n",
    "            stack_hat = np.vstack((stack_hat, hats))\n",
    "    \n",
    "    return stack_hat\n",
    "\n",
    "#Create simple voting mechanism from stack\n",
    "def pred_from_stack(full_hat):\n",
    "    num_mods = full_hat.shape[0]\n",
    "    cutoff = num_mods / 2.0\n",
    "    \n",
    "    #Sum all votes\n",
    "    out_hat = np.sum(full_hat, axis=0)\n",
    "    \n",
    "    #Allocate majority votes the correct label\n",
    "    out_hat[np.where(out_hat <= cutoff)] = 0\n",
    "    out_hat[np.where(out_hat > cutoff)] = 1\n",
    "    \n",
    "    return out_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(691,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_y = Stack_models(best_mods, X_Train)\n",
    "out_y = pred_from_stack(full_y)\n",
    "\n",
    "out_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  70.47756874095514\n",
      "F1 Score =  0.5188679245283019\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True        110.0          50.0\n",
      "1  Pred False        154.0         377.0\n"
     ]
    }
   ],
   "source": [
    "show_acc(Y_Train, out_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now from all these parts its time to generate a full cohesive model class for this genetic binary classifier.\n",
    "\n",
    "### Methods needed\n",
    "\n",
    "* Initializer\n",
    "* Loss function(s)\n",
    "* Activation function(s)\n",
    "* Generate Predictions\n",
    "* Evaluate Child\n",
    "* Generate initial generation\n",
    "* Generate ith generation (i =/= 0)\n",
    "* Parent Selector\n",
    "* Best Child Selector (/Hall of fame generator)\n",
    "* Train Models\n",
    "* Stack Models\n",
    "* Generate Predictions From Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genetic Algorithm Class\n",
    "class Genetic_Binary_Classifier(object):\n",
    "    def __init__(self, func_list, func_mask, eval_func = 1):\n",
    "        self.func_list = func_list\n",
    "        self.func_mask = func_mask\n",
    "        self.best_models = None\n",
    "        self.eval_func = eval_func\n",
    "        \n",
    "    def Log_Loss(self, Y_in, Y_hat_in) :\n",
    "        epsilon = 1e-9\n",
    "        m = Y_in.shape[0]\n",
    "    \n",
    "        Y_log = Y_in\n",
    "        Y_hat_log = Y_hat_in\n",
    "    \n",
    "        #Y_log[np.where(Y_log == 0)] = -1\n",
    "        #Y_hat_log[np.where(Y_hat_log == 0)] = -1\n",
    "    \n",
    "        loss = -(1.0/m )*np.sum((1 - Y_log)*np.log(1 - Y_hat_log + epsilon) + Y_log*np.log(Y_hat_log + epsilon))\n",
    "    \n",
    "        return loss\n",
    "    \n",
    "    #Calculate our accuracy figures\n",
    "    def Get_acc_loss(self, Y_true, Y_hat) :\n",
    "        #Compute inverse vectors for vectorization\n",
    "        Y_true_inv = 1 - Y_true\n",
    "        Y_hat_inv = 1 - Y_hat\n",
    "        \n",
    "        if hasattr(np.dot(Y_true,Y_hat.T), \"__len__\") == True :\n",
    "            return 100\n",
    "        if hasattr(np.dot(Y_true_inv,Y_hat_inv.T), \"__len__\") == True :  \n",
    "            return 100\n",
    "             \n",
    "        acc = 100 - float((np.dot(Y_true,Y_hat.T) + np.dot(Y_true_inv,Y_hat_inv.T))/float(Y_true.size)*100)\n",
    "    \n",
    "        return acc\n",
    "\n",
    "    #Sigmoid Activation Function - with overflow exception\n",
    "    def sigmoid_overf(self, z):\n",
    "        if hasattr(z, \"__len__\") == True :\n",
    "            z[np.where(z < -7e2)] = -4e-2\n",
    "        \n",
    "        else :\n",
    "            if z <= -7e2:\n",
    "                z = -4e-2\n",
    "\n",
    "        s = 1/(1+np.exp(-z))  \n",
    "        return s\n",
    "\n",
    "    #Redefined predictions due to tuple out of range errors\n",
    "    def normalize_predictions_new(self, y_hat):\n",
    "        #y_out = y_hat.reshape((y_hat.shape[0],))\n",
    "        y_out = np.around(y_hat)\n",
    "        \n",
    "        return y_out\n",
    "    \n",
    "    #Generate first prediction level\n",
    "    def Generate_Predictions(self, childstr, X):\n",
    "        return self.sigmoid_overf((eval(childstr)))\n",
    "    \n",
    "    def Evaluate_Child(self, childstr, X, Y):\n",
    "        y_hat = self.Generate_Predictions(childstr, X)\n",
    "        #y_true = normalize_predictions_new(y_hat)\n",
    "        return self.Log_Loss(Y, y_hat)\n",
    "    \n",
    "    def Evaluate_Child_v2(self, childstr, X, Y):\n",
    "        y_hat = self.Generate_Predictions(childstr, X)\n",
    "        y_true = self.normalize_predictions_new(y_hat)\n",
    "        return self.Log_Loss(Y, y_true)\n",
    "    \n",
    "    def Evaluate_Child_v3(self, childstr, X, Y):\n",
    "        y_hat = self.Generate_Predictions(childstr, X)\n",
    "        y_true = self.normalize_predictions_new(y_hat)\n",
    "        return self.Get_acc_loss(Y, y_true)\n",
    "\n",
    "    #First Generation\n",
    "    def Generate_First_Gen(self, X_len, func_list, func_map):\n",
    "        select_func = randint(0, len(func_map) - 1)\n",
    "        mask= func_map[select_func]\n",
    "        funct = func_list[select_func]\n",
    "\n",
    "        column_selector_init = randint(0, X_len)\n",
    "        if column_selector_init == 0 :\n",
    "            rand_num1 = np.abs(gauss(1, 1))\n",
    "            rand_num2 = np.abs(gauss(1, 1))\n",
    "\n",
    "            if mask == 0:\n",
    "                funcstr = funct + str(rand_num1) + ')'\n",
    "            elif mask == 2:\n",
    "                rand_num = np.abs(gauss(1, 1))\n",
    "                funcstr = funct + str(rand_num2) + ','+ str(rand_num1) + '+1e-9)'\n",
    "            elif mask == 1:  \n",
    "                rand_num = np.abs(gauss(1, 1))\n",
    "                funcstr = funct + str(rand_num2) + ','+ str(rand_num1) + ')'\n",
    "            else :\n",
    "                funcstr = 'ERROR'\n",
    "\n",
    "        else :\n",
    "            column_selector = column_selector_init - 1\n",
    "  \n",
    "            if mask == 0:\n",
    "                funcstr = funct + 'X[:,' + str(column_selector) + '])'\n",
    "            elif mask == 2:\n",
    "                rand_num = np.abs(gauss(1, 1))\n",
    "                funcstr = funct + 'X[:,' + str(column_selector) + '],'+ str(rand_num) + '+1e-9)'\n",
    "            elif mask == 1:  \n",
    "                rand_num = np.abs(gauss(1, 1))\n",
    "                funcstr = funct + 'X[:,' + str(column_selector) + '],'+ str(rand_num) + ')'\n",
    "            else :\n",
    "                funcstr = 'ERROR'\n",
    "\n",
    "        return funcstr\n",
    "    \n",
    "    #Gen Subsequent Generation\n",
    "    def Generate_Next_Generation(self, X_len, parent1, parent2, mut_prob, func_list, func_mask):\n",
    "        select_func = randint(0, len(func_mask) - 1)\n",
    "        mask= func_mask[select_func]\n",
    "        funct = func_list[select_func]\n",
    "        \n",
    "        mutate_rand = uniform(0.0, 1.0)\n",
    "        \n",
    "        if mutate_rand <= mut_prob :\n",
    "            #Mutated outcome\n",
    "            Mutation = self.Generate_First_Gen(X_len, func_list, func_mask)\n",
    "            #Generate original non-mutated outcome\n",
    "            Orig_output = self.Generate_Next_Generation(X_len, parent1, parent2, mut_prob, func_list, func_mask)\n",
    "            #Generate final output\n",
    "            funcstr = self.Generate_Next_Generation(X_len, Orig_output, Mutation, mut_prob, func_list, func_mask)\n",
    "        \n",
    "        else :\n",
    "            #No Mutate outcome\n",
    "            if mask == 0:\n",
    "                #Single Input - choose random parent\n",
    "                parent_rand = uniform(0.0, 1.0)\n",
    "                if parent_rand <= 0.5 :\n",
    "                    funcstr = funct + parent1 + ')'\n",
    "                else :\n",
    "                    funcstr = funct + parent2 + ')'\n",
    "                    \n",
    "            elif mask == 2:\n",
    "                #Divide\n",
    "                rand_num = np.abs(gauss(1, 1))\n",
    "                funcstr = funct +  parent1 + ' , ' + parent2 + '+1e-9)'\n",
    "            elif mask == 1:  \n",
    "                #Dual Input\n",
    "                rand_num = np.abs(gauss(1, 1))\n",
    "                funcstr =funct + parent1 + ' , ' + parent2  + ')'\n",
    "            else :\n",
    "                funcstr = 'ERROR'\n",
    "        \n",
    "        select_func = randint(0, len(func_mask) - 1)\n",
    "        mask= func_mask[select_func]\n",
    "        funct = func_list[select_func]\n",
    "        \n",
    "        mutate_rand = uniform(0.0, 1.0)\n",
    "        \n",
    "        if mutate_rand <= mut_prob :\n",
    "            #Mutated outcome\n",
    "            Mutation = self.Generate_First_Gen(X_len, func_list, func_mask)\n",
    "            #Generate original non-mutated outcome\n",
    "            Orig_output = self.Generate_Next_Generation(X_len, parent1, parent2, mut_prob, func_list, func_mask)\n",
    "            #Generate final output\n",
    "            funcstr = self.Generate_Next_Generation(X_len, Orig_output, Mutation, mut_prob, func_list, func_mask)\n",
    "        return funcstr\n",
    "    \n",
    "    def Parent_selector(self, X, Y, gen_in, num_parents):\n",
    "        #Initialize parameters\n",
    "        num_in = len(gen_in)\n",
    "        gen_out = []\n",
    "        loss_out = []\n",
    "        \n",
    "        \n",
    "        for i in range(num_in):\n",
    "            #Get child and loss of child\n",
    "            child_str = gen_in[i]\n",
    "            if self.eval_func == 2 :\n",
    "                child_loss = self.Evaluate_Child_v2(child_str, X, Y)\n",
    "            elif self.eval_func == 3 :\n",
    "                child_loss = self.Evaluate_Child_v3(child_str, X, Y)\n",
    "            else:\n",
    "                child_loss = self.Evaluate_Child(child_str, X, Y)\n",
    "            \n",
    "            if len(gen_out) <= num_parents:\n",
    "                #If not full generation out then append child\n",
    "                gen_out.append(child_str)\n",
    "                loss_out.append(child_loss)\n",
    "            else :\n",
    "                #Otherwise replace worst child if this child is better\n",
    "                max_loss_out = max(loss_out)\n",
    "                max_loss_out_ind = loss_out.index(max(loss_out))\n",
    "                \n",
    "                if child_loss < max_loss_out :\n",
    "                    del loss_out[max_loss_out_ind]\n",
    "                    del gen_out[max_loss_out_ind]\n",
    "                    \n",
    "                    gen_out.append(child_str)\n",
    "                    loss_out.append(child_loss)\n",
    "        \n",
    "        return gen_out, loss_out\n",
    "    \n",
    "    def Best_Children_Selector(self, new_gen, new_losses, prev_gen, prev_losses) :\n",
    "        #Set best as previous best\n",
    "        best_childs = prev_gen\n",
    "        best_losses = prev_losses\n",
    "        \n",
    "        #Initialize variables\n",
    "        len_new = len(new_gen)\n",
    "        \n",
    "        #Iterate through new generation and replace worst of best if better\n",
    "        for i in range(len_new):\n",
    "            max_best_loss = max(best_losses)\n",
    "            max_best_loss_ind = best_losses.index(max(best_losses))\n",
    "            \n",
    "            loss_this = new_losses[i]\n",
    "    \n",
    "            if loss_this < max_best_loss :\n",
    "                del best_childs[max_best_loss_ind]\n",
    "                del best_losses[max_best_loss_ind]\n",
    "                \n",
    "                best_childs.append(new_gen[i])\n",
    "                best_losses.append(loss_this)\n",
    "            \n",
    "        return best_childs, best_losses\n",
    "    \n",
    "    def Train_Genetic_Alg(self, X, Y, mut_prob = 0.1, num_gens = 10, num_childs = 30, \n",
    "                          num_parents = 10, Verbose = 0, initial_gen = None, Output = 0) :\n",
    "        \n",
    "        #Define population containers\n",
    "        generation_par = []\n",
    "        hof_pars = []\n",
    "        hof_losses = []\n",
    "        hall_of_fame = {}\n",
    "        \n",
    "        func_list = self.func_list\n",
    "        func_mask = self.func_mask\n",
    "        #Initialize parameters\n",
    "        X_len = X.shape[1]\n",
    "        \n",
    "        #Generate first generation\n",
    "        if self.best_models != None :\n",
    "            generation_par = best_models\n",
    "        elif initial_gen == None :\n",
    "            for i in range(num_parents):\n",
    "                generation_par.append(self.Generate_First_Gen(X_len, func_list, func_mask))       \n",
    "        else :\n",
    "            generation_par = initial_gen\n",
    "        \n",
    "        \n",
    "        #Iterate through generations\n",
    "        for j in range(num_gens) :\n",
    "            #Kill previous generation's children\n",
    "            generation_out = []\n",
    "            \n",
    "            #Produce k children in generation j\n",
    "            for k in range(num_childs):\n",
    "                num_in = len(generation_par)\n",
    "                \n",
    "                #Pick random parents\n",
    "                par1 = generation_par[randint(0, num_in - 1)]\n",
    "                par2 = generation_par[randint(0, num_in - 1)]\n",
    "                \n",
    "                #Generate child\n",
    "                childk = self.Generate_Next_Generation(X_len, par1, par2, mut_prob, func_list, func_mask)\n",
    "                \n",
    "                #Append to childs generation\n",
    "                generation_out.append(childk)\n",
    "            \n",
    "            #Clear previous generations parents\n",
    "            generation_par = []\n",
    "            \n",
    "            #Generate new parents\n",
    "            generation_par, par_losses = self.Parent_selector(X, Y, generation_out, num_parents) \n",
    "            \n",
    "            #Generate Hall of Fame\n",
    "            if len(hof_pars) == 0:\n",
    "                hof_pars, hof_losses = self.Parent_selector(X, Y, generation_out, 5)\n",
    "            else :\n",
    "                hof_pars, hof_losses = self.Best_Children_Selector(generation_par, par_losses, hof_pars, hof_losses) \n",
    "                \n",
    "            #Show training progress\n",
    "            if Verbose == 1:\n",
    "                best_in_gen = min(par_losses)\n",
    "                gen_avg = np.mean(par_losses)\n",
    "                best_ever = min(hof_losses)\n",
    "                print(\"Generation %s, BestGen=%s AverageGen=%s, BestEver=%s\" % \n",
    "                      (j, best_in_gen, gen_avg, best_ever))\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        if Verbose == 1 :\n",
    "            for p in range(len(hof_pars)):\n",
    "                max_len = 0\n",
    "                test_len = len(hof_pars[p])\n",
    "                if p == 0 :\n",
    "                    max_len = test_len\n",
    "                if max_len < test_len:\n",
    "                    max_len = test_len\n",
    "\n",
    "            print(\"Largest Model = \", max_len)\n",
    "            \n",
    "        self.best_models = hof_pars\n",
    "        \n",
    "        if Output == 1:\n",
    "            return hof_pars, hof_losses\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "        #Stack all models in output from best genetic algorithms\n",
    "    def Stack_models(self, hof_models, X):\n",
    "        num_mods = len(hof_models)\n",
    "        \n",
    "        for i in range(num_mods):\n",
    "            if i == 0 :\n",
    "                pred = self.Generate_Predictions(hof_models[i], X)        \n",
    "                stack_hat = self.normalize_predictions_new(pred)\n",
    "            else :\n",
    "                pred = self.Generate_Predictions(hof_models[i], X) \n",
    "                hats = self.normalize_predictions_new(pred)\n",
    "                stack_hat = np.vstack((stack_hat, hats))\n",
    "        \n",
    "        return stack_hat\n",
    "    \n",
    "    #Create simple voting mechanism from stack\n",
    "    def pred_from_stack(self, full_hat):\n",
    "        num_mods = full_hat.shape[0]\n",
    "        cutoff = num_mods / 2.0\n",
    "        \n",
    "        #Sum all votes\n",
    "        out_hat = np.sum(full_hat, axis=0)\n",
    "        \n",
    "        #Allocate majority votes the correct label\n",
    "        out_hat[np.where(out_hat <= cutoff)] = 0\n",
    "        out_hat[np.where(out_hat > cutoff)] = 1\n",
    "        \n",
    "        return out_hat\n",
    "    \n",
    "    #Generate Predictions from ensemble\n",
    "    def Gen_Final_Preds(self, X):\n",
    "        full_stack = self.Stack_models(self.best_models, X)\n",
    "        out_preds = self.pred_from_stack(full_stack)\n",
    "        return out_preds\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of function strings\n",
    "fin_func = []\n",
    "fin_func.append('np.add(')\n",
    "fin_func.append('np.subtract(')\n",
    "fin_func.append('np.multiply(')\n",
    "fin_func.append('np.divide(')\n",
    "fin_func.append('np.minimum(')\n",
    "fin_func.append('np.maximum(')\n",
    "fin_func.append('np.sin(')\n",
    "fin_func.append('np.cos(')\n",
    "fin_func.append('np.tanh(')\n",
    "\n",
    "#Create function string mask to determine number of elements\n",
    "#0 = no extra elements\n",
    "#1 = 2 elements\n",
    "#2 = 2 elements and +1e-9 to second element\n",
    "func_fin_msk = (1, 1, 1, 2, 1, 1, 0, 0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0, BestGen=6.627846236606595 AverageGen=7.5466385780600795, BestEver=6.627846236606595\n",
      "Generation 1, BestGen=5.57818733019831 AverageGen=6.453357483333528, BestEver=5.57818733019831\n",
      "Generation 2, BestGen=5.158323767634997 AverageGen=5.646346999445601, BestEver=5.158323767634997\n",
      "Generation 3, BestGen=4.858421222946916 AverageGen=5.420056897544595, BestEver=4.858421222946916\n",
      "Generation 4, BestGen=4.738460205071683 AverageGen=4.975655854052256, BestEver=4.738460205071683\n",
      "Generation 5, BestGen=4.228625879101944 AverageGen=4.730281044762008, BestEver=4.228625879101944\n",
      "Generation 6, BestGen=4.228625879101944 AverageGen=4.561245065028725, BestEver=4.228625879101944\n",
      "Generation 7, BestGen=4.228625879101944 AverageGen=4.4058410191449005, BestEver=4.228625879101944\n",
      "Generation 8, BestGen=4.228625879101944 AverageGen=4.228625879101943, BestEver=4.228625879101944\n",
      "Generation 9, BestGen=4.078674606757904 AverageGen=4.214993945252485, BestEver=4.078674606757904\n",
      "Generation 10, BestGen=4.078674606757904 AverageGen=4.201362011403027, BestEver=4.078674606757904\n",
      "Generation 11, BestGen=4.078674606757904 AverageGen=4.133202342155736, BestEver=4.078674606757904\n",
      "Generation 12, BestGen=4.048684352289096 AverageGen=4.075948219988012, BestEver=4.048684352289096\n",
      "Generation 13, BestGen=4.048684352289096 AverageGen=4.062316286138554, BestEver=4.048684352289096\n",
      "Generation 14, BestGen=4.048684352289096 AverageGen=4.048684352289095, BestEver=4.048684352289096\n",
      "Largest Model =  723847\n"
     ]
    }
   ],
   "source": [
    "genetic_mod = Genetic_Binary_Classifier(fin_func, func_fin_msk,eval_func = 2)\n",
    "genetic_mod.Train_Genetic_Alg(X_Train, Y_Train, mut_prob = 0.15, num_gens = 15, num_childs = 100, num_parents = 10, Verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Resuts\n",
      "Accuracy =  80.4630969609262\n",
      "F1 Score =  0.7169811320754716\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True        171.0          42.0\n",
      "1  Pred False         93.0         385.0\n",
      "-------------------------------\n",
      "Cross Validation Resuts\n",
      "Accuracy =  84.5\n",
      "F1 Score =  0.7832167832167831\n",
      "\n",
      "Confusion Matrix\n",
      "       Labels  Actual True  Actual False\n",
      "0   Pred True         56.0           9.0\n",
      "1  Pred False         22.0         113.0\n"
     ]
    }
   ],
   "source": [
    "gen_train = genetic_mod.Gen_Final_Preds(X_Train)\n",
    "gen_CV = genetic_mod.Gen_Final_Preds(X_CV)\n",
    "\n",
    "print('Training Resuts')  \n",
    "show_acc(Y_Train, gen_train)\n",
    "print('-------------------------------')\n",
    "print('Cross Validation Resuts')  \n",
    "show_acc(Y_CV, gen_CV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this was informative as to how genetic algorithms work.  \n",
    "\n",
    "From the above I'm still pretty sure there's a bug somewhere in the class implementation, however I have been unable to locate it.\n",
    "\n",
    "From my own observations its interesting to note you can evaluate over any metric one chooses in this algorithm(in the use as a binary classifier), I chose logistic loss, normalized logistic loss and straight up accuracy and found that normalized logistic loss was generally generating the best algorithms overall.\n",
    "\n",
    "This is also far from an optimized implementation, however this was not the point of the excerisize as adding in K-fold training and batch training are obvious immediate areas for improvement.\n",
    "\n",
    "In terms of performance it was highly dependant upon the number of children/parents per generation and the number of generations ran, however due to the runtimes getting exponentially longer with more generations, however I was able to fairly consistently apprach the 80-85% on training/cross validation accuracy marks on some of the longer runs.  So overall this algorithm does perform okay, just at a heavy computational cost.\n",
    "\n",
    "Also I did not experiment too much with changing the mutation parameter due to hitting the maximum recursion depth on values above ~0.3.\n",
    "\n",
    "However using this programming paradigm to automatically generate and optimize different features of other machine learning algorithm could be very powerful, such as dynamically generating neural network architectures.  Unfortunately it is VERY computationally expensive and slow to run, however its definitely a useful tool to have.\n",
    "\n",
    "In the future I will probably use a genetic programming framework (DEAP is the first obvious candidate) and use that implementation to dramatically speed up progress.\n",
    "\n",
    "Next part to follow, although have yet to decide what to do."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
