{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Classification - Neural Network Model and Ensembling (Part 4)\n",
    "\n",
    "This notebook will be to build a neural network model to see how that performs and put together our final overall ensemble model.\n",
    "\n",
    "So for this notebook I will be using the keras (with tensorflow backend) framework to build a simple neural network.\n",
    "\n",
    "Due to the nature of the problem as a simple binary classification problem a simple fully connected neural network should suffice, and no more complex architectures are necessary.\n",
    "\n",
    "So firstly lets import our packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "#Import Keras\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "#Import mathematical functions\n",
    "from random import *\n",
    "import math\n",
    "\n",
    "#Import  Scikit learn framework\n",
    "import sklearn as sk\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course import our pre-built functions and data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the functions built in Parts 1 and 2\n",
    "from Titanic_Import import *\n",
    "\n",
    "full_set = pd.read_csv('D:/Datasets/Titanic/train.csv')\n",
    "\n",
    "#import data we cleansed last time\n",
    "X_Train, X_CV, Y_Train, Y_CV = Cleanse_Training_Data(full_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets build our first simple model, and ensure that it takes a list as the architecture so we can use this function to quickly iterate over a variety of architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model(input_shape, layers, act_reg, ker_reg):\n",
    "    #Having dynamic input shape as I may do feature engineering later.\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    X = Dense(layers[0], input_dim=input_shape, activation='relu')(X_input)\n",
    "    #X = LeakyReLU()(X)\n",
    "\n",
    "    #Our NN Layers\n",
    "    for i in range(len(layers) - 1):\n",
    "      X = Dense(layers[i + 1], activation='relu', activity_regularizer = act_reg, kernel_regularizer = ker_reg)(X)\n",
    "      #X = LeakyReLU()(X)\n",
    "\n",
    "    \n",
    "    X = Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "    model = Model(inputs = X_input, outputs = X, name='Simple_model')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our function lets test its performance with a architecture, so for simplicity lets make a intuitive architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "791/791 [==============================] - 0s 627us/step - loss: 1.0899 - acc: 0.4260\n",
      "Epoch 2/64\n",
      "791/791 [==============================] - 0s 240us/step - loss: 1.0237 - acc: 0.7206\n",
      "Epoch 3/64\n",
      "791/791 [==============================] - 0s 235us/step - loss: 0.9676 - acc: 0.7838\n",
      "Epoch 4/64\n",
      "791/791 [==============================] - 0s 236us/step - loss: 0.9154 - acc: 0.7927\n",
      "Epoch 5/64\n",
      "791/791 [==============================] - 0s 239us/step - loss: 0.8626 - acc: 0.7863\n",
      "Epoch 6/64\n",
      "791/791 [==============================] - 0s 245us/step - loss: 0.8077 - acc: 0.8040\n",
      "Epoch 7/64\n",
      "791/791 [==============================] - 0s 235us/step - loss: 0.7617 - acc: 0.7965\n",
      "Epoch 8/64\n",
      "791/791 [==============================] - 0s 236us/step - loss: 0.7204 - acc: 0.8040\n",
      "Epoch 9/64\n",
      "791/791 [==============================] - 0s 240us/step - loss: 0.6845 - acc: 0.8091\n",
      "Epoch 10/64\n",
      "791/791 [==============================] - 0s 244us/step - loss: 0.6569 - acc: 0.8078\n",
      "Epoch 11/64\n",
      "791/791 [==============================] - 0s 244us/step - loss: 0.6325 - acc: 0.8180\n",
      "Epoch 12/64\n",
      "791/791 [==============================] - 0s 253us/step - loss: 0.6132 - acc: 0.8180\n",
      "Epoch 13/64\n",
      "791/791 [==============================] - 0s 284us/step - loss: 0.5941 - acc: 0.8217\n",
      "Epoch 14/64\n",
      "791/791 [==============================] - 0s 257us/step - loss: 0.5789 - acc: 0.8217\n",
      "Epoch 15/64\n",
      "791/791 [==============================] - 0s 260us/step - loss: 0.5651 - acc: 0.8268\n",
      "Epoch 16/64\n",
      "791/791 [==============================] - 0s 249us/step - loss: 0.5524 - acc: 0.8293\n",
      "Epoch 17/64\n",
      "791/791 [==============================] - 0s 238us/step - loss: 0.5415 - acc: 0.8268\n",
      "Epoch 18/64\n",
      "791/791 [==============================] - 0s 239us/step - loss: 0.5323 - acc: 0.8230\n",
      "Epoch 19/64\n",
      "791/791 [==============================] - 0s 245us/step - loss: 0.5237 - acc: 0.8281\n",
      "Epoch 20/64\n",
      "791/791 [==============================] - 0s 241us/step - loss: 0.5164 - acc: 0.8268\n",
      "Epoch 21/64\n",
      "791/791 [==============================] - 0s 235us/step - loss: 0.5112 - acc: 0.8357\n",
      "Epoch 22/64\n",
      "791/791 [==============================] - 0s 240us/step - loss: 0.5064 - acc: 0.8306\n",
      "Epoch 23/64\n",
      "791/791 [==============================] - 0s 238us/step - loss: 0.5014 - acc: 0.8268\n",
      "Epoch 24/64\n",
      "791/791 [==============================] - 0s 241us/step - loss: 0.4948 - acc: 0.8293\n",
      "Epoch 25/64\n",
      "791/791 [==============================] - 0s 240us/step - loss: 0.4924 - acc: 0.8230\n",
      "Epoch 26/64\n",
      "791/791 [==============================] - 0s 236us/step - loss: 0.4865 - acc: 0.8293\n",
      "Epoch 27/64\n",
      "791/791 [==============================] - 0s 241us/step - loss: 0.4862 - acc: 0.8192\n",
      "Epoch 28/64\n",
      "791/791 [==============================] - 0s 249us/step - loss: 0.4857 - acc: 0.8268\n",
      "Epoch 29/64\n",
      "791/791 [==============================] - 0s 243us/step - loss: 0.4780 - acc: 0.8344\n",
      "Epoch 30/64\n",
      "791/791 [==============================] - 0s 238us/step - loss: 0.4760 - acc: 0.8357\n",
      "Epoch 31/64\n",
      "791/791 [==============================] - 0s 243us/step - loss: 0.4720 - acc: 0.8394\n",
      "Epoch 32/64\n",
      "791/791 [==============================] - 0s 241us/step - loss: 0.4733 - acc: 0.8268\n",
      "Epoch 33/64\n",
      "791/791 [==============================] - 0s 243us/step - loss: 0.4700 - acc: 0.8268\n",
      "Epoch 34/64\n",
      "791/791 [==============================] - 0s 244us/step - loss: 0.4684 - acc: 0.8331\n",
      "Epoch 35/64\n",
      "791/791 [==============================] - 0s 244us/step - loss: 0.4653 - acc: 0.8357\n",
      "Epoch 36/64\n",
      "791/791 [==============================] - 0s 239us/step - loss: 0.4639 - acc: 0.8382\n",
      "Epoch 37/64\n",
      "791/791 [==============================] - 0s 239us/step - loss: 0.4630 - acc: 0.8331\n",
      "Epoch 38/64\n",
      "791/791 [==============================] - 0s 239us/step - loss: 0.4614 - acc: 0.8331\n",
      "Epoch 39/64\n",
      "791/791 [==============================] - 0s 239us/step - loss: 0.4599 - acc: 0.8319\n",
      "Epoch 40/64\n",
      "791/791 [==============================] - 0s 239us/step - loss: 0.4597 - acc: 0.8344\n",
      "Epoch 41/64\n",
      "791/791 [==============================] - 0s 236us/step - loss: 0.4577 - acc: 0.8319\n",
      "Epoch 42/64\n",
      "791/791 [==============================] - 0s 245us/step - loss: 0.4586 - acc: 0.8293\n",
      "Epoch 43/64\n",
      "791/791 [==============================] - 0s 239us/step - loss: 0.4567 - acc: 0.8319\n",
      "Epoch 44/64\n",
      "791/791 [==============================] - 0s 248us/step - loss: 0.4558 - acc: 0.8407\n",
      "Epoch 45/64\n",
      "791/791 [==============================] - 0s 244us/step - loss: 0.4544 - acc: 0.8306\n",
      "Epoch 46/64\n",
      "791/791 [==============================] - 0s 240us/step - loss: 0.4541 - acc: 0.8357\n",
      "Epoch 47/64\n",
      "791/791 [==============================] - 0s 245us/step - loss: 0.4526 - acc: 0.8255\n",
      "Epoch 48/64\n",
      "791/791 [==============================] - 0s 240us/step - loss: 0.4526 - acc: 0.8306\n",
      "Epoch 49/64\n",
      "791/791 [==============================] - 0s 241us/step - loss: 0.4515 - acc: 0.8382\n",
      "Epoch 50/64\n",
      "791/791 [==============================] - 0s 244us/step - loss: 0.4511 - acc: 0.8331\n",
      "Epoch 51/64\n",
      "791/791 [==============================] - 0s 244us/step - loss: 0.4500 - acc: 0.8357\n",
      "Epoch 52/64\n",
      "791/791 [==============================] - 0s 245us/step - loss: 0.4497 - acc: 0.8319\n",
      "Epoch 53/64\n",
      "791/791 [==============================] - 0s 250us/step - loss: 0.4509 - acc: 0.8344\n",
      "Epoch 54/64\n",
      "791/791 [==============================] - 0s 244us/step - loss: 0.4538 - acc: 0.8319\n",
      "Epoch 55/64\n",
      "791/791 [==============================] - 0s 233us/step - loss: 0.4525 - acc: 0.8293\n",
      "Epoch 56/64\n",
      "791/791 [==============================] - 0s 239us/step - loss: 0.4460 - acc: 0.8357\n",
      "Epoch 57/64\n",
      "791/791 [==============================] - 0s 244us/step - loss: 0.4467 - acc: 0.8382\n",
      "Epoch 58/64\n",
      "791/791 [==============================] - 0s 244us/step - loss: 0.4473 - acc: 0.8369\n",
      "Epoch 59/64\n",
      "791/791 [==============================] - 0s 241us/step - loss: 0.4466 - acc: 0.8344\n",
      "Epoch 60/64\n",
      "791/791 [==============================] - 0s 247us/step - loss: 0.4488 - acc: 0.8319\n",
      "Epoch 61/64\n",
      "791/791 [==============================] - 0s 243us/step - loss: 0.4458 - acc: 0.8344\n",
      "Epoch 62/64\n",
      "791/791 [==============================] - 0s 248us/step - loss: 0.4453 - acc: 0.8293\n",
      "Epoch 63/64\n",
      "791/791 [==============================] - 0s 244us/step - loss: 0.4447 - acc: 0.8344\n",
      "Epoch 64/64\n",
      "791/791 [==============================] - 0s 255us/step - loss: 0.4452 - acc: 0.8344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d0d5748>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_archit = [20, 14, 10, 7, 5]\n",
    "\n",
    "first_model = NN_model((24, ), nn_archit, None, regularizers.l2(0.01))\n",
    "first_model.compile(optimizer = \"Adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "first_model.fit(x = X_Train, y = Y_Train, epochs = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to evaluate how our model does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "791/791 [==============================] - 0s 153us/step\n",
      "\n",
      "Loss = 0.442087286793628\n",
      "Accuracy = 0.8394437427014376\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (1 line)\n",
    "preds = first_model.evaluate(x = X_Train, y = Y_Train)\n",
    "### END CODE HERE ###~\n",
    "print()\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 210us/step\n",
      "\n",
      "Loss = 0.42887641668319704\n",
      "Test Accuracy = 0.84\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (1 line)\n",
    "preds2 = first_model.evaluate(x = X_CV, y = Y_CV)\n",
    "### END CODE HERE ###\n",
    "print()\n",
    "print (\"Loss = \" + str(preds2[0]))\n",
    "print (\"Test Accuracy = \" + str(preds2[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So overall our first neural network performs..okay but not great.  So after spending longer than I'd like to iterating over random architectures, lets build a function to take care of that for us and output a neural network architecture.\n",
    "\n",
    "The idea is to randomly search over an array of architectures and return the best one, in an ideal world some form of reinforcement learning algorithm would be used to do this, however for now lets do a brute force method with a random search.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Architecture(X_Train_2, Y_Train_2, X_CV_2, Y_CV_2, max_layers = 10, num_iters = 32): \n",
    "    best_perf = 0.0\n",
    "    #Iterate through n interations\n",
    "    for i in range(num_iters):\n",
    "        #Reset hyperparameters and initalize nn depth\n",
    "        layers = []\n",
    "        num_layers = randint(3, max_layers)\n",
    "        prev_layer = X_Train_2.shape[1]\n",
    "        \n",
    "        for j in range(num_layers):\n",
    "            #Randomly generate number of units per layer\n",
    "            min_size = math.ceil(prev_layer / 2.0)\n",
    "            lay_size = randint(min_size, prev_layer)\n",
    "            layers.append(lay_size)\n",
    "            prev_layer = lay_size\n",
    "            \n",
    "        #Build and test model\n",
    "        test_model = NN_model((X_Train_2.shape[1], ), layers, None, regularizers.l2(0.01))\n",
    "        test_model.compile(optimizer = \"Adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "        test_model.fit(x = X_Train_2, y = Y_Train_2, epochs = 64, verbose = 0)\n",
    "        train_pred = test_model.evaluate(x = X_Train_2, y = Y_Train_2)\n",
    "        cv_pred = test_model.evaluate(x = X_CV_2, y = Y_CV_2)\n",
    "        \n",
    "        #Evaluate performance by weighted sum of accuracies\n",
    "        perform = train_pred[1]*0.6 + cv_pred[1]\n",
    "        \n",
    "        if perform > best_perf :\n",
    "            best_perf = perform\n",
    "            best_arch = layers\n",
    "            best_train = train_pred\n",
    "            best_cv = cv_pred\n",
    "        \n",
    "    return best_arch, best_train, best_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "791/791 [==============================] - 0s 173us/step\n",
      "100/100 [==============================] - 0s 100us/step\n",
      "791/791 [==============================] - 0s 235us/step\n",
      "100/100 [==============================] - 0s 120us/step\n",
      "791/791 [==============================] - 0s 254us/step\n",
      "100/100 [==============================] - 0s 130us/step\n",
      "791/791 [==============================] - 0s 295us/step\n",
      "100/100 [==============================] - 0s 130us/step\n",
      "791/791 [==============================] - 0s 303us/step\n",
      "100/100 [==============================] - 0s 110us/step\n",
      "791/791 [==============================] - 0s 322us/step\n",
      "100/100 [==============================] - 0s 100us/step\n",
      "791/791 [==============================] - 0s 355us/step\n",
      "100/100 [==============================] - 0s 100us/step\n",
      "791/791 [==============================] - 0s 382us/step\n",
      "100/100 [==============================] - 0s 100us/step\n",
      "791/791 [==============================] - 0s 422us/step\n",
      "100/100 [==============================] - 0s 110us/step\n",
      "791/791 [==============================] - 0s 420us/step\n",
      "100/100 [==============================] - 0s 80us/step\n",
      "791/791 [==============================] - 0s 443us/step\n",
      "100/100 [==============================] - 0s 90us/step\n",
      "791/791 [==============================] - 0s 484us/step\n",
      "100/100 [==============================] - 0s 100us/step\n",
      "791/791 [==============================] - 0s 525us/step\n",
      "100/100 [==============================] - 0s 120us/step\n",
      "791/791 [==============================] - 0s 540us/step\n",
      "100/100 [==============================] - 0s 90us/step\n",
      "791/791 [==============================] - 0s 604us/step\n",
      "100/100 [==============================] - 0s 120us/step\n",
      "791/791 [==============================] - 0s 612us/step\n",
      "100/100 [==============================] - 0s 100us/step\n",
      "791/791 [==============================] - 1s 709us/step\n",
      "100/100 [==============================] - 0s 130us/step\n",
      "791/791 [==============================] - 0s 608us/step\n",
      "100/100 [==============================] - 0s 100us/step\n",
      "791/791 [==============================] - 1s 772us/step\n",
      "100/100 [==============================] - 0s 100us/step\n",
      "791/791 [==============================] - 1s 738us/step\n",
      "100/100 [==============================] - 0s 140us/step\n",
      "791/791 [==============================] - 1s 776us/step\n",
      "100/100 [==============================] - 0s 140us/step\n",
      "791/791 [==============================] - 1s 786us/step\n",
      "100/100 [==============================] - 0s 120us/step\n",
      "791/791 [==============================] - 1s 836us/step\n",
      "100/100 [==============================] - 0s 130us/step\n",
      "791/791 [==============================] - 1s 798us/step\n",
      "100/100 [==============================] - 0s 90us/step\n",
      "791/791 [==============================] - 1s 850us/step\n",
      "100/100 [==============================] - 0s 110us/step\n",
      "791/791 [==============================] - 1s 870us/step\n",
      "100/100 [==============================] - 0s 120us/step\n",
      "791/791 [==============================] - 1s 899us/step\n",
      "100/100 [==============================] - 0s 110us/step\n",
      "791/791 [==============================] - 1s 944us/step\n",
      "100/100 [==============================] - 0s 130us/step\n",
      "791/791 [==============================] - 1s 1ms/step\n",
      "100/100 [==============================] - 0s 140us/step\n",
      "791/791 [==============================] - 1s 1ms/step\n",
      "100/100 [==============================] - 0s 120us/step\n",
      "791/791 [==============================] - 1s 1ms/step\n",
      "100/100 [==============================] - 0s 130us/step\n",
      "791/791 [==============================] - 1s 1ms/step\n",
      "100/100 [==============================] - 0s 130us/step\n"
     ]
    }
   ],
   "source": [
    "nn_architecture, train_perf, cv_perf = Find_Architecture(X_Train, Y_Train, X_CV, Y_CV, 10, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 11, 11, 8, 7, 4, 4, 4]\n",
      "\n",
      "Train Loss = 0.4919141821855239\n",
      "Train Accuracy = 0.8305941837475966\n",
      "\n",
      "CV Loss = 0.4609449076652527\n",
      "CV Accuracy = 0.86\n"
     ]
    }
   ],
   "source": [
    "print(nn_architecture)\n",
    "print()\n",
    "print (\"Train Loss = \" + str(train_perf[0]))\n",
    "print (\"Train Accuracy = \" + str(train_perf[1]))\n",
    "print()\n",
    "print (\"CV Loss = \" + str(cv_perf[0]))\n",
    "print (\"CV Accuracy = \" + str(cv_perf[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So even with our best neural network model, our performance is still not great, approximately on par with all of the previous models.  \n",
    "\n",
    "So far of the individual models the Gradient Boosting Classifier is the best performing.\n",
    "\n",
    "## Building an Ensemble\n",
    "\n",
    "The next step is to see if we can get a bit better performance by making a whole that is greater than the sum of its parts - with an ensemble of all of our previous models.\n",
    "\n",
    "So I will bind each of our models together by virtue of another smaller neural network and use that to feed into our final output.\n",
    "\n",
    "So the first step is to define our parameters for each of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 2000,\n",
    "     'warm_start': False, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 300,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "#Neural Network parameters\n",
    "nn_params = {\n",
    "    'layers' : [19, 11, 11, 8, 7, 4, 4, 4],\n",
    "    'act_reg' : None,\n",
    "    'ker_reg' : regularizers.l2(0.01),\n",
    "    'input_shape' : (24,)\n",
    "}\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 1000,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 32,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate' : 0.05,\n",
    "    'max_depth': None,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 5000,\n",
    "    'learning_rate' : 0.75\n",
    "}\n",
    "\n",
    "#SVM parameters\n",
    "svm_params = {\n",
    "    'kernel' : 'rbf',\n",
    "    'C' : 1.0  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to define our classes that can handle building and prediction on each of the models and produce our outputs. \n",
    "\n",
    "This will be a two step process of first building our first layer of the ensemble which will contain each of our 6 models and contain functions to initialize, train and generate predictions.\n",
    "\n",
    "The second output layer will then contain functions to build the inner layers, define and build a second layer neural network from the output of the inner layer and generate our final output vector $\\hat{y}$.\n",
    "\n",
    "\n",
    "For now I will not include any hyperparameter tuning in this class outside of the second layer neural network, although this may change later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble_inner_models(object):\n",
    "    #Initialization method to take all of our model parameters\n",
    "    def __init__(self, rf_params, ada_params, gb_params, svm_params, et_params, nn_params):\n",
    "        #Initialize all parameter variables\n",
    "        self.rf_params = rf_params\n",
    "        self.ada_params = ada_params\n",
    "        self.gb_params = gb_params\n",
    "        self.svm_params = svm_params\n",
    "        self.et_params = et_params\n",
    "        self.nn_params = nn_params\n",
    "        \n",
    "        \n",
    "    \n",
    "    #Now to initialize each of our models\n",
    "    def initialize_models(self):\n",
    "        self.svm_mod = svm.SVC(**self.svm_params)\n",
    "        self.rf_mod =  RandomForestClassifier(**self.rf_params)\n",
    "        self.ada_mod = AdaBoostClassifier(**self.ada_params)\n",
    "        self.gb_mod = GradientBoostingClassifier(**self.gb_params)\n",
    "        self.et_mod = ExtraTreesClassifier(**self.et_params)\n",
    "        self.nn_mod = NN_model(**self.nn_params)\n",
    "        self.nn_mod.compile(optimizer = \"Adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "    \n",
    "    #Method to train each of the models\n",
    "    def train_models(self, X_Train, Y_Train, num_epochs = 64):\n",
    "        self.svm_mod.fit(X_Train, Y_Train)\n",
    "        self.rf_mod.fit(X_Train, Y_Train)\n",
    "        self.ada_mod.fit(X_Train, Y_Train)\n",
    "        self.gb_mod.fit(X_Train, Y_Train)\n",
    "        self.et_mod.fit(X_Train, Y_Train)\n",
    "        self.nn_mod.fit(x = X_Train, y = Y_Train, epochs = num_epochs, verbose = 0)\n",
    "    \n",
    "    #Debugging function to get the outputs of each system individually\n",
    "    def get_pred_matrix(self, X_Test):\n",
    "        #Matrix output will be of shape (m, num_models) - for now hardcoded\n",
    "        pred_mat = np.zeros((X_Test.shape[0], 6))\n",
    "        \n",
    "        #Build prediction matrix\n",
    "        pred_mat[:, 0] = self.svm_mod.predict(X_Test)\n",
    "        pred_mat[:, 1] = self.rf_mod.predict(X_Test)\n",
    "        pred_mat[:, 2] = self.ada_mod.predict(X_Test)\n",
    "        pred_mat[:, 3] = self.gb_mod.predict(X_Test)\n",
    "        pred_mat[:, 4] = self.et_mod.predict(X_Test)\n",
    "        pred_mat[:, 5] = self.nn_mod.predict(X_Test, verbose=0).reshape((X_Test.shape[0]))\n",
    "        \n",
    "        return pred_mat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to put that into our final output layer and build the relevant functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ensemble_model(object):\n",
    "    def __init__(self, rf_params, ada_params, gb_params, svm_params, et_params, nn_params):\n",
    "        #Initialize all parameter variables\n",
    "        self.rf_params = rf_params\n",
    "        self.ada_params = ada_params\n",
    "        self.gb_params = gb_params\n",
    "        self.svm_params = svm_params\n",
    "        self.et_params = et_params\n",
    "        self.nn_params = nn_params\n",
    "    \n",
    "    def build_inner_models(self, X_Train, Y_Train, num_epochs = 32):\n",
    "        self.inner_models = Ensemble_inner_models(self.rf_params, self.ada_params, self.gb_params, self.svm_params, self.et_params, self.nn_params)\n",
    "        self.inner_models.initialize_models()\n",
    "        self.inner_models.train_models(X_Train, Y_Train)\n",
    "\n",
    "    #Debugging function to optimize the architecture second layer NN\n",
    "    def find_ensem_layer(self, X_Train, Y_Train, X_CV, Y_CV, max_depth = 10, num_iters = 20):\n",
    "        Mod_train_preds = self.inner_models.get_pred_matrix(X_Train)\n",
    "        Mod_CV_preds = self.inner_models.get_pred_matrix(X_CV)\n",
    "        best_arch, best_train, best_cv = Find_Architecture(Mod_train_preds, Y_Train, Mod_CV_preds, Y_CV, max_depth, num_iters)\n",
    "        return best_arch, best_train, best_cv\n",
    "    \n",
    "    #Function to build the ensemble layer\n",
    "    def build_ensem_layer(self, arch, X_Train_2, Y_Train_2, num_epochs = 32, print_result = 0):\n",
    "        #Compile Ensemble layer\n",
    "        self.ensemble_mod = NN_model((6,), arch, None, regularizers.l2(0.01))\n",
    "        self.ensemble_mod.compile(optimizer = \"Adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "        \n",
    "        #Perform inner calculations\n",
    "        Train_preds = self.inner_models.get_pred_matrix(X_Train_2)\n",
    "        \n",
    "        #Fit ensemble layer\n",
    "        self.ensemble_mod.fit(x = Train_preds, y = Y_Train_2, epochs = num_epochs, verbose = 1)\n",
    "        \n",
    "        #Output performance metrics\n",
    "        train_perf = self.ensemble_mod.evaluate(x = Train_preds, y = Y_Train_2)   \n",
    "        if print_result == 1:\n",
    "            print (\"Train Loss = \" + str(train_perf[0]))\n",
    "            print (\"Train Accuracy = \" + str(train_perf[1]))\n",
    "            \n",
    "        return train_perf\n",
    "    \n",
    "    def print_evaluation(self, X_eval, Y_eval):\n",
    "        inner_preds = self.inner_models.get_pred_matrix(X_eval)\n",
    "        perf = self.ensemble_mod.evaluate(x = inner_preds, y = Y_eval)\n",
    "        print (\"Loss = \" + str(perf[0]))\n",
    "        print (\"Accuracy = \" + str(perf[1]))\n",
    "        \n",
    "        \n",
    "    def get_predictions(self, X_Test):\n",
    "        inner_preds = self.inner_models.get_pred_matrix(X_Test)\n",
    "        final_preds_raw = self.ensemble_mod.predict(X_Test, verbose=0).reshape((X_Test.shape[0]))\n",
    "        \n",
    "        final_preds = np.around(final_preds_raw)\n",
    "        return final_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our final model built lets see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "791/791 [==============================] - 2s 2ms/step\n",
      "Train Loss = 0.09697772002532871\n",
      "Train Accuracy = 0.9873577749683944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09697772002532871, 0.9873577749683944]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize model\n",
    "fin_model = ensemble_model(rf_params, ada_params, gb_params, svm_params, et_params, nn_params)\n",
    "fin_model.build_inner_models(X_Train, Y_Train)\n",
    "\n",
    "#Previously ran function to find a good architecture\n",
    "fin_model.build_ensem_layer([4, 4, 3, 2], X_Train, Y_Train, print_result = 1, num_epochs = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "791/791 [==============================] - 0s 97us/step\n",
      "Loss = 0.09697772002532871\n",
      "Accuracy = 0.9873577749683944\n"
     ]
    }
   ],
   "source": [
    "#Print Training performance\n",
    "fin_model.print_evaluation(X_Train, Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 190us/step\n",
      "Loss = 0.6704975652694702\n",
      "Accuracy = 0.84\n"
     ]
    }
   ],
   "source": [
    "#Print CV Performance\n",
    "fin_model.print_evaluation(X_CV, Y_CV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our ensemble model is alas not greater than the sum of its parts, in fact it seems to have a heavy overreliance on the heavily overfitting classifiers.  \n",
    "\n",
    "Perhaps this could be fixed by better regularization of the decision-tree based classifiers, or implementation of dropout regularization in the second layer network. \n",
    "\n",
    "So let us return to using a simple neural networking classifier and try out different combinations of feature engineering to test performance and hopefully get a better model.\n",
    "\n",
    "Part 5 to follow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
