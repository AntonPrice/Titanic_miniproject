{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Classification - First Submission (Part 5)\n",
    "\n",
    "Now we have our basic ensemble model now its time to improve the performance and get a final model worthy of submission ready.\n",
    "\n",
    "So firstly lets import everything we need including all of the previously built functions.\n",
    "\n",
    "To give credit some of the ideas for data transformations taken from - \n",
    "\n",
    "https://www.kaggle.com/sinakhorami/titanic-best-working-classifier?scriptVersionId=566580/notebook <br>\n",
    "https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python?scriptVersionId=2368078"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#First importing some relevant packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Import Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "#Import Keras\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "#Import mathematical functions\n",
    "from random import *\n",
    "import math\n",
    "\n",
    "#Get regular expression package\n",
    "import re\n",
    "\n",
    "#Import  Scikit learn framework\n",
    "import sklearn as sk\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the functions built in previous parts\n",
    "from Titanic_Import import *\n",
    "\n",
    "full_set = pd.read_csv('D:/Datasets/Titanic/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets go right back to the start and have a look at our raw data once more and revisit the transformations we built way back in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "5            6         0       3   \n",
       "6            7         0       1   \n",
       "7            8         0       3   \n",
       "8            9         1       3   \n",
       "9           10         1       2   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "5                                   Moran, Mr. James    male   NaN      0   \n",
       "6                            McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "7                     Palsson, Master. Gosta Leonard    male   2.0      3   \n",
       "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
       "9                Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  \n",
       "5      0            330877   8.4583   NaN        Q  \n",
       "6      0             17463  51.8625   E46        S  \n",
       "7      1            349909  21.0750   NaN        S  \n",
       "8      2            347742  11.1333   NaN        S  \n",
       "9      0            237736  30.0708   NaN        C  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_set.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Experimentation\n",
    "\n",
    "For this notebook most of the experimentation was done in an ad-hoc fashion and thus will not be shown explicitly.  However they can be verified by modifying the functions below.\n",
    "\n",
    "The architectures were derived by the random brute force method as described originally.\n",
    "\n",
    "The experiments were using numeric fields left raw (with medians input in place of NaN) unless stated otherwise, I was experiemting using categorical numeric variables \n",
    "\n",
    "### Results Table\n",
    "\n",
    "Test | Architecture | Train Accuracy | CV Accuracy\n",
    "--- | --- | --- | ---\n",
    "Original Benchmark | [22, 22, 11, 10, 7, 5, 3, 3, 3] | 0.8394437427014376 | 0.85 \n",
    "Original Benchmark (2) | [13, 13, 11, 8, 8, 4] | 0.8419721877077587 | 0.86 \n",
    "No OH, categorical deck, no norm | [6, 4, 3, 3, 3, 3] | 0.7572692797699409 | 0.83\n",
    "Categorical deck, isalone OH, hascabin OH, no norm | [9, 8, 6, 6, 5, 4, 4] |  0.7484197225492311 | 0.83\n",
    "Categorical deck, isalone OH, hascabin OH, normalized fare/age | [11, 7, 5] | 0.804045511482639 | 0.85\n",
    "All tested variables plus name length (no norm) | [7, 6, 5, 5] | 0.7724399495064534 | 0.83\n",
    "All tested variables plus name length (with norm) | [7, 6, 5, 5] | 0.8128950690047629 | 0.84\n",
    "All tested variables plus name length (with norm) and title | [18, 10, 8, 4, 3, 3, 2] | 0.8343868523873813 | 0.89\n",
    "Original Benchmark plus name length and title | [14, 12, 6, 6, 6] | 0.8381795198968629 | 0.89\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Learnings\n",
    "\n",
    "## Experimentation with Data\n",
    "\n",
    "* Initial intuitions about data normalization were accurate and do improve performance\n",
    "* Using One-Hot encodings do improve performance over leaving categorical fields as numeric\n",
    "* Adding Binary features such as if a person is alone or do they have a cabin does not improve performance over adding a cleansed field's full data\n",
    "* Our Cross Validation set is very slightly biased in favour of survivors by comparison to our training data\n",
    "* As discovered from experimentation in an earlier notebook L2 regularization with $\\lambda = 0.01$ seems to be about the best value with a naive search.\n",
    "* Adding name length and title may give slight benefits over not having them but random initialization has a bigger impact\n",
    "\n",
    "## Online research\n",
    "\n",
    "* Initial intuitions about cabin feature were correct - see https://www.kaggle.com/ccastleberry/titanic-cabin-features/notebook for another notebook which independantly had the same intuitions.\n",
    "* While I was initially dissapointed with scores having seen the best kernels have 100% accuracy, with 80%+ accuracy I'm actually doing pretty well - https://www.kaggle.com/pliptor/how-am-i-doing-with-my-score\n",
    "* The kaggle leaderboard is very skewed, the top submissions with 100% accuracy are all likely using the actual data from the event and not a ML algorithm.  \n",
    "* Submissions are then very heavily skewed with 99-88% accuracy models all likely being some genetically programmed algorithm that is non-human readable (given publically available models with that accuracy).\n",
    "* Most other kernels use some form of decision tree with ensemble models and achieve about $80 - 85%$% accuracy on out of sample data. Therefore this seems to be a good target at which to aim.\n",
    "\n",
    "# Final Model\n",
    "\n",
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Title function from Sina\n",
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "#Creating our Training Set\n",
    "def Cleanse_Training_Data_v2(df_in):\n",
    "    #Put our dataframe into new object to avoid corrupting original dataframe\n",
    "    test_set = df_in\n",
    "    \n",
    "    test_set['Age'] = test_set.groupby(['Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    #Name Length from Anisotropic\n",
    "    test_set['Name_length'] = test_set['Name'].apply(len)\n",
    "    \n",
    "    #Normalize numerical fields\n",
    "    age_mean = test_set['Age'].mean()\n",
    "    fare_mean = test_set['Fare'].mean()\n",
    "    name_len_mean = test_set['Name_length'].mean()\n",
    "    \n",
    "    age_range = test_set['Age'].max() - test_set['Age'].min()\n",
    "    fare_range = test_set['Fare'].max() - test_set['Fare'].min()\n",
    "    name_len_range = test_set['Name_length'].max() - test_set['Name_length'].min()\n",
    "    \n",
    "    test_set['Norm_age'] = (test_set['Age'] - age_mean) / age_range\n",
    "    test_set['Norm_fare'] = (test_set['Fare'] - fare_mean) / fare_range\n",
    "    #test_set['Norm_name'] = (test_set['Name_length'] - name_len_mean) / name_len_range\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Getting our Deck\n",
    "    test_set['canc'] = test_set['Cabin'].str.replace(' ', '')\n",
    "    test_set['Deckstr'] = test_set['canc'].str[0]\n",
    "    test_set['Deckstr'] = test_set['Deckstr'].fillna(value = 'X')\n",
    "    test_set['Deckstr'] = test_set['Deckstr'].map( {'A': 1, 'B': 2, 'C' : 3,'D' : 4, 'E' : 5,'F' : 6,'G' : 7, 'T' : 8 ,'X' : 0} ).astype(int)\n",
    "    \n",
    "    #Remap Gender and create number of family members present field\n",
    "    test_set['Sex'] = test_set['Sex'].map( {'female': -1, 'male': 1} ).astype(int)\n",
    "    test_set['Company'] = test_set['SibSp'] + test_set['Parch']\n",
    "    \n",
    "    #Alone classifier took from Sina\n",
    "    test_set['IsAlone'] = 0\n",
    "    test_set.loc[test_set['Company'] == 1, 'IsAlone'] = 1\n",
    "    \n",
    "    #Alone classifier took from Sina\n",
    "    test_set['Has_Cabin'] = 1\n",
    "    test_set.loc[test_set['Deckstr'] == 1, 'Has_Cabin'] = 0\n",
    "    \n",
    "    #Applying Title code from Sia\n",
    "    test_set['Title'] = test_set['Name'].apply(get_title)\n",
    "    test_set['Title'] = test_set['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    test_set['Title'] = test_set['Title'].replace('Mlle', 'Miss')\n",
    "    test_set['Title'] = test_set['Title'].replace('Ms', 'Miss')\n",
    "    test_set['Title'] = test_set['Title'].replace('Mme', 'Mrs')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #Manually populate embarked with correct values (only 2 looked up correct value based on average fare)\n",
    "    values = {'Embarked': 'C'}\n",
    "    test_set = test_set.fillna(value=values)\n",
    "    test_set['Embarked'] = test_set['Embarked'].map( {'S': 0, 'C': 1, 'Q' : 2} ).astype(int)\n",
    "    \n",
    "    \n",
    "    \n",
    "    emb_set = pd.get_dummies(test_set.Embarked, prefix='Emb', dummy_na = False)\n",
    "    title_set = pd.get_dummies(test_set.Title, prefix='ti', dummy_na = True)\n",
    "    deck_set = pd.get_dummies(test_set.Deckstr, prefix='de', dummy_na = False)\n",
    "     \n",
    "\n",
    "\n",
    "    \n",
    "    oh_set = pd.concat([test_set,  \n",
    "                        emb_set, \n",
    "                        title_set, \n",
    "                        deck_set], axis=1)\n",
    "    \n",
    "    #Create output fully numeric dataframe\n",
    "    out_set = oh_set.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', \n",
    "                             'Cabin', 'canc', 'Embarked', 'Title', 'IsAlone', 'Has_Cabin', \n",
    "                           'Deckstr', 'Age', 'Fare', 'Name_length'], axis=1)\n",
    "\n",
    "    \n",
    "    #Segmenting data\n",
    "    X_Train_df = out_set.head(791)\n",
    "    X_CV_df = out_set.tail(100)\n",
    "\n",
    "    #Getting our Y vectors\n",
    "    Y_Train = X_Train_df['Survived'].values\n",
    "    Y_CV = X_CV_df['Survived'].values\n",
    "\n",
    "    #Dropping columns we don't want to feed into our ML algorithm\n",
    "    X_Train_df = X_Train_df.drop(['Survived'], axis=1)\n",
    "    X_CV_df = X_CV_df.drop(['Survived'], axis=1)\n",
    "\n",
    "    #Getting our X vectors\n",
    "    X_Train = X_Train_df.values\n",
    "    X_CV = X_CV_df.values\n",
    "    \n",
    "    return X_Train, X_CV, Y_Train, Y_CV, out_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train, X_CV, Y_Train, Y_CV, test = Cleanse_Training_Data_v2(full_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the brute-force method to find the best model for now. We have slightly modified our old algorithm to allow us to output the actual model used, so we don't have to deal with variance through random initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Architecture_v2(X_Train_2, Y_Train_2, X_CV_2, Y_CV_2, max_layers = 10, num_iters = 32): \n",
    "    best_perf = 0.0\n",
    "    #Iterate through n interations\n",
    "    for i in range(num_iters):\n",
    "        #Reset hyperparameters and initalize nn depth\n",
    "        layers = []\n",
    "        num_layers = randint(3, max_layers)\n",
    "        prev_layer = X_Train_2.shape[1]\n",
    "        \n",
    "        for j in range(num_layers):\n",
    "            #Randomly generate number of units per layer\n",
    "            min_size = math.ceil(prev_layer / 2.0)\n",
    "            lay_size = randint(min_size, prev_layer)\n",
    "            layers.append(lay_size)\n",
    "            prev_layer = lay_size\n",
    "            \n",
    "        #Build and test model\n",
    "        test_model = NN_model((X_Train_2.shape[1], ), layers, None, regularizers.l2(0.01))\n",
    "        test_model.compile(optimizer = \"Adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "        test_model.fit(x = X_Train_2, y = Y_Train_2, epochs = 32, verbose = 0)\n",
    "        train_pred = test_model.evaluate(x = X_Train_2, y = Y_Train_2)\n",
    "        cv_pred = test_model.evaluate(x = X_CV_2, y = Y_CV_2)\n",
    "        \n",
    "        #Evaluate performance by weighted sum of accuracies\n",
    "        perform = train_pred[1]*0.6 + cv_pred[1]\n",
    "        \n",
    "        if perform > best_perf :\n",
    "            best_perf = perform\n",
    "            best_arch = layers\n",
    "            best_train = train_pred\n",
    "            best_cv = cv_pred\n",
    "            best_model = test_model\n",
    "        \n",
    "    return best_arch, best_train, best_cv, best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning the below step can take a while to run.  Modify the num_iters variable to reduce run times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "791/791 [==============================] - 0s 169us/step\n",
      "100/100 [==============================] - 0s 130us/step\n",
      "791/791 [==============================] - 0s 131us/step\n",
      "100/100 [==============================] - 0s 120us/step\n",
      "791/791 [==============================] - 0s 197us/step\n",
      "100/100 [==============================] - 0s 130us/step\n",
      "791/791 [==============================] - 0s 206us/step\n",
      "100/100 [==============================] - 0s 110us/step\n",
      "791/791 [==============================] - 0s 252us/step\n",
      "100/100 [==============================] - 0s 130us/step\n",
      "791/791 [==============================] - 0s 274us/step\n",
      "100/100 [==============================] - 0s 140us/step\n",
      "791/791 [==============================] - 0s 262us/step\n",
      "100/100 [==============================] - 0s 110us/step\n",
      "791/791 [==============================] - 0s 349us/step\n",
      "100/100 [==============================] - 0s 130us/step\n",
      "791/791 [==============================] - 0s 286us/step\n",
      "100/100 [==============================] - 0s 80us/step\n",
      "791/791 [==============================] - 0s 469us/step\n",
      "100/100 [==============================] - 0s 150us/step\n",
      "791/791 [==============================] - 0s 420us/step\n",
      "100/100 [==============================] - 0s 120us/step\n",
      "791/791 [==============================] - 1s 635us/step\n",
      "100/100 [==============================] - 0s 180us/step\n",
      "791/791 [==============================] - 0s 455us/step\n",
      "100/100 [==============================] - 0s 110us/step\n",
      "791/791 [==============================] - 0s 507us/step\n",
      "100/100 [==============================] - 0s 140us/step\n",
      "791/791 [==============================] - 0s 506us/step\n",
      "100/100 [==============================] - 0s 120us/step\n",
      "791/791 [==============================] - 0s 513us/step\n",
      "100/100 [==============================] - 0s 100us/step\n",
      "791/791 [==============================] - 0s 577us/step\n",
      "100/100 [==============================] - 0s 130us/step\n",
      "791/791 [==============================] - 1s 633us/step\n",
      "100/100 [==============================] - 0s 160us/step\n",
      "791/791 [==============================] - 0s 616us/step\n",
      "100/100 [==============================] - 0s 120us/step\n",
      "791/791 [==============================] - 1s 637us/step\n",
      "100/100 [==============================] - 0s 120us/step\n",
      "791/791 [==============================] - 1s 657us/step\n",
      "100/100 [==============================] - 0s 130us/step\n",
      "791/791 [==============================] - 1s 742us/step\n",
      "100/100 [==============================] - 0s 150us/step\n",
      "791/791 [==============================] - 1s 733us/step\n",
      "100/100 [==============================] - 0s 150us/step\n",
      "791/791 [==============================] - 1s 798us/step\n",
      "100/100 [==============================] - 0s 140us/step\n",
      "791/791 [==============================] - 1s 861us/step\n",
      "100/100 [==============================] - 0s 160us/step\n"
     ]
    }
   ],
   "source": [
    "nn_architecture, train_perf, cv_perf, best_model = Find_Architecture_v2(X_Train, Y_Train, X_CV, Y_CV, 10, num_iters =  25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 14, 8, 5, 5]\n",
      "\n",
      "Train Loss = 0.46168896597345016\n",
      "Train Accuracy = 0.8419721877077587\n",
      "\n",
      "CV Loss = 0.4022293281555176\n",
      "CV Accuracy = 0.88\n"
     ]
    }
   ],
   "source": [
    "print(nn_architecture)\n",
    "print()\n",
    "print (\"Train Loss = \" + str(train_perf[0]))\n",
    "print (\"Train Accuracy = \" + str(train_perf[1]))\n",
    "print()\n",
    "print (\"CV Loss = \" + str(cv_perf[0]))\n",
    "print (\"CV Accuracy = \" + str(cv_perf[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems good so far let's have a look at the confusion matrix.\n",
    "\n",
    "However first we need to normalize the output predictions with a simple function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = best_model.predict(x = X_Train)\n",
    "cv_pred = best_model.predict(x = X_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_predictions(y_hat):\n",
    "    y_out = y_hat.reshape((y_hat.shape[0],))\n",
    "    y_out = np.around(y_out)\n",
    "    \n",
    "    return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hat = normalize_predictions(train_pred)\n",
    "cv_hat = normalize_predictions(cv_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  84.19721871049305\n",
      "F1 Score =  0.7920133111480866\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "      <th>Actual True</th>\n",
       "      <th>Actual False</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pred True</td>\n",
       "      <td>238.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pred False</td>\n",
       "      <td>68.0</td>\n",
       "      <td>428.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Labels  Actual True  Actual False\n",
       "0   Pred True        238.0          57.0\n",
       "1  Pred False         68.0         428.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc1, score1, conf1 = Calc_Accuracy(Y_Train, train_hat)\n",
    "\n",
    "print(\"Accuracy = \", acc1)\n",
    "print(\"F1 Score = \", score1)\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix\")\n",
    "conf1[[\"Labels\", \"Actual True\", \"Actual False\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  88.0\n",
      "F1 Score =  0.8333333333333334\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "      <th>Actual True</th>\n",
       "      <th>Actual False</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pred True</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pred False</td>\n",
       "      <td>6.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Labels  Actual True  Actual False\n",
       "0   Pred True         30.0           6.0\n",
       "1  Pred False          6.0          58.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc2, score2, conf2 = Calc_Accuracy(Y_CV, cv_hat)\n",
    "\n",
    "print(\"Accuracy = \", acc2)\n",
    "print(\"F1 Score = \", score2)\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix\")\n",
    "conf2[[\"Labels\", \"Actual True\", \"Actual False\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our classifier is not massively skewed in output inaccuracies and we are thus about ready to generate out submission file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanse Submission File Data\n",
    "\n",
    "Now as we applied the transformations to our training data lets apply similar transforms to our submission data, however we need to keep in mind we manually populated the embarked column by looking at the NaN values and average fares of people who boarded at each of the 3 locations and choosing the embarking location based on the fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>897</td>\n",
       "      <td>3</td>\n",
       "      <td>Svensson, Mr. Johan Cervin</td>\n",
       "      <td>male</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7538</td>\n",
       "      <td>9.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>898</td>\n",
       "      <td>3</td>\n",
       "      <td>Connolly, Miss. Kate</td>\n",
       "      <td>female</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330972</td>\n",
       "      <td>7.6292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>899</td>\n",
       "      <td>2</td>\n",
       "      <td>Caldwell, Mr. Albert Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>248738</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900</td>\n",
       "      <td>3</td>\n",
       "      <td>Abrahim, Mrs. Joseph (Sophie Halaut Easu)</td>\n",
       "      <td>female</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2657</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>3</td>\n",
       "      <td>Davies, Mr. John Samuel</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>A/4 48871</td>\n",
       "      <td>24.1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name     Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    male   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       "3          895       3                              Wirz, Mr. Albert    male   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "5          897       3                    Svensson, Mr. Johan Cervin    male   \n",
       "6          898       3                          Connolly, Miss. Kate  female   \n",
       "7          899       2                  Caldwell, Mr. Albert Francis    male   \n",
       "8          900       3     Abrahim, Mrs. Joseph (Sophie Halaut Easu)  female   \n",
       "9          901       3                       Davies, Mr. John Samuel    male   \n",
       "\n",
       "    Age  SibSp  Parch     Ticket     Fare Cabin Embarked  \n",
       "0  34.5      0      0     330911   7.8292   NaN        Q  \n",
       "1  47.0      1      0     363272   7.0000   NaN        S  \n",
       "2  62.0      0      0     240276   9.6875   NaN        Q  \n",
       "3  27.0      0      0     315154   8.6625   NaN        S  \n",
       "4  22.0      1      1    3101298  12.2875   NaN        S  \n",
       "5  14.0      0      0       7538   9.2250   NaN        S  \n",
       "6  30.0      0      0     330972   7.6292   NaN        Q  \n",
       "7  26.0      1      1     248738  29.0000   NaN        S  \n",
       "8  18.0      0      0       2657   7.2292   NaN        C  \n",
       "9  21.0      2      0  A/4 48871  24.1500   NaN        S  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_set = pd.read_csv('D:/Datasets/Titanic/test.csv')\n",
    "\n",
    "sub_set.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    418\n",
       "Pclass         418\n",
       "Name           418\n",
       "Sex            418\n",
       "Age            332\n",
       "SibSp          418\n",
       "Parch          418\n",
       "Ticket         418\n",
       "Fare           417\n",
       "Cabin           91\n",
       "Embarked       418\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_set.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily we don't have any missing embarked values so we do not need to account for this.\n",
    "\n",
    "We do however have missing ages and one missing fare.  The missing ages are fine as we have this in our original algorithm.\n",
    "\n",
    "However lets have a look at our missing fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1044</td>\n",
       "      <td>3</td>\n",
       "      <td>Storey, Mr. Thomas</td>\n",
       "      <td>male</td>\n",
       "      <td>60.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3701</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                Name   Sex   Age  SibSp  Parch Ticket  \\\n",
       "152         1044       3  Storey, Mr. Thomas  male  60.5      0      0   3701   \n",
       "\n",
       "     Fare Cabin Embarked  \n",
       "152   NaN   NaN        S  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_set[sub_set['Fare'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the best method of accounting for this will likely be to insert the median fare for his class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy = sub_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy['Fare'] = testy.groupby(['Pclass'])['Fare'].apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1044</td>\n",
       "      <td>3</td>\n",
       "      <td>Storey, Mr. Thomas</td>\n",
       "      <td>male</td>\n",
       "      <td>60.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3701</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                Name   Sex   Age  SibSp  Parch Ticket  \\\n",
       "152         1044       3  Storey, Mr. Thomas  male  60.5      0      0   3701   \n",
       "\n",
       "       Fare Cabin Embarked  \n",
       "152  7.8958   NaN        S  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testy[testy['PassengerId'] == 1044]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seemed to work fine, so lets create our submission file, first by cleansing our data.\n",
    "\n",
    "Also doing some cursory analysis to ensure our features will line up correctly (so our one hot encodings work correctly), we have all the same titles and embarked locations, however we have 1 extra cabin in our Training data.  Fortunately this extra cabin is T (marked as 8 in our original data).  \n",
    "\n",
    "In order to easily account for this we will use the get_dummies function with an extra column which by definition will always be 0 using the dummy_na function which will appear in the same column index as our T index from our original data.\n",
    "\n",
    "Other than that our columns will line up perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating our Training Set\n",
    "def Cleanse_Submission_Data_v2(df_in):\n",
    "    #Put our dataframe into new object to avoid corrupting original dataframe\n",
    "    test_set = df_in\n",
    "    \n",
    "    test_set['Age'] = test_set.groupby(['Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\n",
    "    test_set['Fare'] = test_set.groupby(['Pclass'])['Fare'].apply(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    #Name Length from Anisotropic\n",
    "    test_set['Name_length'] = test_set['Name'].apply(len)\n",
    "    \n",
    "    #Normalize numerical fields\n",
    "    age_mean = test_set['Age'].mean()\n",
    "    fare_mean = test_set['Fare'].mean()\n",
    "    name_len_mean = test_set['Name_length'].mean()\n",
    "    \n",
    "    age_range = test_set['Age'].max() - test_set['Age'].min()\n",
    "    fare_range = test_set['Fare'].max() - test_set['Fare'].min()\n",
    "    name_len_range = test_set['Name_length'].max() - test_set['Name_length'].min()\n",
    "    \n",
    "    test_set['Norm_age'] = (test_set['Age'] - age_mean) / age_range\n",
    "    test_set['Norm_fare'] = (test_set['Fare'] - fare_mean) / fare_range\n",
    "    test_set['Norm_name'] = (test_set['Name_length'] - fare_mean) / fare_range\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Getting our Deck\n",
    "    test_set['canc'] = test_set['Cabin'].str.replace(' ', '')\n",
    "    test_set['Deckstr'] = test_set['canc'].str[0]\n",
    "    test_set['Deckstr'] = test_set['Deckstr'].fillna(value = 'X')\n",
    "    test_set['Deckstr'] = test_set['Deckstr'].map( {'A': 1, 'B': 2, 'C' : 3,'D' : 4, 'E' : 5,'F' : 6,'G' : 7, 'T' : 8 ,'X' : 0} ).astype(int)\n",
    "    \n",
    "    #Remap Gender and create number of family members present field\n",
    "    test_set['Sex'] = test_set['Sex'].map( {'female': -1, 'male': 1} ).astype(int)\n",
    "    test_set['Company'] = test_set['SibSp'] + test_set['Parch']\n",
    "    \n",
    "    #Alone classifier took from Sina\n",
    "    test_set['IsAlone'] = 0\n",
    "    test_set.loc[test_set['Company'] == 1, 'IsAlone'] = 1\n",
    "    \n",
    "    #Alone classifier took from Sina\n",
    "    test_set['Has_Cabin'] = 1\n",
    "    test_set.loc[test_set['Deckstr'] == 1, 'Has_Cabin'] = 0\n",
    "    \n",
    "    #Applying Title code from Sia\n",
    "    test_set['Title'] = test_set['Name'].apply(get_title)\n",
    "    test_set['Title'] = test_set['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    test_set['Title'] = test_set['Title'].replace('Mlle', 'Miss')\n",
    "    test_set['Title'] = test_set['Title'].replace('Ms', 'Miss')\n",
    "    test_set['Title'] = test_set['Title'].replace('Mme', 'Mrs')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #Manually populate embarked with correct values (only 2 looked up correct value based on average fare)\n",
    "    values = {'Embarked': 'C'}\n",
    "    test_set = test_set.fillna(value=values)\n",
    "    test_set['Embarked'] = test_set['Embarked'].map( {'S': 0, 'C': 1, 'Q' : 2} ).astype(int)\n",
    "    \n",
    "    \n",
    "    \n",
    "    emb_set = pd.get_dummies(test_set.Embarked, prefix='Emb', dummy_na = False)\n",
    "    title_set = pd.get_dummies(test_set.Title, prefix='ti', dummy_na = True)\n",
    "    deck_set = pd.get_dummies(test_set.Deckstr, prefix='de', dummy_na = True)\n",
    "\n",
    "    \n",
    "    oh_set = pd.concat([test_set,  \n",
    "                        emb_set, \n",
    "                        title_set, \n",
    "                        deck_set], axis=1)\n",
    "    \n",
    "    #Create output fully numeric dataframe\n",
    "    out_set = oh_set.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', \n",
    "                             'Cabin', 'canc', 'Embarked', 'Title', 'IsAlone', 'Has_Cabin', \n",
    "                           'Deckstr', 'Age', 'Fare', 'Name_length'], axis=1)\n",
    "\n",
    "    X_Test = out_set.values\n",
    "    return X_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_Test = Cleanse_Submission_Data_v2(sub_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pred = best_model.predict(X_Test)\n",
    "test_hat = normalize_predictions(sub_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = Create_output_frame(sub_set, test_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>897</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>898</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>899</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892       0.0\n",
       "1          893       1.0\n",
       "2          894       0.0\n",
       "3          895       0.0\n",
       "4          896       1.0\n",
       "5          897       0.0\n",
       "6          898       1.0\n",
       "7          899       0.0\n",
       "8          900       1.0\n",
       "9          901       0.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to output our data and submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"Predictions.csv\", index=False, float_format='%1d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well it worked but the actual submission was pretty terrible at 78% accuracy. \n",
    "\n",
    "So the random intialization actually played a pretty large part from running various versions of the above functions multiple times.  Overall however this was prone to overfitting to the cross validation data based on the random intialization more than anything.\n",
    "\n",
    " So more work is needed to climb a bit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
